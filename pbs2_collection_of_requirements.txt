possibility to run commands directly from the log
	we need the directory to be set properly too

node config
	nodes can specify what config variables they need (probably written somewhere else)
	nodes verify, insertion time? build time?) if the variables they need exist
	ENV is reduced to the declared variables
		what about variables used by tools, does the node need to declare them?
		does the node declare the tool as dependency to build and import the tools variables?

	if no ENV is specified per node, do we have an empty ENV or the pbsfile ENV?

	
building in docker
	where does the log go?
		in docker but we'd like it outside, via mount or by making the logs available on a different machine via scp
			scp a tarball

generation of pbs_log_dep takes a long time 0.8 s for 800 files ... in parallel
	it's also re run for all the nodes in the graph, ? including warp nodes
	the depend is overridden if the build goes forward
		maybe we should generate the dep files when the build fails instead
			at least for the nodes that fail and their dependents

		it's useful to understand the the build structure

node logs contains information about their build time and what pbs generated it
	it could always contain some extra information like the compiler version, ...
		although the compiler digest should be in the dependencies

	it would be nice if the node also pointed to an extra file where the builds using the node
	can add themselves, extra information that shows how many builds are using a node that's already
	build

	the node log can contain the dependencies hashes

parallelization of build is done within a structure in memory
	we could reimplement it externally with multiple queues and some external worker that starts builds
		starting build is triggered by different stimuli
			for terminal nodes it's the availability of a builder 
				builders can be external applications, on different machines
			for parent nodes, by having all the children build, making it a terminal node

	we need to handle build errors
		particularly removing all the failed node parents and maybe continue
		the build for nodes that are OK

	the external builders, and queue manager can continue building in the background
		EG: pbs gives the hand back to the user because of errors

	on next build, pbs can verify which nodes have been build while it was not running
		it can also verify if a node from a previous build is not build yet and wait on it

	this makes sharing nodes between build much easier, there is no need to synch between pbs builds
		we only synch between build-nodes/being-buld nodes
		we can find the nodes we need via the node digest (is the digest of its sub tree needed?), including config

	
	
 
following variables defining link or compile flags
	from a node build, follow the changes done to a configuration variable (including where it was changed and why)
		this could be achieved with files so we can read them in vi
			or vi a query (that goes through the files)

insert color codes in the logs
	pbs2 generate monochrome data (logs)
	pbs1 generates colored node logs and adds some color tag in the pbs_md5 files

	the easiest way to look into logs is with ranger|broot|fzf and vim
		these show files in different "viewers" which often can handle ansi codes
		we need to generate those files from the pbs2 achromatic data

a very simple way to run in the debugger, including pbsfiles
	remember debugging DEFINED in cmake
		I would have liked to stop at the set
		execute it
		look at result
		run other set
		clear set
		write the current file modified
		continue building

	the repl can generate files, interactive generation of pbsfile, with localized runs

		psfile lines
			1
			2
			3
			current : modify current, ie insert multiple lines that will replace 4
				run the block till happy with the result
				possibility to undo, IE go back to state before current was modified
			5

		window to visualize what's going on

		the modified block must be run with a start config and target
			isn't the block close to a subpbs?

		we can keep the modified block and restart evaluation at previous line
			thus we need to keep a state per line to be able to undo

	we need to be able to specify which files to debug, simply
	we can continue running till next breakpoint

make pbs a normal lib so we can run in the REPL, including substraction of rules, going back in the graph evaluation ***, etc ...
	add rule
	pbsuse
	normal perl code
	set rules in rule set
	apply rule set to node(s) and get graph
	modify graph
	apply rules to node in the graph
	diff graphs
	check graph (without adding parent relationships as we me re use it)
	manipulate node group
	get build sequence
	build

including non automatic or very long steps in a build
	the user must agree on something being done
		that may be receiving a mail back with the agreement or directly typing yes

	check snake build
	no-build scripts, script that only whrite what needs to be done and wait for a key to press
		document the process
		partial script that automate some of the work (not different from the build system 
			calling a no-build script except that the process is self contained in a scrip
			it would be an error to contain the script in an external, non pbs, script when
			that can be achieved by writing the partial scrip in a pbsfile where taking a step
			toward automation is much simpler than having half the work in a script
			and half in pbs 

			of course we need to let the user decide but providing a framework and example help
			make a better choice
				the example show how to partial automate something and handle dependencies
				which is not a trivial step when going from manual to automated, particularely
				when declarative scripts are involved

what to do with large builds waiting for user input or a very long running command
	the build (possibly distributed) is taking resources in the form of open file handles, memory, ...

	even if not waiting, using resources is bad and should be reduced
		a flagrant example is keeping the whole graph and nodes in memory even after the nodes
		have been built.

tracking what build a file once it's moved from the build directory
	pbs node logs tell everything already in the build log
	manifest that can be searshed
	global md5 table of all node build pointing back at their build
		note that this is the same as a build cache except the search is reversed
			actual md5 to build instead for expected md5 (build) to node(actual md5)


	not installing but linking the nodes helps (and keeps them packaged)

parallel build systems
	if the warp file is split, each warp file can be run in its own build system/process
		if the start config and node for a dub build system is the same, the 
		sub build system generates its graph (which is righ if input is unchanged
			and delivers a trigger or not trigger for the top node
			no need to deliver the graph only the triggered node
				if its input is unchanged it can also start building immediately

		the sub build graphs are delivered on demand to upper level
			graph is node names only, the data is in the nodes_log/digest

		at the end of the run, global graph, component graphs, config data, ... is
		put together, rules could do that on nodes in the pbs layer (pbsfiles, configs, ....
		
	the complexity lays in what parts are run in a separate process
		once the build is set, the process should end and the cpu left to build tools

		this means that everything is serialized as there is no more object to talk to
		maybe we should never be able to talk to a service but only via files
			one advantage is that synch is done via files and files/processes can be on other nodes

		since a node information is serialized, including its dependencies, we could parallelize ad nauseum
			and let the nodes to be build synchronize the files they need, including compiler, ....

			the build node should keep the dependencies around as much as possible, ie multiple compilers could be kept
			on the build node
				some dependencies, IE: compiler, build themselves by installing themselves

			we could install in some sort of containers which would give the possibility to download parts of the install
			from nodes which already have those parts, a p2p solution is even better
				compare this with jenkins nodes where compilers are installed to avoid copy
				of course we can still install them and the node's build simply verifies that the compiler is installed

		we need to keep a weighted graph so the splitting is simple

	administrating the different nodes that compose the build should be trivial, like the log for nodes in pbs1, location is obvious
		CIT also has some idea about how to find a job which is not different from finding where a node is being build

		we could overlay the sub build fs, if it's on another machine, to give a local view
			could this work for CIT too?
				fs could be read only when the user only had read rights to the job, etc ...

		after running the build system to generate graph data it is possible to take a sub tree and 
			copy everything that's needed to build it on a remote node manually
				some trace information should be left so the automated build can try building on the node
				where me made the manual copy

			to find which nodes have a sub tree build data we can
				use a distributed hash
				query all the nodes
				ssh and query and check the nodes ourselves

				we're not sending GB of data just a few hashes so a simple solution is enough

				we need to trust the node since it returns binaries, pki?
				
				how do we debug with files on other nodes -> mount the other nodes
					but we need to lock the other nodes''s data so another build doesn't override data
					we could use an overlayfs layer to make the generated files uniq, other builds
					build in another directory which is another overlay

	remote node build also contains information about what started the build (and where)
		remote build may start another remote build
			some correlation ID must be shared as well as a system to collect error information from any node

build on an overlay, makes the output locked for a build
	other builds can still access the layer as read only if they need the data
		
	this smells like docker!
		maybe we should build in docker and each sub node/sub project can be build on it's own layer
		this would allow us to fetch builds from other nodes
			except that we want to keep stuff on the nodes  where it was build not bring it back: see local in pbs1

			we could also mount the other directory via a network fs or even sshfs (for CIT create a new user per build)
			
			given a local filesystem in a single blob (IE not the native fs but an extra fs) we wouldn't need to
			pack the files to relay them to another node, they are already packed, except we want to keep thing on the node
			where they are build :)

use bao to compute full graph md5s and serve the result
	also fix inotify by using an agent that works better than our implementation
		inotify should watch the directories not the files to save resources


pbs name of application + user info optional, via filter or option?
	via option --print_pbs --print_extra --print_location

	build: using 3 processes ...

	pbs: build: using 3 ...
	[some user text, eg: name of a build] pbs: build: using 3 ...
	pbs: @file:line build: using 3 ...

	via filter
		pbs: is added by the filter
		extra text is handled by a filter option not pbs option
		the line information must be added to the structured log

depend steps should create depend.log files in a directory structure just like build does
	we can still output the normal data (and the structured log)



directly show graphical representation of queries
	can be done via a wrapper
	query can output the file name to pipe in viewer
	sd can create a shadow structure

	use visidata for queries?
		can it lauch vi?

show access to config variables during run
	put them in the log
	internal variables and environment variables

	display which variables have not been used
		for a node
		for a graph
		for the graph
		
		tell where they are declared and used

		in a format that be queried, including their usage in sub trees

generate build manifests
	of nodes in the build
	of nodes triggering
	of rebuild nodes

	a graph that's easy to traverse for visual inspection
		sd | broot
		broot on _output_nadim

		the source graph and the output graph are not the same, we may need to overlay them
			sd links directories it doesn't link the nodes in a directory so we need to add that

		the node triggering in the source directory don't change names nor have config and extra data so we may need to 
		create extra data files in the source shadow tree, alt we can create a source node link in out directory

		it may be ok for a few thousands nodes but not for half a million
			we may need to create the links dynamically
				when looking at the tree

				when running a query that needs access to the source code in the shadow tree

					although all files are referenced in the graph

		

		although broot is OK, it's made for searches not traversal



	this is to look at the build at a high level

	to look at the details we need a graph and detailed 


	format of the manifests

		preferably tab separated, with column header


		query the graph
			preferred way to generate manifests
			we may need to keep a warp and complete graph
				or the graph is the warp graph and we load extra data on the fly

			the 1.8 warp graph is not even a graph anymore it's a cache lists to trigger nodes

		having the graph allows us to query more than files
			virtual nodes
			configs
			...

		user can write a simple printf as git uses in pretty print

		the query interface is a script using other query scripts
			this lets the user build her own queries on top of query modules
			the graph is loaded and the modules run on it

			using a standard graph query and traversal language
				we can still format our data as we want, we just need to proxy the language to our implementation


	query examples
		see above
		create a shadow tree matching the query
			eg: I am going to change this configuration in that pbsfile
				show me how the graph would look like starting at another node

generate a log for nodes as soon as they are
	added to the dependency graph
		from pbs1 implementation of this functionality
		1 # section below is disable
		2 # we could generate the node log info after each node depend but do it after the check step
		3 # that adds the check status for the dependencies
		4 # the best solution would be to add information incrementally, generate the node log info during depend (rules  inserting dependencies)
		5 # and adding the check information later
		6 # alternatively we could check the nodes immediately but that wouldn't work with late depend that delays the insertion of dependencies
		7 #◐◌◌◌◌◌◌we would have a wrong status for the dependencies

	build failed modes have a failed postfix

	the pbsfiles run
	the command line
	the environment variables
	
	everything that get on the screen
		for build we redirect stdout but we can't do it for the main process
			external filters can do that

user can grep/fzf/ranger to find broken nodes
	a file with the list of nodes can be generated -> give to fzf

graph queries
	find nodes that are broken
		generate a context graph

	find a node, generate a context graph

	find node(s)  common to two or more nodes

	find common pbsfiles
	find common variables, that changed, ... where (this needs config history)


	=> all this would be easier with external node data in a parser friendly format


config history in a file
	serialize the config, at the end of the pbsfile
		with history
		nodes can link to that history
		pbsfile can link to that history


	if nodes contain their configs, their parent, their pbsfile, and that their pbsfile contains links to its
	parent node, we can re-construct the config history without having to serialize the config object
		some information is lost, where the config was changed and why



file system shadow directories
	_output is exactly that except for
		virtual nodes
		pbsfiles
		config


log files can have different extensions depending of if the build succeeded or not
	LS_COLOR can be used to make the error files stand
	we should never have a build and failed log present at the same time
		except if the user wants to save the previous log, which can be done by the user

API
	time to think about an api to control the build system

	this includes better options which are a type of api

	REST? REST for command line

	REPL? isn't the debugger a better REPL?

use hashes to index dependents in the warp file rather than file indexes
	it's just slightly more expensive but we can find the nodes

an option turns line output for print functions/log functions
	we need to know what logged not only what is logged
		IE: PrintXXX 

finding build nodes from somewhere else
	root
		tools
			utils
				the_tool
		install
			the_tool
				which means that it needs to have it as dependency

		test
			some_test
				need to run the_tool
				which means that it needs to have it as dependency
				
				point to the tool at root/tools/utils/the_tool or root/install/the tool


	=> which means that it needs to have it as dependency
		a dependency is either
			inserted by a trigger
				some_test > trigger > some/node @ alias
				

			pointed at full path
				if it's not there we get a late dependency
					can late dependencies be aliased
						eg: root/x/z/my_tool is aliased to root/tools/utils/the_tool

					who makes the alias?
						x/t/my_tool

						anyone? anywhere?

					can we have many aliases to one node?
						what happens when aliases are already used?
				
					what happens to aliases that are never used?
						do we warn about them


			found in the graph
				need support from pbs for that
				does it even make sense?

for CI
	importance of logging the start time (and end) of a job

parameter to a target (or sub target, a la "just" program)

	default: (build "main")

	build target:
		@echo 'Building {{target}}...'
		cd {{target}} && make


	example 2

	backup +what
		scp what me@server

	$> just backup a.1 a.2 b.1
		scp a.1 a.2 b.1 me@server

	# not sure it's better than: $> make backup WHAT="a.1 a.2 b.1"


	just has three types of variables
		variable
		+variable
		*variable

		they can have default values



	$> just backup a.1 a.2 b.1


polyglot command
	in just, prepending commands with a shebang runs the commands as scripts

	roughly translated 
		with the exception of string that is not needed in just as its syntax looks like make
			and that it accepts a single command

		rule polyglot
			...
			builder
				<<EOP,
				#!/bin/env perl
				some perl script
				EOS
				<<EOA,
				#!/bin/env awk
				some awk script
				EOA

listing rules
	idea from 'just', --list doesn't list "private" target starting with __, grep -v' ^--' sound like a better idea

	pbs --list 
		rule abc (:line)
		rule 123 (:line)

	pbs --show 
		rule abc (:line)
			body of the rule
		rule 123 (:line)
			body of the rule

listing body of rule before executing it
	idea from "just"
	
	pbs --node_rule_body
		add the rule body to the node log, although we can open the node and jump to the definition easily 



using #! instead for autorun in pbs2
	#!/bin/env/pbs -f
		#rest of the script



default target
	works nice with make because the target is the rule (which is pretty nice)
	but pbs applies rules to the target passed on the command line

	we know which rules we have since we register them and could make the first rule match
		but match what? a regex?

		rule name => xxx, Match /all/

		a lot of things match that!

		rule name => xxx, Match /^all$/ | Match 'all'
		
		only match all

		first rule which matcher has a string as input?
			special code _just_ to allow a default?! we don't like default variable, config, rules, ...
		

	we can show what the rules would match, IE regex or string

	
	prf, is it needed?
		no but it's convenient

		it allows us to put all the special cases in one place
			default target (user target really)
			configuration variables
			options


passing environment on the command line
	other build tools have the environment as a configuration db
		this is not good enough, to audit config IE

	still pbs allows environment variables to sip through for commands that may need them
		pbs only keeps the variables that are allowed to pass
			this is still not good enough!

		it is not possible to completely eliminate environment variables because tools depend on them

		it's also very convenient to be able to set variables before calling a tool

		multiple actions are possible and some already done

			filter out the environment variable

			make all the builds depend on the same environment, IE: if a variable was set when a
				command was run, it must be set next time or the node is rebuild, env variables
				become first class dependencies

				this is means that the user needs to have control over the environment variables
				that get in the build environment, probably per node to build
					REH_USE_ENV

				since we have the graph before building, we can alert the user for environment variable
				mismatch (or tools version mismatch) before building.

				tool version should be done via dependencies on the tools version instead
					can also be used for global variables we need to be set

		where do we filter the environment variables and how do we report them as we could report configuration variables

			per pbs process, and pbs forked processes (who inherit them)
			per pbsfile
			per pbsfile run, they can be passed down (by whom the parent node or the parent pbsfile)
				how does it work with pbsfile env filtering, who takes over?
			per node

		reporting environment configuration means keeping track of it
			do we keep it in a separate configuration object?
			can it be on a layer in C::Hierarchical?
				with what priority?

		whats the relationship with the pbs config
			are $CC and %CC the same?
				no because we must have a change to control the configuration at the node level

				but we also need to be able to override config from the command line
					that would work if env variables where on a layer in C::H
			
			are the environment variables put in the config when pbs starts?
				do we put them in all the build environment automatically

			do node handle environment variables in a special way or through pbs config

		if a pbsfile filters out environment variables, what if we let them through?

			shell: 		a=1 b=2
			pbs:		--keep_environment: a ...
			subpbs:		pass limited environment
			pbsfile:	filter_environment: b, must have c
						must have is an interesting concept, it can be verified when the rule is added
						
						it would be good if all this was through specialized REH in pbsfile_run rules

displaying/loging configuration
	on the command line
		??

	display config event when they are "special", I.E.: an override,
		shall we also display the normal variable setting/changes in the log/output?
			we prefer totally silent output with options to swith on specific output
			the total opposite of make (almost, all goes to logs instead for stdout)

	config is node specific
		if a node doesn't have  any specific config it gets it from the pbs-run node and it then
		becomes the nodes specific config.
		the pbs-run node config is read only for nodes, only the pbsfile run can set configs
			inherit from parent pbs-run, filter, set, ...
			the config is kept in the pbs-run-node  log/digest

	every node had the config in it's digest/log
		not all the config are part of the value included in the signature but the config is listed

		this also applies to pbs-run- nodes, ...

	is the config also a node ?
		if it is it can get verified and trigger 
			on any or specific entries
	pbs can save a pbs run config, shall we also save an environment config or should they be equivalent

node build commands
	pbs 1 can run multiple commands or pel scrits but there is no relationship between them, once
	can even set a config from one build step to the other
		note that pbs2-proto hands the node so there's a place to store data to be moved around
			that could also be in the builder loop
			or the command can do it on its own, like a shell command does



../../../something
	explain . and how to use it
	explain full path and no relative path
	catch ../..
	how does not having relative paths help?
	triggers in higher level directories (should be accessed from the machines root)
	how does full path impact reuse of artefacts in other builds
		can someone else reuse the builds if they are rooted differently
		C depender has full path and that stopped re-use
			a good thing? and what's the run time impact
			bad?
	how does one get a node path in a builder (and why would someone want to do that)

pbs_run rules
	rule name 
		is rule a REH?
			it takes a name ar argument and a list of REH that it registers?
			there's not reason the registration would be done by pbs when a REH can do it
				suubpbs are already handled this way

	rule name
		MATCH_PBS_RUN #is this the same as matching a subpbs rule? no there are two phases, start subpbs, be the top of the subpbs
		MATCH_TARGET
		FILTER_ENVIRONMENT (to subpbs, how about in the subpbs)
		OTHER_REHs

	if they have rules they have builders
		good place to sneak in warp distribution

	should the pbs run builder run the target builder
		that includes loading the subpbs (how do we do in the pbs2 prototype)


parallelization of depend step
	fork?
		on node
		on subpbs

	fork worker pool that can be controlled from the main process?
		
	we should stop handling nodes as objects in memory that reference each other and start handle them as 
		a text description that can be loaded on demand
		trees can be much smaller as they just just keep a reference to the files to load
			there will be an impact on performance but it may be small

	we can use the file system as a locking mechanism to know if a node is already in the graph or not
		being careful with nodes that are already serialized on disk from another build

		create the node, exclusively, first process creates it, other process use it
		gmake used a trick to control how many parallel process it runs









difference between config variables and environment
	should there be a difference



add section to elf file with pbs config for the node

nodes can be prioritized against each other to make them build in a specific order
	priority is a set via a REH as a node attribute which is used by the scheduler
	this is for nodes that are not dependent and only to influence the scheduling

nodes can catch/and/or forward that sub nodes are being build and thus that they are being built.
	this generates a lot of signals and catching
		the nodes who want to catch a signal have to register. 
			add_rule ...
				REH_CATCH_DEPENDENCIES_BUILD_START

nodes can add a notice to pbs information displayed when node is build
	print_info (maybe 'notice' is a better name) does just that

node can display information in the node log (which is also displayed on the terminal)
	REH_NODE_INFO can insert any amount of data, since it's a REH it can register to be called at different levels (depend, insert, build, ...) and
	thus insert information in the stream at different levels.

REH_NODE_INFO can be used in a rules for a subpbs, a node, ...

REH_xxx_CONFIG can insert data in the information stream
	like pbs1 inserts warning on the screen except that these warnings a structured

nodes can take over the normal pbs terminal output (which is normally the stream output)
	a rule can display its build differently
	a pbs option disables this behavior if necessary (debug)
	everything happening gets into the logged information stream 
	the fact that the information stream is changed is also put in the log

the node log file name is inserted in the information stream so it can be filtered out or opened directly from the terminal
	a related problem is finding out the build order
		it's sometimes the source of problems, mainly because dependencies are no declared properly

	we log the order of build but there's a difference between having a log and using it
		problem #1 is the amount of data
			filtering
				depth
				matching nodes
				matching pbs runs
				amount of data displayed
					each in own sections and can be filtered
			folding
		
		we want to know the build sequence of nodes that are dependencies
		we want to limit the depth of the dependencies shown
		we want to start from vim or the command line or a repl (although the repl should be dumb just running external script)
		we want to look at configuration for the nodes 
		we want to know when the configuration/rules where added/changed
		
		we want to see the pbsfile run for a node, that log entry should be in a pbsfile_run.log and linked in the nodelog
		
generate a list of nodes that have changed and a list of node that trigger (changed nodes and triggered nodes), the list contains the node log file path
	the list can be added to the information stream
	an option --display_triggered_nodes_list, displays the list and end the build, except if --force-build is active


caching tree for fast build
	fast build in this context means
		fast graph generation
		fast checking of dependencies
		fast determination of the commands to run and in which order
		not fast compilation 
			although finding where the sub graph could be compiled the fastest is part of the build 

	no build case, where nothing has changed, is the least interesting case
		not even interesting to compare build systems because it's, well, a stupid case of no relevance

	minimal build change is much more interesting and changes can be of many types
		deepest file
		file with most dependents
		pbsfile
		configuration change with different impact on the build
		...

	fast cache loading
	fast cache checking
	fast graph generation/regeneration
	fast dependency check
	fast command determination/order
	fast distribution
		that's the order pbs1 uses and it is not optimal
		
		if a graph is made of sub graphs (mini warp files) checking the micro-warp
			can be done in parallel
			only triggering micro-warp need to be merged to the global cache (maybe), the other just need to give the top node, un-triggered to add to the graph

		the libs and pbsfiles are checked first, if they are dependencies of the nodes then node checking does the same in a more generic manner

		the microwarp can be handled by an instance of pbs, which means that it can start building immediately
			the instances of pbs, and their result, must be orchestrated

			if the micro-warp is on another build node, the pbs instance can be run on that node achieving distribution

		the size and topography of the micro-warps can be computed to be optimal for the build node

		not checking intermediate nodes is the default, specially if artefacts are in a read only/pbs only filesystem
			an option forces the checking of all the nodes

		fast cache loading can be achieved by having less data in the cache, it can point to  the data in other files (log file, so we have the data in a single place)
		graph regeneration is to have data that may be used in display (IE: insertion line) 
			 we often set it to N/A only so it's not undefined and generates perl error when accessed

		command determination means re-loading the pbsfile but if the pbsfile wasn't changed then it's the same command as before

		the build order is the same as before and we can re-use it, it's just that some nodes, the one that didn't make it in the graph because they were not changed, won't be in the build sequence

		

	one case not handled optimally by pbs: only source change

	    since we have a dependent list we could use it to generate the build sequence instead we reload all the pbsfiles down to the change source and recreate the build sequence from scratch. It is not sure a more optimized rebuild is better for the code or the comprehension of the build but this is certainly less efficient even if it is really fast.

	to rebuild only the node and it's dependent we need to recreate the code that created it. Best would be to serialize it and/or keep it in memory

	the node may be build by a shell command (and a shell context, environment, installed version of toos, installed libraries, ...) or by calling a perl sub. the perl sub (it also has a context, closures, ...) transforming the code into a simpler "call context" may allow us to write an new code just to build the node in effect this transforms the sub into a shell command where perl executes a simpler script

	Rule ......, BUILDER => \code_ref, args

	writing node_builder.pl which contains the code_ref source code makes it possible to call it as an external command
	this could be achieve with a special code definition for builders, either by tagging its beginning and end or by making the code a string
	some perl modules serialize subs, they may be an even simpler solution, at least more transparent for the user

	even if we can serialize the code, we still need to handle errors in the same way as a full pbs build
	serializing the code makes it possible to build a node without having the build system involved, not sure what advantage that could be and calling the code through the build system is simple, simply build that node and only that node by running pbs on the node's pbsfile



debugging
	it should be easy to debug a build that runs command in the command line (and subs)
	if a node is build with shell commands, pbs should shellout for every command and give the control to the user
	the command, and other pertinent information should be written in the terminal, after commands are run the user can continue the build either
	by returning in the parent (pbs) shell or by sending a continue command and stay in the shell
	the user should be able to open multiple shells for nodes to be build (or no node if necessary but that doesn't need to be done via pbs, tmux is fine)
	this means that pbs needs to listen to a socket for "continue" commands
	how is this different from gdb? multiple terminals because we want to keep output separated
	should we implement gdb on top of pbs or continue with perldb?

	if snapshots of the files to build are done before building, we can trace back
		the user should be able to snapshot more than the node to build
		pbs need to snapshot its own data
		every time a node is build, a snapshot is made, which we get commit per buils

	git merge with conflicts can be seen as a template for this kind of commands line options (--continue, ...)

	on error create a file to open in vim with all relevant links

	on error create a shadow directory with links to
		failed node log
			dependencies log
		relevant pbsfiles
			their dependencies
		directory with saved config and command to run in that directory (while keeping everything at its place)
			commands to run with different types of debugging switches
		link to the whole source tree (since this is only a shadow directory with specific files)

		
	
handling of bulk dependencies added by a search function, eg: find all the libraries in a directory, makes debugging difficult
	because the dependencies are not named and grepping for their names gives no result, this forces one to grep through
	 the build node/run a query. Note that the biggest problem is a CM problem.

warp with reduced data
	to load faster, it instead points to full data for nodes, this allows us to do queries without having to reload pbsfiles.
	node data can be in microwarp, node digest and node log.
	microwarp should point to data not be data. 

a job in a CI system can be scheduled at any time, we need timestamps to see which one is started first
	a job that is depending on another job will always start before the dependency
	dependencies should be on artifacts not jobs! that a job created it is not relevant

	a CI job is an administrative process over a normal build
	but that's wrong, except the scheduling there should be not difference
	maybe a ci job should be a job, which generates artifacts not virtual "I build the latest"
	node builds must have information, time started, duration, in which context (build number if relevant, setup, config, ...)
	since some of the information is shared by many nodes it should be in configuration nodes that are serialized (looks like pbsfile nodes),
	linked to from the node build or serialized in it. if not serialized, on top of an url to the config/set/node, a md5 is kept, after all its a dependency too)

Telling a build story
	top down build systems are not always described top down, an example is setting up needed files before telling who needs them,
	and the importance of CI being driven/verified with the build needs; or distributed and all CM data gathered in a CM manifest

	by telling the story top down (possibly in different description files) 
	how do we define in a flat file or a series of flat files a build which is a graph and not a list in the best way? while keeping the build story.
	build system may need more comments if the build definition itself doesn't use a vocabulary that's clear enough, what makes a good book?
	links, bread crums, history, bookmarks, history from a bookmark down, scope+folding

	reading the story in the build description files is goo but we'd like to have the story readable in the graph and in the log while building
	in the graph means that chapter in the story are also node, with dependencies. Those can be added as virtual nodes, although sometimes the fit an intermediate file well. The virtual nodes also have a builder and thus are visible in the build log, they can also display extra information since they have a builder

	how to add dependencies to a story
		often dependencies are in a list and the  list is added to the dependent's dependency list, ach dependency then match a rule which creates a node
		in pbs1 a virtual node can be added, or a non virtual node

		in pbs2 the same mechanism exists but we can extend rule with specialized REH

		add_STORY_dependencies("the story", dependencies)

		the above REH can, create a virtual node, and/or, add information to the dependency nodes (using rules?)

		The REH can, during the nodes build, display information as it could also be a builder REH

		alternatively the story can be added to the dependencies
		Rule X, dependencies(), comments, belongs_to_stories(one or more story names) # I prefer dependents to set that information


		A story resembles more a container than a dependent, there may be a better way to describe that (a scope for example, which could be a name space at run time), a use case with description and output is needed

--node_environment
	display the environment variables active during the node build
		maybe in a separate file to not spam too much in the terminal
		=> in the log

	how do we control the environment variables per node?
		we don't, the only thing we control is $ENV at the start of the build

	for pbs2
		control env per node, per command in the node, per phase in the node?
		inheritance of env from node to node
		package env is default for nodes
		env should go in the digest

warp
	having unbuild nodes in warp is moot
		the node will build anyway after being removed and first revivified
			can we have only the build nodes or do we need all the nodes in the type of warp we have
				unbuild nodes trigger other nodes as their digest is not found
				warp file nodes contains dependents so we need the nodes
					but we do not need to check what we know will fail (because it is not build yet)
						check if MD5 ew 'not_build' rather than compute a hash

	warp can be prepared in the background as soon as the depend starts
		if the processes are kept synchronized as the new nodes do not appear in the forked worker
		yet another reason to keep everything textual so it can be streamed over

	it can also be prepared parallel (parallel with depend and build) (if forking is not more expensive)

	warp, after depend/build can be build in parallel as the graph is static
		rather than returning warp nodes, the workers could write file on disk and
			their format could be "cat"-ed together

	warp nodes should be used as is to reduce time transforming then back to warp nodes after revivication
		EG: no fiddling with the node data when serializing and de-serializing

remove package dependencies by matching them directly to nodes
	from pb1 code
	# add package dependencies to the node
	# package dependencies is sugar, we need to assign it in nodes to avoid triggering the whole
	# warp graph for a change that only impacts a few nodes


parallel depend and post build commands
	pbs1 runs all the depend in the same process/thread
	the node definitions belong to the main process till it's forked
	the node definition can be assigned a __POST_PBS_BUILD_COMMAND which is to be run after the build and before the warp
	__POST_PBS_BUILD_COMMANDS must be run in the main process

	if we depend in parallel we need a mechanism to synch the node definitions back to the main process to run __POST_PBS_BUILD_COMMANDS
		and to synch the nodes between the dependers

late dependencies
	check can't be done for every node after depend

	the depend and check step are separated in pbs1
	we can check in parallel in the check step but can depend a node and check it immediately because of late dependencies
		check is already done in parallel in pbs1

	late dependencies are dependencies that have no matching rule while depending a dependent, a separate part of the build system
	depends a late dependency making the graph valid

	if we were to check immediately after depend, the late dependency would not have sub dependencies (yet) and the check
	would be wrong

	late depend may be theoretically wrong
		the idea is that a sub module needs a dependency, say a library, it doesn't know nor care how it is build
		the top level over the sub module can decide to have the library as source or as a node to be build
		
		the right approach would be for the top level to add a depended library before calling the sub module
			and the sub module should fail if it doesn't exist 

		say the library is build by sub module 2 and needed by sub module 1
			with late dependencies it just works

			other wise we need to make sub module 1 depend on the library directly
				this is usually done by installing the library and configuring where it is so sub module 1 finds it
					this means building sub module 2 first rather than making a single build

				in any case the library must end in a location that sub module 1 knows, we can decide it's in ./libraries/lib
					now need to make sub module 2 put it there
						can we alias the nodes, before they are even depended?
							the top level can insert a node that's not depended in the graph

						ALIAS is used for the "target" of a subpbs run 
							we can't alias a specific node 
								well we can! inserted_nodes{alias} = inserted_node{whichever}
								but that's perl code not pbs code, not user friendly


				3 cases
					the library is installed, we want to point at it
						we want to make sure it is installed
							the node needing it would fail if it wasn't there, no rules to make it and not present on disk

						AddRule all => sub module needing lib
						AddRule sub module needing lib => ./libs/my_lib
						
					the library will be installed, we need to make sure it's installed first 
						IMMEDIATE_BUILD
						we can still use late dependencies

						AddRule all => ./libs/my_lib, sub module needing lib
						AddRule IMMEDIATE_BUILD ./libs/my_lib => ./sub module 2/path/path/lib
							ln  $dep1 %FILE_TO_BUILD

						AddTrigger sub module 2 => ./libs/my_lib 
						
					the library is used where it is build
						alias

						AddRule all => sub module generating lib, sub module needing lib (order doesn't matter)

						# if both modules already agree on location, we're done, late dependencies handles is
						# if not we need to point at the lib

						AddAlias ./libs/my_lib => sub module 2/path/path/my_lib # add both nodes to graph if necessary

						# nice, no need for installation

						# We could also pass the location via a configuration variable bu we would have no node in the graph!


history browsing 

	the time line is not based on when a node is build since the parallelization may schedule nodes at different times between builds

	note that it is a single build that is interesting but we should push the user to understand dependencies rather than build time

	at time zero (t0) we have a lot of nodes that are terminal and can be build
		that is the list of nodes that are of interest at t0

	that specific node is build at some specific time is of interest only if the dependencies are wrong

	finding a specific node, or specific nodes and being able to follow their build chain is more interesting than WHEN a node is build
		it is possible that we want to follow multiple node
		see what they have in common
		where in the graph they are relative to each other
		if they are parts of each other dependencies
		grep for dependencies and dependents (on command lines is even better than REPL)

		we must be able to generate a list of all dependencies from a node, recursively till we find terminal nodes
		generate the path from a node to another, displaying the information about rules that inserted each node (and a link to the nodes log)

	it is completely useless to have a REPL when the command line works better and is more flexible

	having the build information in the log is enough to follow the node build (from vi for example)

	the build time, build result, error due to sub dependencies not building, and which one (can be computed instead) is available in the log.

	pbs 1 stops as soon as a node fails building, it should continue generating logs for the nodes that have failing dependencies
		that's the equivalent to dumping the graph 
		can take a long time if 10s of thousands of log files must be generated
			can generate the file while depending and add data to the file (making it a real log)
				pbs1 has an option for that but it is not the same file as thee nodes build log
	 
	an old pbs1 requirement was to have the node in a database or in files rather than in memory, it seems that requirement, meso warp, and history are implemented the same
		the rules to generate the nodes, their config, …  are themselves in files (the pbsfiles)
		nodes should have a pbs package information, pbsfile, target, start config, so it is possible to regenerate the node un memory as when global pbs is run
		all nodes in a package should point at the pbs package file as it is identical

	?should we keep the log and the digest together ? 

	We now have a lot of files that we use but don't verify, should the log have a digest or a way to be verified?


	use cases:
	file build failed, understand why

	understand dependencies between files when a build fails because of a missing dependency
		previous build succeeded now fail because a dependency has been removed that is needed


File system as the graph
	After depend is run (during?) It is possible to search the graph (grep, fzf, …) and go through it (cd, cat, find, ranger, vim, …)


pre-build warp
	when is it generated
	why are all pbsfiles re-run when   pre-build warp done but before warp generated

	see Meso warp


--node_environment
	display the environment variables active during the node build
		maybe in a separate file to not spam too much in the terminal
		=> in the log

	control env per node, per command in the node, per phase in the node
	inheritance of env from node to node
	package env is default for nodes
	env should go in the digest


text displayed with terminal control characters
	in pbs1 a control character is used to erase the previously written text and write over it

	some text will not be in the terminal because of this and that may make the debugging more difficult
		output is structured in pbs2 so everything will be in the log and it is the display plugin that is 
		responsible for using control characters
			eg: add control characters or not, pbs2 never does

long text display
	pbs1 use elision in some contexts
		--dd
		name of target of subpbs
		...

	sometimes we want the full name of things
		elision should be an option
		elision in some context only
			eg: --dd 
		elision on some paths only
			or not on some paths

graph display
	in text mode we need to be able to color the glyphss to show where they come from
		triggered nodes
		warp nodes
		merged build nodes
		...

	text mode graph doesn't show triggered graphs!

Information trees
	always present the information in the most logical order
	
	IE: displaying a trigger rule
		DEPENDER is of no interest
		NAME is not first

		trigger rule:
		├─ DEPENDER = CODE(0x558e27765d48)  [C1]
		├─ FILE = ./Pbsfile.pl  [S2]
		├─ LINE = 16  [S3]
		├─ NAME = T1  [S4]
		├─ ORIGIN = :PBS::Runs::PBS_1:./Pbsfile.pl:16  [S5]
		└─ TEXTUAL_DESCRIPTION  [A6]
		   ├─ 0 = X  [S7]
		   ├─ 1 =  blessed in 'Regexp'  [O8]
		   │  └─ REGEXP = (?^:^\./y$)  [S9]
		   ├─ 2 =  blessed in 'Regexp'  [O10]
		   │  └─ REGEXP = (?^:^\./b$)  [S11]
		   ├─ 3 =  blessed in 'Regexp'  [O12]
		   │  └─ REGEXP = (?^:^\./bb$)  [S13]
		   └─ 4 =  blessed in 'Regexp'  [O14]
		      └─ REGEXP = (?^:^\./z1$)  [S15]
		  
pbs has many phases
	verify warp
	depend
	check
	build
	generate warp
	post build

reloading rule does take time
	Depend: pbsfiles: 100, time: 1.67 s.
	Depend: nodes in the dependency tree: 1901 [W:0, R:1901]
	Check: total time: 0.09 s.
	'PbsUse' statistic:
	├─ /home/nadim/perl5/perlbrew/perls/perl-5.28.1/lib/site_perl/5.28.1/PBS/PBSLib/Builders/Objects.pm
	│  ├─ LOADS = 100
	│  └─ TOTAL_TIME = 0.034684
	├─ /home/nadim/perl5/perlbrew/perls/perl-5.28.1/lib/site_perl/5.28.1/PBS/PBSLib/Configs/Compilers/gcc.pm
	│  ├─ LOADS = 100
	│  └─ TOTAL_TIME = 0.107334
	├─ /home/nadim/perl5/perlbrew/perls/perl-5.28.1/lib/site_perl/5.28.1/PBS/PBSLib/Rules/C_depender.pm
	│  ├─ LOADS = 100
	│  └─ TOTAL_TIME = 0.078792
	├─ /home/nadim/perl5/perlbrew/perls/perl-5.28.1/lib/site_perl/5.28.1/PBS/PBSLib/Rules/C_EvalShellCommand.pm
	│  ├─ LOADS = 100
	│  └─ TOTAL_TIME = 0.064047
	├─ /home/nadim/perl5/perlbrew/perls/perl-5.28.1/lib/site_perl/5.28.1/PBS/PBSLib/Rules/Compilers/gcc.pm
	│  ├─ LOADS = 100
	│  └─ TOTAL_TIME = 0.054585
	├─ /home/nadim/perl5/perlbrew/perls/perl-5.28.1/lib/site_perl/5.28.1/PBS/PBSLib/Rules/C.pm
	│  ├─ LOADS = 100
	│  └─ TOTAL_TIME = 0.445084
	├─ /home/nadim/perl5/perlbrew/perls/perl-5.28.1/lib/site_perl/5.28.1/PBS/PBSLib/Rules/Object_rules_utils.pm
	│  ├─ LOADS = 100
	│  └─ TOTAL_TIME = 0.05924
	├─ TOTAL_LOADS = 700
	└─ TOTAL_TIME = 0.843765999999999

	=> let the user decide if rules are reloaded or re-used
		PbsUse vs PbsUseNoCache
		or global caching policy via a cli switch

trigger rule import
	in pbs1 an ExportTrigger is defines in a pbsfile, ImportTrigger looks for the function and calls it
		this has the advantage that we can do something special in the ExportTrigger sub
		
		declare rules as EXPORTED and let pbs create the ExportTrigger sub or simply scan the rules for an EXPORTED rule in ImportTriggers
		Exported rules are not called during depend if not imported

fzf
	completion via fzf, showing the help text
	--options which can be redirected to fzf
	documentation via fzf
	replace documentation module with fzf

layers
	warp nodes could be on their own layer
		this allows us to display graphs without the warp node
		we'd like to show the nodes that are directly linked to the new  graph (during warp)
			those are linked nodes

		we can generalize the system and make it possible to decide which nodes go to what layer
			via REH

			when displaying nodes from one layer in a graph form, we need to compute the minimal set of nodes  from other layer that are needed to make a graph

			? can nodes belong to multiple layers
				would be nice if we want to merge builds

	layers seem to be an attribute of the node (like linked or warp_node, ...)
		difficult to know if layers should be handled as layers or node attributes
		we can also add the node to a layer (push @layers, $new_layer)  even if the nodes have a layer attribute!

	can a node belong t multiple layers?
	do the node, existing in the different layers, be different from each other (aka just their name is the same, an other attributes similar is OK too)

how to show dependencies on pbs libraries, environment variables, installed tools
	if they were all dependencies to each node -tno would become unusable
		this happens already in pbs 1 when nodes are loaded from the warp file
		which puts them as dependency for each node to trigger in a better way

	we could have a "lib/env/tools" node, so we'd only see it once

	they could be in another layer of the graph
		we can chose which layer to show ... per node!


trigger start another build and makes the current nodes graph available
	importing graphs doesn't start another build

	is the order of trigger handling important
		if yes, how can we do that if we don't have any order for the normal dependencies?

the global build cache links back to graphs
	the idea was that each node get a UUID that completely defines them so other builds could use the previous build
		(it would be nice to have the attributes for th enode so we can make a partial matching, eg: some node but different compiler versions)

	if the global node also had its graph, we could load its graph in the current build


REH can add dependencies named dependencies
	the REH register itself as a depender and something else

	Rule something
		Matches
		dependencies => a b c
		named_dependencies => Frank => 1 2 3

		Builder => cat %NAMED_DEPENDENCIES_FRANK > %NODE_TO_BUILD
		Buuilder_two => some_file # some_file is  dependency but it's not declared as one

when building only one node a node number and percentage is added to the log
	why is here a difference?

log contains links to other files
	node dependent trees should contain link to dependent log so vim gf can open it
	also add links to dependencies log
	or any other file that is interest

log contents
	log build command
		so it can be run from vi, in terminal
	file names so they can be opened from vi
		libs, pbsfiles, nodes

node info generation
	can the generation depend on the pbsfiles md5 and thus may not need to be generated if already ok?

md5 request for C dependency cache are not counted in the md5 stats

assign color and level of verbosity to all the output classes
	we use colors right now
	to  support --quiet directly in the PrintXXX functions
		per class

	PrintXXX should be Print(XXX, ...) where XXX is the class corresponding to what is being done
		the class is set by the context instead for forcing the context to use an existing class

		instead for : PrintInfo(sprintf("Warp: total time: %0.2f s.\n", $warp_generation_time)) ;
			Print [class, class, class, could be a single string], ....) ;
			Print [WARP, TIMING, GENERATION], sprintf("Warp: total time: %0.2f s.\n", $warp_generation_time)) ;

		if the class doesn't exist, it's displayed in a color that makes the developer/user know that it needs to be set

		classes can be set from the command line, in the prf
		non_set class used to display warning can also be set, it's just a class like any other

	classes can be set during apbsfile run, in the pbsfile or by the parent



distribute the warp generation in the graph
	node generate their warp data
	pbsfile runs collect their nodes (who collects linked nodes)
	pbs collects the pbsfile runs

	leaving the data separated allows use to parallelize the reconstruction of the graph
		the reconstruction creates files containing reconstructed data (checked)
		
	it seems possible to distribute the creation via a warp rule matching everything

	the reconstruction also seems to be possible via rules
		actually a pbs run which creates a graph of already build nodes, to depend and build nodes

	another advantage of not collating the warp file is that distributed sub graph builds can keep their warp cache local
		warp can be returned as a sub graph with references to remote build system for the build
	

in warp nodes that are not to be rebuild are re-constructed
	we can't see which nodes are re-depended (only linked) because their package is loaded
	this information is interesting to show how much work is done in loading the pbsfiles that could
	be avoied if the node were not all together
		although the only work involved is linking
		visualization of re-constructed, re-depended, re-build could be interesting (although not as much as how many and where in text form)


Generate a log (like the build buffer) for nodes not build yet
	=> implemented as --lni in pbs1


	when a node fails, we may want to look at the dependents and their configs

	warp doesn't keep the information so -ni can't show them
	-ni shows all the nodes that are in the graph, not the nodes not yet build
		and easy option to add and a good information to show even when all nodes are displayed
		=> but -ni/-lni will display the nodes that are left to build, which is better

	even more reasons to distribute the warp data to meso warp
		?? should the whole node data be serialized?!
		including reverse dependencies?!

	the node data should be structured so it can easily be manipulated
		but how do we look at the build log in a text editor without the need of an external program
		write the external program!
		first section of the node data is the build log, as displayed

	what about pbs data? eg: pbs file chain

	=> practical after an error
		either generate with -ni manually 
		or when a build fail generate a node info for all the remaining node
			including which dependencies failed (this is only valid till the node is rebuild)

failed build should also be logged in a file

build priority
	node with most parents has higher priority has it may keep more cpus busy

	depth first vs width first
		depth build parents are much as possible, width builds children as much as possible
		cpus allocation is as high in both

build sequence, build triggering 
	in sequential build (-j 1) we use the graph to build a build sequence
	in parallel build we build a data structure where the children success build updates the parent till the parent children count to build is zero, we then start the parent build.

	using build event
		the build event is generated each time a build is done or by the check step if a node is already built
			=> this is already how pbs 1 does it!
		the event can be local or can be generated by remote build nodes
		or a separate builds (when we dynamically merge concurrent builds to reduce the number of builds).

		nodes waiting for children builds register themselves till their children build count is zero (and then de-register themselves)

		Note that this not different from the data structure (modified graph) that pointed to the parents
			we have moved the information to a different data structure, one that may be simpler to split and distribute

		Note that the data structure (node pointing to dependents) is a warp 1.8 equivalent
			each node child list should be serialized separately and the warp file generated from them

		Having the data serialized allows a more flexible management of the nodes
			The builders of the nodes may be functions that are not easy to serialize or which serialization takes too much time and space
				but we know where the builders come from! The pbsfile run with its initial configuration (given no rule are inherited)
					given that we know the dependencies, the config, the pbsfile, we could send them to other build node (having the same setup)
						for build
						we could also run the build system on the build node, so it has the same data and have it wait for a node name to build
							parallel builder does that except it is on the same node, distributed build need the same
							information, either by receiving it or by running the samebuild system

						=> cd project ;  pbs target --as_build_node ; cd - # poor man distribution! this how jenkins works, lol!

				we can recreate the rule for the node to build when its children have been built and start its builder
				pbs doesn't even have to be active, only the build phase
				no graph needs to exist
				This becomes a coordination problem rather than a build system generation problem.

		it's easy to pick a node to build, any one with child count zero
			we move the nodes to a location where only nodes with child count zero exist
			having multiple locations for nodes with different child count

		how do we handle build success and failure, and feedback?!  
			feedback is easy, we output a stream of information to the same place pbs would
			success doesn't need to be handled
			failure is handled by the event manager (just like in pbs 1)
				stop building, give feedback

			how do we handled nodes that build even if their dependencies fail
				REH_ON_ERROR, works because it's part of the node build

				REH_ON_DEPENDENCY_ERROR is part of a parent node which build may never start if nodes fail building and stop its scheduling

				What about generating a build_error event and pass it to the parents immediately
					what if parent has multiple children, what do we do with the children still building when a build_error event reaches the parent?
						the event handler can wait till all children are build, no_stop in pbs, before sending all events to parent
						the event handler stops all the node builds and goes in build_error mode, another build of sort
						the event handler passes the build error, when another child builds ok, the event is not propagated to parent

node log
	generated when the node is build, what if we want information but not build?
	
	we need to generate a log continuously and add the build info when building

	redundant info is OK if it is legible 

config node serialization 
	node have a digest and a build log
	configs have themselves and a history

	nodes point to a config, package or node config, the node log should point to the config log

nodes shared from cache 
	can be shared via inode linking, if multiple local build need them
	nodes are present only if needed locally
	the nodes to build could be put in a common directory (in a structure hash0/hash1/...) and the current build has a link to the node
	locally build nodes stay local but are visible in the global cache
	the cache can be remote
	nodes are identified by their digest
	no location information in the digest
	intermediate files stay in the global cache
	dependencies to needed nodes stay in the global cache
	cache entries, and sub caches, can be added an removed at any time
		knowing that a node existed but is not available can be useful for stats and cache strategies
	registering the local build generated node to the global cache is transparent
		local build put nodes in a local cache which can be made part of the global cache
			(oops artifactory but transparently)
		local cache connects to global cache, depending on local config)
	add metadata to cache nodes?
		retention time, build info(hmmm, njaaaa), ...


WAF: good presentation of what an automation framework does

build command (pbs) install:
	build command is an executable with no dependencies at all

build start directory
	 where is the build started
		what if I am in another directory?
		what if I want to build sub modules? from another directory?

DOT the root of everything
	. (dot) is the root of everything, what if I want another dot in sub modules? 
		aliases to depend a node that's named something else

context per rule, pbs uses a package context but can do a node context with node specific config
	how is node config inherited, should it?

function calls directly in the rule definition, possible but weird since the rule is not run when defined (except if the function is to create the definition)
	if the rule is a function definition (not a call to register it) then code can be inlined in it

removing generated files
	pbs doesn't at all but a user may want to know what's generated and what is source, and maybe implement "clean" on her own
	the graph doesn't even need to be traversed
		node_list contains them all, build_directory points at them, and IsNodeDigestGenerated, tells us which nodes are sources
		it's up to the user to list which nodes are to be kept, copied, run, ... attributes set on the nodes, via rules and custom REH, gives the user full control.


problem encountered with cmake
	guessing what and how things are installed, configured, or where they come from, is a headache


release/install target 
	the user tags the nodes to be part of a release
	the user adds a new global "release" rule at release time which uses the above tags (or decides itself which nodes to release/install and where)


	installing targets (different kind of targets)
		a node, created by a rule, that is depending on the build node, it's builder traverses the graph and copies (or whatever) the tagged nodes to some install directory. Since this is a user defined rule, there may be different rules that install a file in different directories. We want to know why a file has been installed. Maybe the best is to let the files install themselves instead for a top node doing the installation, the node installation can still be triggered by the top node but the lower level nodes need to match a rule to do the intallation, that rule we can track, that is the installed file itself is the target of a node

	tracking installed nodes
		rule, match A, install somewhere/a mean that a node install depends on node install/A that depends on node A
			the right dependency chain but a headache to implement and a nasty choice to make
			install depend step must wait for build step to be depended before it can run or
			install has a cache (like object files), if it doesn't exists install is run and the dependency cache is generated at build time by scanning all the node in the graph
		it's nice touch and uses the same mechanism as other types of nodes or
			the problem is that all the nodes have to be scanned, scanning 30_000 node to find 3 nodes is not very efficient, specially if the nodes themselves know they have to be installed, it would be better if those nodes registered themselves for installation (note that this doesn't exclude a dependency cache), but how do we register nodes for install when we want to use rules and that the install node was already depended with no dependencies as result. We want to re-depend the install node! with the rule added by the --install switch in the dub modules. I believe it's ok to redepend a node to add a dependency while we are not building yet (although that case is possible and should be taken into account), we already have nodes that exist in the graph and that have not been depended, those nodes can be depended later in other sub modules (late depend, a mechanism where we say I need node X and node X pops up later (hopefully)). there is little difference with Install node saying I need node ?, and node ? popping up later.

	install
		ask all node to install themselves with some configuration passed by the install node or install node does the install, up to the user


	node_a
		reh_install => name_of_dependent. will create a rule namespace, add rule install: install/node_a, add rule install/node_a: node_a, and re-depend install
		this way we get a chain of events that allows us to trace where installed nodes come from
		
		or it is a build info, information from lower levels send back to higher levels
		
		or nodes install themselves in a directory and the higher level just collects what's there, it's a bit like build info but dumber and simpler, not sure we can trace where the nodes are coming from; we could have an extra file with information but it is not like having rules that fire at the right moment.



release vs debug configuration
	 in pbs1, ifs in the config module, using one config namespace or the other, ifs in the pbsfiles
	can debug, release, ... be nodes? they are active during configuration, build, and install, or are they specific sets of config and rules included by --type options/global_rule? nodes/config/rules, ... should be tagged so their origin is clear, this is for any --option, not just build type (there is no such thing as a build type, just different rules and configuration). How do we merge/overlay nodes from different types of build? does this mean that reloading rules is better than global rules as one type of build can't give its rules to another type of build? Nodes with different configs/dependencies but the same name will co-exist in the same graph.

how does one start different types of builds at the same time?
	is it for different, possibly  distributed, builds to share nodes or to look at both build in an overlay common graph

 
pbsrun is a node that has a pbsfile, a config , and rule set dependency nodes

configure and build phases separated to run faster
	cmake, waf, ninja, all use this system (bad imo)
	pbs, when using warp, re-configures if it is necessary and only re-configures the part that needs it.

intermediary files
	interesting in the case their re-build would give the same result, when giving a build to someone, we can give only the targets (the nodes tagged as interesting to have at the end of the build) and source (which they can get from VCS) and the pbs digests. The other user will have enough to check the build and re-build only what is necessary. this is efficient in term of size of cache needed to be shared but the most efficient is to get all the nodes to never rebuild anything that already been built. if the cache is represented as a service (actually if the _checker_ can use a service or look for build nodes itself on other build machines), we don't need to give anything, just setup the checker. Pbs1 has a very simple mechanism where the build directory and local repositories are checked, this could be generalized.

Targets nodes 
	created by the command line and nodes tagged as targets
	nodes that the user want to have available in her local files_system after a build (see LOCAL in pbs1, and remember that nodes can exist in the distributed cache (and thus the build succeeds) but not locally.

transactions 4.3.2
  a transaction mechanism (shallow in waf, thus useless) to allow changes to environement (I guess not only config) and revert if there is a problem
  is it something that's needed in pbs2? isn't that already existing but with deep copy in pbs1 when a node is assigned a node config?

portable tools
	cmake has some crappy ones
	Perl has a full module of them, to replace thoses tools
	I think it's better to configure them for each os but if one wants to use portable ones they should not be part of the build tool, eg: no one stops the user to use the perl tool modules by configuring the tools 
		add_config SED => 'perl -Mtools -e 'run_sed'
	some good soul could write on of those configs for all the tools, with the associated module and installation procedure (a different project)

OS specific paths
	we need a command line tool to munge the paths and an function in the build tool ( preferably with the same name) which does the same thing ( IE: call the external command, even if it is slower it guaranties the same result)

Executing tasks before and after the build
	rule my_task : pbs_build ; target my_task # yohhooo done
	
	if pbs_build is automatically generated, have a getter for the name of the node
	
	this can be tested in pbs1 by replacing --post_pbs with a rule


Running the debugger for specific nodes
	a rule that matches the nodes (globally) added by a switch ( or in an included pbsfile)
	 when a specific step with debugging is to be run, start the step in the debugger instead for simply running it, the user is given the choice to stay in the debugger or leave it after the node. This is a bit of a bazooka to kill a mosquito, often a pause at the node and displaying the configuration is enough, if it is in a REPL, query other nodes is possible. Alternatively, a web based REP for the current build (or multiple builds if one can select which is accessible directly from the REP) could allow the same debugging without interrupting the build 

   
in-memory nodes
	not sure I like it but nodes for locally produced nodes
	nodes can be remote or a simple calculation that doesn't need to exist on the disk.
	The problem is that it becomes impossible to have a warp cache, how do we say if a node has changed or not if it doesn't exist?
	
	Note that intermediate nodes are removed by make and it still works; not sure it is possible to do the same with just a digest (not containing the nodes md5!)

coupling between nodes in the file system and in the grapk
	pbs nodes are just names who get matched to files during check step, in pbs the nodes can be anywhere, even in different file systems or networks

	pbs1 allows nodes to be just named as dependencies and left dangling for other steps in the build system to depend them
	pbs1 allows aliases (all subpbs targets are aliased)
	pbs1 allows trigger builds where a node in the current graph corresponds to another node ( can we do this nicely with graph overlay?)

locking the build directory till the build is done
	--build_directory does part of the job but there is no locking
	locking makes no sense, we should be able to build the same thing ( with or without config change ) at the same time
		but they should get in different directories
	
	WAF the locking is nice but it's half baked. How do we lock source files? how do we reuse node from previous builds (if we use a different directory) or even share nodes between concurrent builds? Node should get into a global cache and the build directories just link to the cache; the links don't even have to be to physical files.


build parallelization explanation
	concentrating  on how it is done, makes a documentation mess
	more than the build step can be parallelized

scheduling time prioritizing
	long running tasks can start first (doesn't pbs1 allow that too? node - __WEIGHT)

generating graphs of the current build step (for the paranoid)

Dependency on build tool, per functions, ...
	pbs rebuilds if the pbsfiles or pbs libs have changed
	but dependency on compiler (etc ...) is not automatically handled
		user can define an extra dependency but it should be very simple
			for shell commands, reminding the user that the command is not in the digest may help

	this mechanism can definine a dependency on a specific compile version or the operating system version

	for global dependency, eg: OS version, we need to add a rule in each pbsfile run
		we have a defined system to add rules via switches --os_dependencies but we may
		want to add it in the top file via "pbsuse 'os_dependencies'"
			it doesn't work if we are rebuilding a node in a subpbs directly with --load-config
			then it's the user's reponsibility

			note that "pbuse 'os_dependencies'" applies the current pbs run only, we have no way to inherit it

REH_EXPORT_RULE
	pbsuse rules that are not to be loaded in each subpbs but inherited
		then they should be exported down making it very clear what is done and where they come from

		AddRule xyz
			REH_EXPORT  # generates a warning, fills a history entry, ...

REH_EXPORT_RULE per rule
REH_EXPORT_CONFIG per config
REH_IMPORT_CONFIG per node
REH_IMPORT_ALL_CONFIGS per node

nodes with only configuration dependencies
	in waf:  was an example on how to add configuration in digests
	nodes without real dependencies? just virtual nodes? do we create a digest ;) ?


pbsfile in multiple languages
	snippet "binary", arg1, arg2, ... # free format, "java javafile" is a valid binary
	json_snippet "binary", some_json
	java_snippet "javafile", whatever

	the snippets return pbsfile code which is immediately evaluated as perl code, json, ... which ultimately returns some pbs commands (perl code)

	the snippets are added as dependencies, and snippets can return even more dependencies, rues, configs, ...

	there was a requirement stating that dependers, or any REH, can be written in any language, this is an extention where the pbsfile itself can be written in another language via specialized REH or functions (could we make the REH an function the same format)

	example multi language pbsfile:

		use "1" # perl use which can do anything a pbsfile can if it is run in the current package
		pbsuse "2" #can do whatever a pbsfile can do and is executed in the current package except if put in another package
		
		use pbs_bash # load specialized bash interface

		bash <<<EOB, config1, argument specific for the bash code # run bash code adn evaluate stdout in current package
		some bash code
		
		printf "AddRule from_bash, REH_BASH_DEPEND => "some other bash script", REH_BASH_XXX 0> this same bash script with oher arguments", REH_BUILD => "config1.CC", argument
		printf AddConfig "......"
		EOB

	this still is run by pbs but we could make pbs itself in multiple language
		when a subpbs is run, a target and some supporting function are given
			the subpbs doesn't have to be in perl or in pbs language it needs to return something that
			pbs can integrate in the graph (a sub graph)

			when running pbsf calls REH at specific phases, the nodes returned by the non perl subpbs can contain
			REHs that run code written in any language



Advanced examples
	build a compiler that's going to be used to build nodes

	in pbs, compiler is a configuration variable so it can be used in builders
	nodes that need the compiler have it as dependency
	dependency is added via a rule, the rule can be added by a switch
	the rule can be global
	the compiler itself has a rule to build it
	the target of that rule uses the configuration variable
	the compiler can be build in a different build system (possibly with the current configuration)
	the other build system can be triggered automatically
	multiple versions of the compiler can be build and used
	the compiler may be named differently, depending on config,  although it is build from the same code and compiler build system
	the compiler graph can be accessed
	the compiler graph can be merged to the current graph for display and query (and always for digest generation, as for any node)
	the compiler can be fetched from a remote node/server/...
	the compiler is either build or used as is it is found, in that case it becomes a source

	? is the compiler the value of the configuration variable or a node (with sub graph)? both?

	how do we transform a dependency (node) in something we can call

		# pbs 1
		Config CC => gcc_2.6
		Rule object_file
			dependency => "$path/$name.c",
			dependency => \&depend_source 
			build "%CC -o %FILE_TO_BUILD"


		Rule object_file
			named_dependency =>  [compiler => gcc_2.6],
			REH_SOURCE => "$path/$name.c", 
			build "%compiler -o %FILE_TO_BUILD"

		# above with sugar
		Rule object_file
			REH_COMPILER => gcc_2.6,
			REH_SOURCE => "$path/$name.c",
			REH_C_DEPENDENCIES, 
			build "%CC -o %FILE_TO_BUILD"
			
			# REH_COMPILER can be used multiple time to override the compiler, just like a configuration variable
			# REH_COMPILER can set itself to the default compiler in the system
			# REH_COMPILER can set the configuration variable, with comment and history
			# REH_COMPILER can verify the configuration variable and warn or bail
			# REH_COMPILER can be run in multiple phases, including when the command line has to be evaluated (although setting the config variable is enough)
			# REH_COMPILER can add the compiler as a dependency 


REH arguments
	pbs2 proto parses rules made of tuples REH => REH_argument

	REHs could say how many arguments they have
		this allows REH without arguments or with multiple arguments

		this can make debugging a bit harder just to make rules look good
			

	Rule something 
		REH_1arg  => gcc_2.6,
		REH_3args => "$path/$name.c", 123, something_else,
		REH_0args ; 

	Rule something 
		REH_1arg  => gcc_2.6,
		REH_3args => [ "$path/$name.c", 123, something_else ],
		REH_0args => [] ;  or REH_0args() 

	REHs can ask pbs if an argument is itself an REH
	
	?the biggest problem is that REH_0args is not valid perl syntax, even if we declared it as a function (which we do not want), it would still fail this:

		use Data::TreeDumper ;

		sub test {print DumpTree \@_, 'test'; 7}
		sub rule {print DumpTree \@_, 'rule'}

		rule
			a => 1,
			test , # test => 2 makes test a string
			b => 2 ;
		
	declaring a string constant works
		we also want to manipulate REH symbolically
			allows easier debugging, logging
			late compilation of rules

log file
	for each node a log file is generated
	the log should be structured, even if it only contains the output of the build tool
	extra elements (pbs config config, ...) should be in separate sections
	the node's digest coud be kept in the node
	pbsfile runs are nodes, with a log, and the meso warp could be in that "log" file


generate_ast
	it is not easy to follow the chain of dependencies when debugging even if the chain is not wide nor deep
	pbs has --tt --node_parents --node_parents ... 


REH registration 
	can be inline in the pbsfile (although not reusabe)
	the reh, can be applied in any step,
	its execution order within a step can be set relatively to other reh in the same sep, first and last are virtual reh used to place a reh relative to the begining or end.
	the sequence of reh executions in a step is validated so we do not have any priority problems.
		the sequence is logged in the node (already in pbs2 proto 1)
		it is possible to visualize the sequence of steps/reh_execusion without building

global rules
	means pbs adds them at every level

	GR can be added from command line switches, desc_extra, pbs, within pbsfiles (for all the pbsfiles after, withing the scope?)

	are GR
		added as text in each subpbs
		added in the current package and used in subpbses
		re-evaluated in different context

	information about GR origin are kept and displayed with high visibility

	if gr has associated code it must be reloaded in subpbses

	solution is to put the GR in a file and include it
		inlined GR can be serialized (we can also add meta data in the serialized file)
		switches that include GRs push the name of a file in the GR list

	GR can be selectively Applied, even from command line switches
		pbs --gr filename,regex,regex

		command line switches are aliases to --gr 


rule producing multiple targets at the same time


Textual output
	REH_VERBOSITY define different output message depending in the current verbosity
		the REH_VEBOSITY can be run at multiple steps
		use REH_VERBOSITY to 
			display all the output pbs normally displays
			generate logs
			progress bar info to the output stream (or not, they still are handled externally)...

		REH_VERBOSITY => phase, verbosity level, string # from that level up unless a most fitting REH_VERBOSITY exist
			best to not show phase in pbsfile

	--pretty-print
		content of the pbs display (progress bar or other) can be changed with pretty print option displaying only what the user wants


	Output elements
		each section
			can be associated with a display color
			section can belong to category (which has it's color, and be overriden. the color choice for any section can be computed at the moment of display by user code

	log output element
		log is just a normal output tagged with log, so external output processor know where to save it

	all the normally displayed information can be in different rehs and pbs adds a global rules to add all of them at once
		the rule is kept in a separate file

		add rule
			global
			match all
			REH_AT_BUILD_START => generate some output section 
			REH_AT_BUILD_COMMAND_START => generate some output section 
			REH_AT_BUILD_END => generate some output section 
			REH_AT_DEBUG_NODE => generate some output section 

		all depend on verbosity level

Global rule efficiency
	rehs that are applied to all nodes should be quick to apply on nodes
		EG: sort them and just apply them without running any regex (if that takes time, otherwise apply the regex)
	since every node is going to match these GR, node should point at them rather than have copies
	pointing at them should be done symbolically, by name not reference
	rather than storing all the GR, store the name in a global node section and always run it (optimization is the root of ...)


reh_short command description
	rather than display the build command, display a short comment
		 eg: gcc ccc abc ccc vvv eee fffff becomes compiling abc
	
	depends on verbosity level

	how do we override REH_AT_BUILD_COMMAND_START
		the override should be by specific node 
 

WEB interfacwe
	eg: gtg is send to the web server
	all the information stream is

	can we create a web REPL?


reh_needed_resources
	like jenkins node capability description, just an attribute
	can be used to reserve nodes for specific tasks
		the specialized pool resources should be made available when no more special node needs them

handle intermediate files a bit like make
	well, better than make 
	the choice of file removal can be left to the user
		 via a command line option with option for regex
			through a rule



bash completion
	commands taking arguments should get provide completion

	unfortunately the whole command line is not available so we can't analyse context

	plugins providing options should provide completion

	fix generated code to 

	my($trie) = new Tree::Trie;
	$trie->add(@completions) ;

	my ($command_name, $word_to_complete, $previous_arguments) = @ARGV ;

	if($word_to_complete !~ /^\s?$/)
		{
		my @possible_completions = $trie->lookup($word_to_complete) ;
		print join("\n", @possible_completions) ;
		}
	#~ else




handling of failed node build
	depending on the build some node may have less importance than other
		ie: doc build can fail but tests must pass

	parents of node, subset of nodes,  decide the importance of dependencies
		the nodes (with level of importance to be changed) may live multiple level deep in the dependency graph

		the higher level node must be able to catch a failed build  at the sub levels
			all nodes can catch sub node failures, scan the nodes to know why one
			has failed or ask the node directly

	Do we set REH_CATCH handlers after the dependency graph is build?
		this can't be applied on a node since it is not visible outside the package

		global rules could traverse the graph post dependency
			collect global rules while creating graph
			run global rules on the top node
				set global_depend_flag

			AddGlobalRule
				matches qr// # can match a node or pbs node (pbsfile, config, ...)
				dependencies => &get_dependencies_from_node
				REH_stuff we want to change

		desc_extra global rules are added automatically to sub levels
			AddRule --dd
				match
				REH
				GLOBAL

			are rules added to digest
				must as lower nodes don't include higher nodes digests (see pbsfile_path)

			what happens when linking a node
				does it have to have had the same global rule applied
					otherwise order of dependency becomes important

		
		AddRule(..., ..., GLOBAL) can be added by a switch
		pbs --pbsuse_global 'xxx' (see --gr)
			

	nodes can themselves catch a failure

		Rule
			REH
			REH_BUILD ...
			REH_CATCH

	? isn't this "--keep building"

REH_TIMER and REH_NO_TIMER
	possibility to add timing via rules
	
	the timer need to either wrap all the builder or to be run at the start and the end

	how do we time individual builders or build steps?

	can we use REH_TIMER to time the loading of pbsfile and where is the statistic gathered
		?REH_STATS 'where/how to save'

	display of stats can be via a rule applied on the root node
		? how do we display stats on a failed build
	 
	can TIME_OUT be applied the same way

		isn't TIME_OUT better as a wrapper?

		rule
			name => no_time_out
			...
			build => ...

		rule
			name => single_command_time_out_wrapper
			...
			TIME_OUT => 10, [ build => ... ]

		rule
			name => multiple_command_time_out_wrapper
			...
			TIME_OUT => 10, [ build => ..., build2 => ... ]

		rule
			name => global_time_out_wrappers
			...
			BUILD_TIME_OUT => 10,
			build => ...  


pbsfile is normal perl file
	running subpbs is, already, handled by a reh, one less thing to have in the pbs core

	packages, package configs and anything local to the pbsfile can be handled in a normal pbsfile

	global actions are more difficult
		options, prf, targets handling, colors, debugging, progress bar,
		statistics, indentation level, plugins, ...

		phases: depend, check, build, post_pbs
			maybe possible to replace them with rules

	possibly 
		use "pbs" ;
		use "reh_some_flavor" ;

	the pbs module can instantiate a singleton that does what pbs.pl does
		check the command line and include option_rules
			each option matches a file that is run to parse part of the command line
			non matching CL arguments are processed by a target "option-rule"

option rules
	have help
	parse the command line arguments an return 
		the rest of the command line
		text/rules to be run in the current package

	# this allows us to define options dynamically for debugging e.g.

	example --dd option rule
		lives in options/-dd.pl
		parse command line
			return command line minus parsed portion
				can insert elements in command line
					eg --all_debug can add --debug_level1 --debuge_level2 ...
		handles --help--dd
		returns code to be run in the package
			debug option to show the code
		
		-dd has multiple types (call time type) so it can add a config when a node is created
			and display dependencies after a node is depended

	this mechanism can be used to implement rule injection
		create a new "option" which adds a rule, add it to the pbs call
		since options can take arguments, the injection can itself decide when to act

	debugging uses rule injection

	"options" can
		add rules
		manipulate package configs
		add config rules
		run code
		access the graph
		access the package node

	option rules are 
		perl scripts (non executable)
			loaded within pbs during the run

		executables
			can be written in any language
			how do they communicate back?
				json/yaml on stdout

			example --default_colors option
				cat plugin/option/--default_colors
			
				#!/bin/bash
				printf "mine/text\n--color_define 'debug=red on_yellow'"

				alt:
				
				#!/bin/bash
				tail -n +3 $0
				exit 0
				some yaml
					here
			

target rule
	parse the targets, update the target "variable"
	start the build system

	if the targets are added as dependencies to the "package" we just need
	to start building the package, which we wanted to be a node anyway

	the package rule can be added by "use pbs;"

	"building" (maybe itself a step-node)  must be done after the rules are added
		and the configuration is build
	
	there can be multiple target rules

prf
	is an option rule that only adds options to the command line
	the options are read from the file given as argument
	
	pbs1 prf files are powerful (albeit restricted) perl scripts
		? remove that possibility or emulate it

colors
	part of the package->node config
		can specialize color per node type

	package->node configs are dislpayed with -nc during build but we don't want to
	see mundane config => filter them out
		see: structured text output  
		--nc is an option, a new option can be written to display whatever the user wants

	normal output is without color
		the color names are used (some pbs defined, some user defined)
		the colors are white on black

	--color
		option sets the package colors which are inherited

	--color_C_nodes
		adds a rule matching C node and setting special colors for them
	
	--color_define "name=defintion"

* debugging
	debugger
		not really part of pbs even though pbs provides special functions while running in the debugger

	breakpoints
		REH_DEBUG, matches all the levels
		stop in debugger
			nothing to do with pbs, except the starting of the debbuger
	
* progress bar
	output an update progress bar code for each build
		external filtering app, displays the progress bar

	see structured text output

display node info
	rule
		Global
		REH_display_node_info

* statistics
	Rule
		MATCHES type pbsfile
		REH_PBSLOAD_STAT
			type, pre-load and post-load
		GLOBAL

	Target pbs_load_stats
	
	Rule
		MATCH pbs_load_stats
		DEPENDENCIES => PBS_DEPEND_STEP or PBS_END_STEP


parallel depend phase 
	(these thoughts started with how we display the parallel depend phase and continues on how to implement it)

	for debugging, depend in parallel first then with a single process and display the indentation as pbs1

	a regex to display the depending of a specific node, or node and all its sub dependencies can limit which node are displayed

	triggering a specific node will display only that node's depending

	display node depend as node build
		the information about rules, triggers, ... displayed during depend are simply
		aligned to the left

		we can also display a progress bar

	depend display during parallel depend is still not simple to display

	depending nodes is not ordered like in pbs1, depend node, depend it's dependencies
		depend node, put the list of nodes somewhere a depend thread will pick it up
			this let us use the same type of scheduling as a build and even remote depend

		the current mechanism is efficient because the rules and package configs are already in memory but a node to be depended could be serialized and deserialized in another thread/process. if we keep the rules serialized, the serialization of the node is just its config and its matching rule.

	the output of the depend phase is unsorted, to display something we can use for debugging we must sort it
		we can  follow multiple nodes depend if each one is sorted in it's own stream

	depend log
		the output during the depend phase can be difficult to follow as a node depend is followed by
			its dependencies depend, the first node depend continues and al the sub dependencies have been depended
			when the next dependency to the first node is depended it's difficult to see what parent that dependency has
				? is it that important to know which parent

		the output could contain graph lines to help but that works only for smaller graphs

		the output background color could change with depth of depend, that's like graph lines

		the depend output of a node is written in a node depend log which can be looked at with an editor
			the sub nodes depend is not displayed but links to the sub nodes exist (build_name path + node name)

		the depend output for the whole graph is logged (pbs1 output)

		it is possible to crawl the node depend logs and generate a structured output (indented by level)
			the structured output can filter the node's depend output to a minimum and be decorate so one can jump from node to node in a text editor
			this is meso warp!

	fzf can be a great tool to use going around in nodes depend logs

	NOT RE-DEPENDING A NODE
		during depend in different threads/process/build_nodes, some nodes will be common dependencies
		we don't want separate depend nodes to depend the same node as it wastes cpu time
	
		first of all we should use depended nodes if they exist from an older build
		checking with a central location should be done in batch if possible, the depend process also gets a batch of nodes to process not just one  

*** THE GRAPH IS A BUILD CONSTRUCT ***
	we don't need a graph most of the time
	this allows us to spread nodes depend and build on multiple machine processes as long as the environment is identical (nodes carry their config and build instructions) 

synchronizing between build nodes (given they have the same (verified) environment) can be done with git/ssh (diff the remote nodes with local repo)
	the remote repo can be shallow

history of build
	time is defined by the dependency relationship, not by build time which means nothing!

	if the build log contains a time stamp we can put the node build logs on a time line
	the time stamp also contains the parents, or should it be the parents, time is not really important it's the relation between the nodes that describe the build sequence
	still nodes of equivalent build levels are build at different times (and on different build nodes, and can stay there, no need to copy them back

	We don't want to keep a dependency to dependent relationship, even on it's own layer
		we have the dependent to dependency available so we could build a reverse time line
			we can display the time line in reverse

		can this be used to schedule the build without having to keep the dependent information
			we use the information to increment the dependent count of dependencies left to build

			in any case this information shouldn't be kept in the node as we do in pbs1 as it is not a node data but a build data

Global rule

* indentation level
	mainly used during depend phase
		the depend phase does more than just find dependencies
			load pbsfile
				display when it starts and stops
			load pbs libraries
				do stats on it
			setup config
			trigger rules
			...
 
	indentation level is the depth of pbsfile levels
		pbsfiles have names
		some pbsfile can be called for different targets

	=> level is a path to the node: pbsfile:target/pbsfile:target/node
		we don't need to have an indentation level, the nodes name contains it

		an option can make the display indented or not

		the path to the node is the pbsfile path! node name can be different


		parent nodes give their path to their dependencies
		linked nodes get multiple parents paths
			but a node really doesn't care about it's parents, it has it's own config, rules, ...
				the parents paths are a layer of information needed by the build not the node

		
		the depth can also be a numeric node attribute rather than embedded in the nodes pbsfile/target path



* plugins
	add switches dynamically, problem with prf (see pbs 1 FrontEnd.pm)
	plugin path can be defined on the command line or prf after the switches that the plugin defines

		all this because pbs does the parsing of command line arguments using switches added by the plugins
		this, I believe, is a not a problem when the plugins parse themselves
			pbs runs the parsing (via the plugins/option rule files)
				if the command line is not consumed or transformed, there is an error
					if an option is not recognize because the plugin/option is not in the plugin path
						the plugin path option will add a plugin path and change the command line
							pbs calls the plugin/options again and finds them

* graph generation
	Rule
		MATCHES type pbs
		REH_PBSLOAD_STAT
			type, pre-load and post-load
		GLOBAL

	sub graphs
		Rule
			MATCHES type pbsfile
			REH_GRAPH
				type, pre-load and post-load
			GLOBAL

	graph option
		set a configuration variable in the package which is use by the graph node builder

* configuration setting and inheritance
	config is a node, its values are set with REH in a rule matching the node

	package
		add_config is equivalent to adding a rule

		get_config 
			!problem is that node exists after the rules are run
			add_config could create a node (with PBS::get_node) $package_package_config and run a rule on it locally
				is running a rule like immediate_build?

			get_config can then ask for the node and look for values
				better yet run a GET_REH which returns the value

				


		inheritance
			config nodes have a parent data entry

		
	node
		have a dependency config_node
			which can inherit from the node's parent
				would be nice if the inheritance was just symbolic (no real reference)
					easy to serialize, parent is just the name of the config node
					language agnostic, can be reused 
			which can be passed to dependencies
			which can filter the parents config node values

	sub pbs
		we want to override some of the config variables
		we want to pass multiple config nodes to the sub pbs
			normal node config
			pbs config
*warp
	warp, meso, ... all are targets
		added by switch

		need to run even if the build fails

*global artifact cache
	 not really part of pbs, it could be any mechanism on many types of cache

	one good thing is that it is possible to locate the artifact (including not finding one) with
	rules

	rule
		rehs ...
		locate_somewhere in the world
		locate_in_build_directory
		create_in_build_directory


	this allows us to implement ccache like  caches very simply


	pushing to the cache can be as simple as adding a reh to the rule
		
	rule
		...
		post_build_reh_push_to_cache

		# could also be node depending on the 


pbs builds nodes in differen overlay graphs


* post pbs
	also a target
	needs to be run even if the dependencies have failed


overlay graphs
	same build different variants
	different build overlayed later

	understand common parts of builds and differences
	without building know what parts can be reused and in which order to build different builds
		for maximum reuse, ie: build only part of what is common and wait for the other builds 
		to build the rest, distribution must be globally controlled to avoid race conditions 

	the same build could produce overlay graphs
		pbs nodes
		warp
		C files dependencies

		all valid grpahs on their own but of little interest during normal build

		the other solution is to have them in the main graph but filter out when presenting node
			this has the disadvantage to need code for each new type while overlay graphs are 
			not part of the normal graph at all
step nodes, pbsfile nodes, package nodes
	make all the elements nodes that are added by rules

	pbs can add the rule for the current package
	target add dependencies to the current package
	rules add dependencies to the targets
	configs are node which are dependencies to the other nodes
	
	depend, check, build can be seen as nodes
		build: check
		check: depend
		depend: package config
		package: targets
		target: rule generated dependencies
		config: rule generated or package generate


REH types
	reh can have multiple types so they are called multiple times
	how can we make a reh (in a matching rule) run when another rule matches
		e.g.: --dd needs to show dependencies when any rule matches
		? add a node element handler (NEH) (via a rule and a REH type) and each time a node is used
			(in any step, trigger, check, build, build step, ...) call the NEHs that match the step


	what did pbs_use do that use can't 
		adds the required package to the current package dependencies
		load time statistics

rule component can be shell scripts, service via rpc or http, ...
	rule name, regex, depender, builder, node_sub, ...
		perl dependers have access to everything, that's difficult when the depender
		is an external command. 
		
		we can chose some data and either pass them as arguments or serialize them in a file

		the great advantage of REH is that we can have different REHs depending on how we want to call the external code

		the REH can be given a command name to run and that command can be an executable with a shebang
			external command can be written in any language

		REH pointing to a language and give code is possible too
			REH_CMD_DEPENDER 
				language => Python
				code => some code 


		which takes us to other languages to write the pbsfiles
			given that REH are not built in but included, nothing stops one to load language
			specific REHs

			use 'perl_reh' ; # with the possibility to filter what reh and change their name

			Rule
				Regex => /must be perl regex/

			use 'java_reh' ;

			Rule
				Regex => /must be java regex/ 

			making pbs a REH tunner and nothing else

			=> of course there's more than writting the REH in other language for good support
				the possibility to define variables, includes, ...


	can AddRule be a REH?
		rule name needs to be a REH to be handled in another language
		
		Merge AddRule and REH_MATCH
			AddRule
				NAME         => 'first',
				Match        => /regex/,
				Dependencies => qw(x y z),
				Builder => 
					[
						'do something' ,
						'do another thing'
					];
  

			Match /regex/ =>
				NAME         => 'first',
				Dependencies => qw(x y z),
				Builder => 
					[
						'do something' ,
						'do another thing'
					];
	
			Target /regex/ , qw(x y z) => # target is AddRule, Match, and Dependencies at the same time
				do(
					'something' ,
					'another thing'
				);
	

? stop mandating NAME
	naming has the main advantage of forcing the user to think about what she's doing
	debugging gives the file name and the line, and could print the rule, matchers, ...,
		reducing the need for a name

	REGEX is also mandatory but why? if you forget the regex it will not match (siso)

	some elements are set by pbs2 like DEFINED_AT_FILE
		can't we let the REH handle that so the pbs engine doesn't have to?

Target-specific variables
	are considered harmful by other make implementations: kati, Mozilla pymake.
	Because of them, a target can be built differently depending on if it's built standalone, 
	or as a dependency of a parent target with a target-specific variable.
	And you won't know which way it was, because you don't know what is already built.  
		=> this means the node config has to be added to the digest
			we do that for some variables for object files, a generalized mechanism to chose which variable is needed
		=> pbs checks configuration variables when linking nodes

let user define log level
	use relative level, ie not a fixed value but relative to already defined levels (text without value)

intermediate files
	no such thing in pbs since we need to have a digest for any file that's not a source

structured text output
	output structured object (yaml|json|perl) natively and let external application transform the output for the user

		pbs ... | special_pager
		pbs ... | html_data | nc ... ; pbs_web ... &
				pbs_web can keep sessions, have timeline, ...

	multiple external programs can be chained

		pbs ... | tee >(nc to_server) | show_depend | show_build | show stats
		
		the external program can either consume the data or pass it through

	pbs has it's own structured text transfoms that are applied
		if no other are defined
			via a user defined switch  
		or disabled
		or own can be displayed on STDERR while structure text is written to a file descriptor (user defined)

show current directory when executing commands
	--pwd

	we can also add the cwd to thwe structured data


work flow primitives
	on_error
	or_build
	and_build
	try_catch
	! parallel => everything must run parralel, one can use a dependency to synch if necessary

	distribute
		to heterogen nodes, with synch, timeout, re-try on a different node

Interactive build/ kinda REPL except it runs in or around the build

	given a dependency graph, do interactive steps for debugging and recording
		close to debug mode but rather than running in a debugger user runs in an 
		interactive shell.

		INTERACTIVE_REH => 1
			BUILD => [shell command 1, shell command 2, ...]

		at each command start a shell, with the command inserted, and let the user control the command
	
	save the session to create build commands from it

	interactively add node to graph
		building node_1 the user realizes that a dependency must be build first
			add_node $current node dependency
				adds the dependency and takes user to the node build where
				commands can be added
	
	if we could record the state of the file system (hu hoo docker!) we could go back and 
		forth in the build time line.

	
PBS2 
	AddNode adds by name
		creates node if necessary and adds it

	we should be able to give already created nodes (eg: from cache or other graphs) to AddNode

	node and node content can be separated


matching urls and other objects
	rule 'name' matches some_URL, just a node in the graph, doesn't matter if it's an url or not
		depend on nodes which themselves can be as remote as this one or not, again just a node name
		checker the digest for the file, which may be on another machine
		builder, can be local or remote, pbs doesn't care as long as it gets a digest
		digest generator computes digest of not

	checker and generator could be the same code
	

	alternative: url is just an installed file
		url/file depends on file
			checker checks the remote file
			builder copies dependency to remote
			digest returns the digest of file

		file depends on local dependencies
			builder


	if the object type corresponds to a set of specialized depender, checker, builder, digest generator (or a subset of them)
		we can wrap those REH in one of
			Specialized add rule
			Specialized add REH


		AddRule
			name => *.special
			depender => special_depender
			checker => special_checker
			builder => special_builder
			digest => special_digest_generator

		AddRuleForSpecial
			name => *.special
			reh => ...

		AddRule
			name => *.special
			special_REH()
			reh => ...

		*** all user defined
			but depend, build, check, digest must be overridable

		Best would be that AddRule adds the default depend, build, check, digest (making AddRule a user defined function)
			overriding any of the base REH can generate warnings, log, whatever the REH want
				How does the "default" checker REH know that it has been overriden?
					Does the overriding REH manipulate the REH list?
					Is there a single slot for checker REH? or builder REH (although we already want multiple build REH)
				Do we need to allow the REH to check the REH list? 

		Addrule is in a module that needs to be pbsused

		top pbs just runs the top pbsfile

shorter rule syntax
	AddRule
		NAME => 'the name',
		REGEX => "some_regex",
		ACTIVE => 1,
		COMMENT => "some description",
		REH_WHATNOT => 1

	Rule 'the name', MATCHES("some regex"), IS_ACTIVE,
		COMMENT("some description")
		REH_WHATNOT => 1

other rule syntax
	do we need Rule?
		? any REH adds a rule till ; is found
			not possible with fat comma

	functions with the same names as REH allow:

	Rule 'this rule',
		MATCH => "some regex", 
		COMMENT => "some description",
		REH_WHATNOT => 1 ;

	MATCH "some regex", 
		COMMENT => "some description",
		REH_WHATNOT => 1 ;

	COMMENT "some description",
		MATCH => "some regex", 
		REH_WHATNOT => 1 ;

	DISABLED 
		MATCH => "some regex", 
		COMMENT => "some description",
		REH_WHATNOT => 1 ;

command to add non active rules
	Rule 'the name', MATCHES("some regex") ; #ACTIVE
	
	Disabled 'the name', MATCHES("some regex") 

can REH add REH in rule or do we wrap the addition of REHs in a normal function?
	
remote file syntax
	all files are local, how do we describe and access remote files

		service:server:/path/file
			service scp, webdav, http, ...
			credentials (and the dreaded key ring)

		virtual file or state
			eg: entry in DB
			not different from service:server/ ...


		difference with local file is that the signature must be computed differently
		pbs dependent doesn't care a node just a name


		Rule 
			Match(/$URL)
			REH_COMPUTE_SIGNATURE(any code/pre existing code reference)
			BUILD

		
		Rule
			match_ssh_file $server_file, credentials:xxxx
			# this also adds a REH_COMPUTE_SIGNATURE to the REH list

			# much  easier to have match_ssh_file return a list of REH rather than be an REH

REH_COMMENT
	(maybe backport to pbs)
	display the comment when the node is build
	comments could have level, like a log, and be shown over a threshold
		threshold should be dynamic

pbsuse
	does it look in the pbsfile directory or just in the include paths?

	it's not scoped within a rule (do we want that?)
	the include path are set on the CLI
		can be hacked via pbs_config but it's  hack
		needs a clean interface, with warnings, and scoping to local pbs run

	warnings (option?) if multiple files with the same name exist
	
	can we generate the libs dynamically?
		use case? (we have the mechanism to generate pbsfiles dynamically)

	modules are assumed to have a pm extentsion
	full path modules are handled but ./module is looked for in module_path not root

REH_ON_FAILURE

hardware failure
	on large clusters failure is inevitable
		how do we detect and differentiate hardware failures from build failures?
			how do we restart the part of build failing because of hardware?

text mode, data oriented
	node and node group orientd
		textual data to describe the node

	work on a local group of data text files

	distribution of processing for large data sets

	small utilities (unix style)
		check_digest
		...

remote debuggers
	possibility to run shell builders in a debugging shell
		eg:
		AR  .....
			name
			builder
				shell command 1
				shell command 2

		pbs -interactive_shell IP -name 
		
		when shell commands are to be run, a list of commands is displayed in
		the client's shell window
		
			the client can be on a different machine, thus we need to ssh back

	running the build system (or part of it) in a remote debugger

how to handle no memory in very large builds
	separate the different phases to reduce memory usage
	put nodes in DB
	on demand node loading/unloading

interactive repl, scons .98
	once the graph is created, the repl allows one to build repeatedly
		is the graph hashed at each build?
 
limit parallel build based on resource usage 
	eg do not schedule while load is high

multiple node created by a single build command

	Match(pat1 pat2), BUILD(...) 
		means both nodes are build by BUILD
		but there is not relationship between Macth and BUILD

		in pbs it is TWO builds and we try to  remember that one of the node
			was build and the second one is not run
			problem, it doesn't work in parallel, it doesn't work for multiple libs, ... 

		see the gnu make book ~ p94

	BEWARE of parallel build where process don't share data (pbs1 fails there)
		which means that we need to check if the node was already build via the file system!

		BEWARE 2 even if we use the file system sentinel 
			the processes may be NOT running on the same machine (distribution)
				can only be fixed by a common build database but complicates unnecessarily
				and is much slower and gets slower as we parallelize more

			even on the same  machine, two nodes may check for the presence of the sentinel
				at the exact same time
				this can be fixed by making the nodes depend on the last node thus forcing
				the last node (and the sentinel) to be build first
	

	AR BUILDS(regex1, regex2, ...), BUILD_MULTIPLE...

	regexes are ANDed

	BUILDS runs at depend AND build stage to link them together
		we can "tag" nodes as "build together" and act accordingly

	BUILD_MULTIPLE is a specialized BUILD


REH can be run at multiple stages

REH run order is defined by saying which REH are run before or after
	this lets us check at compile time if the REH sequence is valid

variable dependencies
	in pbs dependencies to variables are handled through a set of commands
		AddVariableDependency, ... and equivalent commands that act on uniq nodes

	REHs can replace them
		global REHs are added in a rule that matches globally 
		node REHs are added in the node rule or a rule that matches a specifi node
			node REH can remove gloabla variable dependencies
			
			config node REH can take a script as argument for complex cases
	
	this unifies the handling and reporting of dependency listing with the handling
	and reporting of digest entries


	in other build systems this is usually done by remembering the command used to build the 
	node, if it changes it has to be rebuild. In pbs  we decided that it didn't have the right
	granularity, what if a file location change, if an argument doesn't change the output, ...
	
	we also want to build nodes with multiple commands and even, maybe with builders defined
	in multiple rules

	this put a burden on the user which can be lightened slightly
		warn if a non dependency config variable is used in a build command
			separate commands to declare variables that are not part of the digest
				this removes the warning
	
	the user may want to depend on the command line and not bother with declaring variable dependencies
		Rule match((), dependencies('*.c')
			build("%CC

multiple rules builder
	a warning is displayed when multiple builders are used
	warning end up in log (all warnings should)

--dd, ddl, ddr
	often followed with --tno
	ca we present --dd as a tee to start with ?

use case
	Rule V all_tests: test1 test2

	# we want the tests to be run in individual directories and in parallel
	# they can't be in the same directory because they use the same input files
	# which must be different for each test

	Rule V ^test*: sub_dir*/test* @dependencies
		cd sub_dir*/test
		run the test
		
	Rule sub_dir*/test*
		mkdir %FILE_TO_BUILD


generalize the concept of object_files
	a sub target creates a list of information to be used at a higher level
		eg: a list of oject files to be linked
			the concept was created to reduce the time spend creating
			intermediary libraries

	command line utility to manipulate the object_files
	%EXTRACT to manipulate the object_files in the build commands
	change name to reflect the generic nature

	another case could be a lib that returns
		a lib
		the  location of header files

		my_stuff: informative_lib



		informative_lib: @c_files
			%cc @c_files
			object_files %FILE_TO_BUILD add LIB %FILE_TO_BUILD
			object_files %FILE_TO_BUILD add INCLUDE_PATH somewhere/ ....
			...
			
easier named dependencies
	by common prefix?
		t: dir/a dir/b another_dir/c d
			command %MATCH(dir/, %DEPENDENCIES) # lol, this is gmake macros

check gmake debugging options
	database dumo
	variable definition file:line
	--warn-undefined-variable
	...


multiple versions of the same node in the graph
	is it really worth it? 


%FILE_TO_BUILD short versions
	a la make $@
	%FTB
	%TARGET

	the build time variables should be in the config too to allow easier debugging
		they are computed per line run in a shell, not at the build start!
			users can define their own!

	in any case they should be visible for debugging
		there's an EVAL_DEBUG switch bit not sure it works on user defined %VARS

pass arguments to prerequisites targets
	a: setup(a, a1, a2, ...)
	b: setup(b, b1, b2, ...)

	setup:

	if setup is a target, it will be run just onece

	it is possible to have multiple targets by using
		%setup or setup%
		but this runs the same setup when we want to run different setups

	we can define a function and call it
		a:
			 setup(a, a1, a2, ...)
		b:
			 setup(b, b1, b2, ...)
		but it doesn't show the prerequisite nature 

matching a rule, run one time
		when all match?
		when a single one matches?

templatized output
	eg: output for when a node is build (-bni) corresponds to a template that the user can modify
		access to basic element/color, and options, are provided

		a bit like the code that does the visualization but elements are computed and made available to user
			may be less efficient

		we can start by using templates internally and moving display code to plugins
			note that node visualization is already a plugin
		
		breakpoints + bni + bnir 'no_match' is a workaround
	
	the templates should use functions that can be used in post processing
	the templates themselves should be run on node
		say during post processing or debugging I want to see the same output as during the build
			fill_template(node, template, args)


output is by default off
	opposite of -q, we need to ask for output
	output is often necessary during creation of the build rules
	output is needed when a node fails to build
		and only for the node that fails and its dependencies
			and we can generate dependency node output on demand
			if we need to see the output of the successful nodes, if not kept),
				we can build those nodes

Microwarp composition
	as the warp cache is composed of MW, multiple configuration can share the nodes
	can we check them in?
		this would give us a history of the dependency graph
			it would be nice if the system history was also under version control
				difficult to force devs to checking code when doing local builds/tests
				the nodes hash is kept in the dependency graph
					we can find which version of the source repo is used
						or if it is a local version


log is not generated
	the log contains node information and build time information (including output from the tools)
		no need to keep tool output for node which build succeed

		better to generate a log on demand from the dependency graph and build time information
			see templatized output

		possibility to generate a single node build environment for debugging

		

REH_BREAKPOINT
	a bit like a pbs1 breakpoint, called back on events
	REH_EVENT?

	possibility to alias the name and arguments easily
		REH_ON_BUILD => REH_EVENT(TYPE => 'BUILD', POST => 1, PROCESS => sub { ...})

REH_USER_TARGET
	"targets" target which queries the registered rules and displays the user targets
	https://marmelab.com/blog/2016/02/29/auto-documented-makefile.html

REH SHELL and ENV
	as ENV is stripped off we need a way to bring it back for applications 
	running under some SHELL

	setting ENV is done by rules which allows us to selectively apply ENV to a 
	node build

	there is a need to control which rules set environment variables (the same need
	we have to control which rules bring which dependencies, also a node
	environment control problem) 
		how to define node environment and what rules are active within it
		how external environment impacts the node environment
			in pbs rules are controlled by running in a specific package
			but that separated parts of the graph not the nodes within that
			part

			still in pbs (without --neo_config) configuration are global even
			if they are hierarchically inherited

		how to visualize the rules changing the environment
		how to control the of the interaction between rules
			how one rule can stop other rules
  
	

Create a clear display framework
	in pbs, redirection is in multiple files to handle multiple options
	it is difficult to change and get an overview of

Move dependencies to __DEPENDENCIES

graph generation, node display filters
	per node
		rules decide which nodes get which filters, this is not part of the pbsfiles (but can be)
		but use the same mechanism, eg: the nodes, even if they already are in the graph (depended) get
		subject to a set of rules that define how, and if, a node will be be displayed

			if part of a subpbs, graph rules work as node subs and save data in the node

		

		the rules also create, one or more, graph nodes where the nodes are inserted,
		this allows the creation of multiple graphs
			do we add a target for the graph generation or create graphs directly?

		
		the graphs can be rendered differently, eg: table, CSV, image, ...
		
		nodes can belong to multiple graphs and be displayed differently in each one


		eg:
		
		PBS_ROOT: graph

		graph: root_node
			create_graph
		
			


target naming
	given the rule:
		A => B, C(A)

		where C, it's name, depends on A's name
			ie '*.lib' => tests, *.module1

		if B also depends on C
			B => C(?).test

		how do we propagate A's name?


	this is because the rules are completely separated, they should be or we would not be able to
	reuse rules

	this has been implemented this way in the passed:
		my @modules = qw( module1 module ) ; 

		# we can also get the modules from a configuration
		AddConfig MODULES =>  'module1 module' ; # keep them as text
		my @modules = split /\s+/, GetConfig 'MODULES' ; # which can be hidden behind some sub


		And then rules added:

		AddRule *.lib => tests, @modules ;
		AddRule tests => map {"$_.test"} @modules ; which can also be hidden behind a sub


		with syntactic sugar where no variable is used:
		PbsUse 'sugar' ;  # define get_modules and get_modules_tests

		AddConfig MODULES =>  'module1 module' ;

		AddRule *.lib => tests, get_modules('MODULES') ;
		AddRule tests => get_module_tests( 'MODULES') ;

		# the above has the asdvantage of hidding all the details and keeping it Config and Rules only

		it is also possible to replace both rules by a meta rule:
		PbsUse 'sugar' ;  # define AddModuleRules

		AddConfig MODULES =>  'module1 module' ;

		AddModuleRules => 'MODULES' ; # looks like a module but is a rule wrapper

		we can even make it a one shot config + rules wrapper
		PbsUse 'sugar' ;  # define AddModuleRules

		AddModuleRules => 'module1 module' ; # adds config, adds rules


 		There are differences between the different solutions
			details are more or less hidden
				but it is still under the users control

			in the sugary solution, the rules and configurations are not created in the pbsfile
			but in the lib file, that's what pbs will report, except if we force it otherwise

			

		* Aternatives *

		* export the TARGET for the rules
			! a very bad idea, rules coould be far down the chain and would have to know about other
			rules


		* create config variables before running rules
		PbsUse 'ModulesConfig' ;

		AddConfig MODULES =>  'module1 module' ;
		AddModulesConfig 'MODULES' # Adds in the config everything a module needs
						# MODULES_PATHS => 'module1/module1 module2/module2'
						# MODULES_TESTS => 'module1/module1.test module2/module2.tests'
						# AddModulesConfig is verbose about the new variable creation

		AddRule *.lib => tests, GetConfigSplit => MODULES_PATHS ;
		AddRule tests => GetConfigSplit => MODULES_TESTS ; 
		

		or 

		PbsUse 'ModuleConfigs' ;

		AddConfig         MODULES =>  'module1 module' ;
		AddModulesConfig 'MODULES' ; 

		AddRule *.lib => tests, GetConfigSplit => MODULES_PATHS ;
		AddRule tests => GetConfigSplit => MODULES_TESTS ; 

		# pros and cons
		pros:
			very simple, no modifications of the API, flexibility for the user
			everything ends up in the configuration which is visible with --nc/--bni
			config locking/override is already in place
			rules are added in the right pbs file

		cons: 
			everything ends up in the configuration!
				the configuration is passed to levels below
					may be a good thing
					may be made LOCAL


Language independent builders
	when possible transform perl builder in external perl builders
		take the code and put it in a bin directory 
		if the builder accesses internals, split the builder in a builder that writes
			the needed information and a builder that is a shell command
		
			this has the disadvantage of starting a new process

	use inline::Language and keep the external language in the pbsfiles
		how easy is it to debug then?

Setting up paths for binaries under the build system control
	if the build system has a set of utilities that are used to build, add the set's path
	to $PATH before running the commands

		this removes the need for the build system users to change $PATH



	

Binary repository layer
	rather than as if a specific file has an md5 on the repository we should
	ask if it has a file with that md5, it doesn't matter where it is

	repositories can send a patch over the list of previously available artefacts to reduce traffic

	the artefacts are needed locally only when a dependent has to be build

	a node that needs to be build does not need to be checked with the repos

	feels close to what warp does except we need a physical file rather than a pseudo node in $inserted_nodes

	if a node needs to be build, it doesn't exist or a depdndency has changed, we can still find it
		using it's dependencies and config as a hash

	

in large project, digest files take a significant time to load (1s :) ) can the size be reduced?
	C includes are repeated many times

Can we get the nd5 of a sub? of a closure?

PrintDebug2
	we need to have a color tag at the beginning of print output to be able to differentiate
	different entries in the same category
		PrintInfo
			what kind of info?
				if we have three categories then it should be made clear
					print output to be able to differentiate
	different entries in the same category
		PrintInfo
			what kind of info?
				if we have three categories then it should be made clear
					aprint output to be able to differentiate
	different entries in the same category
		PrintInfo
			what kind of info?
				if we have three categories then it should be made clear
					■ ■ ■ 
					each ■ gets colored 
					alternatively the catagory name is displayed
--display build result displays result for each builder element

Warp verification message
	display some statistics about the types of nodes in the cache in verification message
	make display single line

		Warp load time: 0.06 s.
		Verifying warp: 3801 nodes ...
		Warp verification time: 0.14 s.
		Warp total time: 0.20 s.
		Warp: Up to date.
		Total time in PBS: 0.23 s.

		Warp: UP TO DATE. 3801 nodes (1400s, 1600g, 801p), load: 0.06 s. verification: 0.14 s
		Total time in PBS: 0.23 s.


wrapper to check test result
	AddRule :Name(test)
		Is("any_command arg %arg", regex)	



bash completion should be done by pbs as some options are loaded at run time
	wrong, the options are loaded after the application is executed so we don't
	know what will happend in the completion script if we do not run the current command line
	which is prohibitive for completion purpose

warp, on pbsfile different => need to rebuild everything
	because warp doesn't have pbsfile-nodes dependency? see mini warp


Add a mode to depend as little as possible, IE no C dependencies cache, no checking sub nodes if top node needs to be rebuild, ...
	generate the dependency graph after building if the information is available

--no_warp creates warp file even if it doesn't use it

how much more time does it take to create a warp 1.5 and 1.8 compared to only warp file 


declare possible top target in pbsfile
	allows us to scan all pbsfiles (what files?! pbsfiles can be called anything) to find targets
		 - can help the user decide what target to build or where a target is build

	we can cache the target names in warp

	target can be  pbsfile tag or a reh
		1/ Target(something)
		2/ Rule :NameMatch(/xxx/), :IsTarget ;

pbs/subpbs return a miniwarp
	there is no reason to give the top part of the graph to subnode 
		linking to existing node?
		better encapsulation of dub level

	the pbs serialized the miniwarp
		calling pbs can ask for a live sub graph

	calling pbs has to link nodes if they exist in the graph
		if same node and config
			otherwise we have conflicting configs

	create check file/build sequence at the same time as the creation of the mini warp
		we can parallelize the check 
		nodes in mini warp build sequence must be tagged if they are terminal
			we don't need to compute it that in the parallelizer if it is already done

	returning a warp file + check/build sequence means that we do not have to keep the live nodes 
	in memory
		NOTE: the build sequence is warp 1.8 first level dependencies, up to the calling pbs
		
		we can create w1.8 first level on the fly

how do we detect circular dependencies if nodes are depended in different process each creating
	a graph that is not circular
	
	we can run a check pass on the dry graph just for that, same as today but much lighter

	we can merge the build sequence and if a node is present twice, we have a cyclic dependency

	use tsort

work out the user help vs the pbsfile help
	=for PBS 


sub options set the parent option
	IE: --ttno needs --tt on the cli or it will have no effect

	writing --ttno will automatically set -tt

	if parent option needs an argument, display error message and stop

display options as a tree when -h is used
	category
		sub category
			top option
				sub options
	options
	├─ help
	│  ├─ -h|help = Displays this help.
	│  ├─ -hs|help_switch=s = Displays help for the given switch.
	│  ├─ -hnd|help_narrow_display = Writes the flag name and its explanation on separate lines.
	│  ├─ -generate_bash_completion_script = Output a bash completion script and exits.
	│  ├─ -pp|pbsfile_pod =
	│  │  Pod declared with "=for PBS =pod_formating title" is extracted when option is set
	│  │
	│  │  =for PBS =head1 Targets\n"
	│  │
	│  │  =item * all
	│  │
	│  │  =item * debug
	│  │
	│  │  =cut
	│  ├─ pod
	│  │  ├─ -d|display_pod_documenation:s = Interactive PBS documentation display and search.
	│  │  ├─ -pbs2pod = Extracts the pod contained in the Pbsfile (except user documentation POD).
	│  │  └─ -raw_pod = -pbsfile_pod or -pbs2pod is dumped in raw pod format.
	│  └─ wizards
	│     ├─ -wh|display_wizard_help = Tell the choosen wizards to show help.
	│     ├─ -w|wizard:s = Starts a wizard.
	│     └─ -wi|display_wizard_info = Shows Informatin about the found wizards.
	└─ colors
	   ├─ -no_colorization = Removes colors from output. Usefull when redirecting to a file.
	   ├─ -c|colorize =
	   │  If Term::AnsiColor is installed on your system, use this switch to
	   │  colorize PBS output.
	   │
	   │  PBS has default colors but colorization is not turned on by default.
	   │
	   │  Colors can be defined through switches (try pbs -h | grep color) or
	   │  by setting you colors in the environement variable 'PBS_FLAGS', ex:
	   │          export PBS_FLAGS='-c -ci green -ci2 blink green -cw yellow'
	   │
	   │  Recognized colors are :
	   │          'bold'
	   │          'dark'
	   │          ...
	   │  Check 'Term::AnsiColor' for more information.
	   └─ element colors
	      ├─ -cw|color_warning=s = Set the warning color.
	      └─ -ce|color_error=s = Set the error color.
	  
keep logs, output, ... for every run of pbs
	so we have something to look at when somethingg builds, or doesn't, when we expected it not to, even if pbs is most probably right

	one often reruns pbs just to get more information with an extra switch
	if the information is not to be displayed, keep its data in a separate file

	we need a system to cleanup periodically the unnecessary data, an option
	--run_data_limit/-rdl (and --no_run_data) which is set by default to, say 5, and that can be
	changed in the configuration.

		--nrd 		don't keep this run's data
		--rdl 0		never keep run data and remove the data found on disk

		if we keep multiple logs, we will need to keep multiple node data for nodes that are rebuild

	node data should be in the node's digest and only tree data kept in log
		we can reconstruct on the fly

		node digest's name should contain the md5 to ease its localization
		with command line tools

		node data should be kept as string to avoid compiling it when loading the digest
			in comments that we can "uncomment"
			in strings that we can ask the digest to return to us

		the node object could itself be the digest, eg: the node is serialized and it can
		be queried for its digest or data

		node data contains the build output

	* support --jobs
		builder creates node's data files

		master creates 
			warp file
			build sequence (linear, although it can be recreated from the graph if we have a list of triggered nodes)
			build event (queuing, starting, ending) which can be used to create a timeline
		
tree display additive fields
	--tt is the bare tree, one can add elements to the tree via switches
	completion and help are available

tree display color
	node name should be made clear
	additive fields are color like command line switch

HTML output
	terminal that is HTML aware to allow DHTL tree dumps

	fast output
		revert to pure text without html
		display html with max depth set
			user can still open data structure dump further
		load data on demand
			trees are just links

		filters
			show only some of the pbs phases
			show specific node
			show specific subsystem


	pbs shell should support HTML

continuous integration
	keep logs of builds and html interface
		builds are not started via interface nor via commit hooks

	check everything is commited
		commit in a CI repo

	builds when local machine is not powerful enough
		
	start remote build on local machine

	check and reuse environement if possible
		

IPFS as synchronized build cache
	a node to be build can request itself from some other build-node cache

	caches can  synchronize as long as they have disk space so if a node is 
	requested, it can be delivered in parallel from multiple build-nodes
Tmux
	like gnu parallel, in a specific session for the pbs run
		output  in a window or in a pane
		do not keep parts of the builds that succeed by default

	controlled by REH

Build distribution
	the build system can clone itself (and all deppendencies) to another computer
	if a node is changed
		if it impacts many other nodes, the node is synchronized and all the 
		build nodes co-operate to the build

		if it impacts little, we can know that from warp 1.8 data, the build is
		made locally and nodes are synchronized after the build

debugging
	when node fails to build, setup a miniature environment with copies
	of the failing node, the necessary dependencies (a warp tree for the nodes that only be
	present but not used), ... 


	use rules and REH
		ca setup an environment specific to the node type

		Rule
			MATCH_ALL
			REH_ON_FAIL => \&setup debug environment

Tmux test environment
	keep only failing tests
	stop after x number of failing tests
	

three pane debugger with preview
	a ranger like display parents (can be multiple parents), this, children
	preview shows rules, config, ...

debugger
	trace: shows how we got to the point where we are

	"next": show the rules that match, this should also show in the graphical browser

	in case of error show all the relevant data, including the results of the command, and the
	possible output, graphical browser shows what other rules would fail

	break point show type of breakpoint, pre-depend, ...

	set/unset debug point easily and quickly
		set and unset based on some code returned value
			ie, as long as number of nodes in graph < 50 continue
			have a rich data set the functions can work with
			data set is per node, module, level, range of levels, ...

	REH_DEBUG_SET_BREAKPOINT

environment variables
	pbs removes all the environment variables
		this helps but when a sub command fails or produces an unexpected output
		(because it did not check properly) this helps the user very little

	pbs removes all the environment variables if option is set

	pbs replaces all the environment variables with "SET BY PBS" to make it clear

	find a system to "break" the commands that use environment variables

	removal of environment variables should be per node/level
		node/level can set variables

	warnings if environment variables are set or not set

 
debugger watches
	extracted by code snippets from the available data set, they don't have 
	to represent a variable, they can represent whatever, a bit like an element in
	a bash prompt
		
	doesn't have to be perl code or an element of the graph
		ex. a service is available on a remote server
		ex. cpu load, free disk space, ...
	
	good if implemented as debugging REH in a rule

warp does not need normal digest
	when running in warp mode and something need to be rebuild
		this helps but when a sub command fails or produces an unexpected output
		(because it did not check properly) this helps the user very little


warp does not need normal digest
	when running in warp mode and something need to be rebuild
	the no-warp mode building is run; it generates digest files
	and c dependency files. those files are not used by the warp 
	mode. the warp mode should apply to normal build too to avoid
	generating the digests. normal mode data normally ready from 
	digest files can be obtained from the warp file.
	Note that this is not a big problem as digest file generation 
	is trivial and that most of the time is spend computing md5
	which are also needed for the warp file. It does have an 
	"advantage" if the warp data is deleted or a different warp
	mode is selected, the normal mode is fast.

warp 1.8
	can we reduce the format of the warp files, textual amount as
	well as fields in the warp files

	there is ROOT directory in the warp 1.8 data that should not be there!

	single node warp file take the longest to process, is there a way to 
	optimize the process by collating related nodes in the same file?

	node regeneration should not need to set BUILD_NAME
	try to make node regeneration more light weight
		if some rarely used plugins need more data, it is better
		to do more re-generation in the plugins

parallel depend
	done on the subpbs level
	can generate a mini warp
		who merges back
		merging should be distributed too

	merging is mini warp is no necessity, we can follow the chain
	and read multiple mini warp files

	mini warp must merge very quickly
		two data sets
			one with complete data
			one lightweight to merge quickly
				see warp 1.8 node regeneration
			keep a md5 between them
		mini warp have warp 1.8 format
		merging 
			merge checks link errors
			adding node md5 to list
			single node warp files need parent dependent added
			 
	a set of hierarchical mini warp can be merged at multiple levels
		only merge the quick data and keep reference to the complete data

	

warp nodes can be formatted as the reconstructed nodes to speed up loading
	
nodes that are the deepest trigger the most files
	verify them first as they re-vivify the most nodes and we cache those

nodes that trigger often are more likely to trigger again

sample data digest generation for big files
	md5 on a 5 Mb file takes longer than on a 4 kb file
	compute md5 on a few well chosen sections  rather than the whole file

option to generate digests for source files only
	MUCH less secure but much faster

	we should actually CHECK the digests for source only, we can still generate digests
	for generated files, that would give the possibility to run with checking or do an
	external check if needed.  

check optimization in Ag
	https://github.com/ggreer/the_silver_searcher

node name completion can be done with the help of nodes listed in the warp file
	we still need to know which warp file

log all builds
	every command line that builds should be save in a log

keep statistics about what is the most often build
	gives us an idea about what has most impact on the system to build and
	what part of the build system is most often used

	let a user specific post pbs handle that (we can give an example)

file database vs classic database
	files allow direct manipulatiom
	cdb allows fast seach

	why choose?
		generate the files and fill the db

time cursor (see Log)

	a history of the build is kept so that a cursor can be placed at a user chosen
	time when she can look at what the graph looks like, what will happen next, what rules
	inserted a node, ...

	keep a history of the graph generation, a history of the  build makes no sense as we can build in parallel
	and the order changes. It makes more sense to get the build for a specific node and select
	its children or parents to look at their build.

C depender triggering
	if dependency file does not exist, trigger and post generate the file instead for an immediate generation 
	this should be the default but an --option may be added for people that want an analysis not a build

C depender generates digests in mini warp format (because it is exactly that) and the 
	linking of the mini warp nodes for the C depender is no different from other
	mini warp

mini warp does not contain node data, just enough for regeneration

document why, when using warp, the C dependency files are neither used nor verified
	=> they are merged in the warp graph and warp doesn't care about them, but the 
	dependencies are still verified

Output from Hashes must be sorted as Perl randomized the key order

shell build nodes can receive tools, commands and the code to work on from master
	reduce the need for a shared file system

shell build nodes can share code with other nodes
	when the master starts a build, it also starts the sharing of the code, while the
	master works to find what is going to be build, the build nodes share the code.

shel build nodes keep a repository of shared code

shell build nodes can update their repository
 
build system has startup rules that install tools and checkout code
	although checking out the code with the build system (e.g. in the same commit) makes more sense

parallel depend
	OK to have multiple instances (in each process doing the depend) if the common nodes have the
	same digest, including the configuration. if the digests are different we need a mechanism to
	let the user decide what happens next.

documentation is part of every release

REH instead for named dependencies or as a complement
		rule
			named => (c-dependencies => a, b, c), 
			c-dependencies => (a, b, c), 

	this also allows the REH to 'type' the dependencies and itself


REH user defined type
	allow visualization of the REH in a user specified manner
	e.g. also when the dependencies are listed, the dependencies from a specific REH could be in red color


display dependency list 

	Node [V] './objects':
		Inserted at ./pbsfile [PBS]:__ROOT.
		dependencies:
			./1.objects
		rebuild because of:
			./1.objects (Subdependency or self)
		matching rule: #2[B] 'objects'
			=> ./1.objects
	  

	displays the REH that matched and uses the color from the REH type


rebuild because of: ./1.objects (Subdependency or self)
	be precise, dependency or self, or both, and which dependencies


always save the subpbs configuration
	possible to query the configuration for a specific node
	history of configuration is also displayed

build subset
	a better way than node@root and save/load configuration

			  .---------------------.
			  | .---.               |
	       .------------| A |--------.      |
	       |          | '---'        |      |   wait for B to build 
	       v          |              v      |   correctly to build  
	 .-----------.    |            .---.    |
	 | subsystem |    |            | C |    |
	 '-----------'    |            '---'    |
			  |              |      |
			  '--------------|------'
					 |
			  .--------------|------------.
			  |              |            |
			  |              v            |
			  |            .---.          |
			  |        .---| D |---.      |
			  |        |   '---'   |      |  Only build this till OK 
			  |        v           v      |
			  |      .---.       .---.    |
			  |      | E |       | F |    |
			  |      '---'       '---'    |
			  |                           |
			  '---------------------------'


	When building A, if an error occurs, find which subsystem the error occurs in and
	give all the necessary information to build that subset from the command line, see
	setup subset build for a manual setup

	building a subset is like building a specific subpbs with a saved configuration, there is no
	need to build the parents of the subset

	it is possible to limit the build to the first level of the subset even if configuration
	or graph changes are made at the subset level

	when building a subset, make it clear to the user that it is only a subset build so that 
	it is clear that a top build needs to be made, return an error code and the warnings


setup a subset build
	the user can ask the build system to generate the command line to build a specific file or
	subsystem.

	say that a subsystem build a library
		the user can ask for the closes pbsfile to build that library
		the build system
			creates a (universal based) command line
			creates a configuration file copy for the subsystem
	
	the creation is done 
		based on data in the warp file
			if no pbs related files trigger (libs, pbs, pbsfiles) 

		by running the system in depend only mode to generate the necessary data

user defined node type
	subsystem, debug, UI, ..

	the user can give attributes to nodes

	those attributes can be used to 
		display nodes differently
		display the attributes in a --tt
		match rules
		search for the nodes 

	queries can use node type

universal root for build
	it is possible to build from any location
		a cwd not the root of the system to build
			finding the files under . (virtual root)

		a cwd root of the system to build
			normal build

		a cwd under the root of the system to build
			subset build			

subpbs rule should not care about the path of node
	rules now have to match */regex as the start node has a path
	but the path doesn't make sense in the local environment of the subpbs, the nodes
	should get their path based on their parent's path. this would simplify the run of 
	pbs -load_config as we can give the local node name instead.

	some node have path information ./lib/xxx gets under the pbs root under ./lib (late depend)
	some nodes may be related to the root (/)

	before starting a subpbs, the node to depend could get an alias without the path
		

 	this would allow moving whole subsets of the system and still be able to use the warp file
	as is since there is no path recorded anymore




All display code is a plugin, allowing the user to change how the display looks like
	without changing the core code

	use a templating system or subs

	can the display code be triggered by injected rules?

H files are dependencies of the object file
	if two object files use the same C file, the dependencies may be different because of the
	object file configuration. Linking the C file with a configuration that is wrong, when
	depending the second object file, is an error.



virtual nodes without dependencies ALWAYS trigger
	a virtual node without dependencies always trigger
	there is no way we can know if a virtual node without dependencies "succeeded"
	we can know that it failed because the commands it runs fail but if they all succeed
	we don't have a node we can make a md5 on so we must re build the virtual node each  time

	need example of virtual node without dependencies

	this is so frustrating. All those years talking about virtual node digests in complex setup (subpbs linked nodes, warp graphs, ...) when we should have kept it simple and see what the problem is with VIRTUAL, it's simple, we didn't know where to write its digest (that's so hard to decide) and that it didn't trigger. So rather than do the right thing we introduced FORCED, Do I hate make and all the make files that other morons like me wrote that forced things.
	FORCED must die and than everything will be fine because we must do the right thing.
	a virtual node, whatever the context, as it has no digest will ONLY trigger if one of its dependencies also triggers. This means that you can add a virtual node rule and it will never trigger, you can modify a virtual node rule and it will never trigger, virtual nodes don't trigger, they have no control over their triggering, it's the dependencies that trigger them, and the dependencies may trigger for various reasons none of them related to the virtual node rule.
	You can even write a virtual node rule that has dependencies on source nodes, EG document the source nodes, and it will never trigger.
	VIRTUAL nodes don't trigger, so rather than make them trigger properly we FORCED them to trigger, I hate make.
	if virtual nodes have triggers, they will trigger on a dependency that trigger, as before, on source files, and on the pbsfile that where the virtual node rule is defined. 
	Why did we need a virtual node digest? For everything!
	Where to put the virtual node digest is obvious, in the out directory, as everything else. What if they match a directory name, no problem, the directory and the digest don't have the same name.
	It's easy to be smart after the fact but it is pretty obvious that if you don't handle nodes the same way, there are going to be differences, why didn't we handle VIRTUAL  as a normal node? why didn't we handle VIRTUAL as we defined in the documentation " a node that has no physical representation in the file system", a 2 lines check, but instead make a monster out of it.
	Next PBS will be better, VIRTUAL will have a digest. 

	what if the dependency is a virtual node itself, what do we write in the digest?
		the virtual dependency digest!

2 dependents share dependencies. I don't think it will be in the form t1, t2 -> d1, ....
	but rather t1 -> d1, ... ; t2 -> get_deps(t1).
	that allows a tx -> get_deps(t1, t2, whatnot) picking up dependencies other nodes have without having to change t1, t2 pbsfiles. probably late_depend will work on them too so t2 can be dependent on t1 dependencies even if t1 is not in the graph yet


NAME => 'any generated data is exportable to another format',

NAME => 'defining_subpbs_with_a_sub',
LONG_DESCRIPTION => <<'EOD',
	give a sub that checks the type of the node to generate and create
	a virtual subpbs if non exists in the sub directory (for known types)
 
	show a message if a subpbs is generated
	use pbsfile if it exists
	if given --generate_missing_pbsfile, ask if a real
		pbsfile should be generated. practical if the virtual
		generated pbsfile doesn't fit (ex directories or files to ignore, more configurations)


REH to match configurations
	AddRule
		MATCH => $regex
		MATCH_CONFIG => ...

	this is related to REH_IF

REH to match steps in the pbs run
	this is already done in the registration of the REH which doesn't look like rules
		and REH can not be registered to multiple steps 

	it also need to match a node and pbssteps are not nodes (yet)

	a good example is -gtg, we generate a graph at specific moment
		gtg is implemented as a plugin in pbs1, thus pbs calls it

All output is done via a registrable sub to allow PBS2 to be embedded in another
	system, EG: neo

	display system can be setup to display information in specific cases only
		node
		pbsfile
		pbs run
		...


REH  NAMED_CONFIG AND NAMED_CONFIG_GROUP
	no problem adding the REH but how does one get to the config
	by name later?

command line options and wizards
	can a rule match a command line option as if it was a target
		MATCH_CLI => /dd/
		
		how do we define our command line options
			some global place plus in the plugins
	
	wizard is just a target 
		how do we add all the targets?
			just load a library with the rules

plugins
	some are just targets, ie generate_graph, which need to be run at specific
	pbs time

	Rule 'generate_graph'
		MATCH_CLI => /-gtg/
		GENERATE_GRAPH_REH => ....  # knows when to be run


	bleah! make the target a virtual node!
		Rule 'generate_graph'
			MATCH => '__pbs_graph'
			AND_MATCH_CLI => /-gtg/
			BUILD => ...
		
plugins
	some are called during the pbs run, eg they register subs that pbs calls
		EvaluateShellCommand
		CheckNodeName
		
		The problem being is that they are not node specific
		we can eliminate global plugins and query the nodes for the 
		plugins to run. A global plugin simply matches all nodes

	AddRule
		MATCH => $regex
		EVALUATE_SHELL_COMMAND_specific => 1

	We can't audit plugins as we did with pbs1, finding the plugins becomes more
	difficult as they can be inlined 

prefix all the REH that have an impact on internal implementation
	DISPLAY_DEPENDENCIES => 1
	PBS_DISPLAY_DEPENDENCIES => 1

	except if it is the DISPLAY_DEPENDENCIES REH itself that does the displaying

generalize the C depender
	- what configuration variables must be in the configuration should not be in the code but arguments
	- the same code can be used for other types of files
		- creators use an equivalent mechanism, could be merged

	AddRuleTo 'BuiltIn', [POST_DEPEND], 'C_dependencies', \&C_SourceDepender, \&C_Builder ;

	Rule 
		NAME =>  'C_Depender'
		REGEX => '*.cpp'
		C_DEPENDER => { ........ }

	Rule 
		NAME =>  'C_config'
		REGEX => '*.cpp'
		CONFIG => { ........ }

		C_DEPENDER_VARIABLES => named list




-bni ,which show a node header, makes it clear which node is being build, there is no such system
	for -dd, the display is very compact

NODE_LINKING is a PBS step with potentially matching REH
	what configuration to check
	allowed or not

configuration dependency for node
	knowing what configuration is used to build a node makes it possible to check
	if a linked node uses the same configuration as the current pbsconfig  build node
		maybe the linked node, with a different configuration and dependencies is OK!
			need a mechanism to allow that.

	extract all the configuration variables from the build commands
	force declarative style for linked node checking
		note that this can still check the build commands

	addrule
		name xxx
		regex '*all*'
		NODE_LINKING_ENABLE => 0 # current pbsfile run must provide one

		NODE_LINKING_ENABLE => 1
		NODE_LINKING_CHECK_VARIABLES => ['name', 'name2', ..]
		NODE_LINKING_CHECK_VARIABLES => {'name' => \&checker, ..}
		NODE_LINKING_CHECK_COMMAND_LINE => 1

		
	knowing which configuration is used also allows to not rebuild some nodes declared in
		the same pbsfile because of a configuration change that has no impact
		on the node

if a rule uses sub rules, EG lib uses object rules, when the rules for obj
	are used, the user can easily trace it back to the lib rule and to where it was used
	
	IE. inserted_at is not enough as all libs are inserted at the same place, the lib rule

move statistic display to plugin (or plugins)
	add a plugin display point in pbs

console
	possibility to query between steps

	starts build or loads a graph dump

	loads warp and checks
		query about warp and 	
	
	depend
		can stop
		can add breakpoints
		can query about graph or node
		can generate graphs

	check

	build
		
option so warp information display is mutted

ExcludeFromDigest may not make sense
	use to say that a node is a source node, better call it that
	why not give a digest to a source node
		win time generating and checking it
		forces the node to be treated differently


EvaluateShellCommand REH
	pre-build REH

	does not evaluate the shell command but createa node specific configuration to match %Whaetever
	
	this allows us to define the REH in a user lib and it can be reused

	a better name is REH_EVAL_CONFIG for a generic sub declaration 

REH can be argument less
	they are more like sub calls then

	this allows a more declarative style for the REH tha do not need arguments
		Rule
			Name => 'declarative rule'
			DECLARATIVE_REH,
			...
			;
		
Pbs creators generate mini build systems
	mechanism used to generate and check the creator digest
	pbs command line switches (-bni, -conf, ...) are also used
	
Mini warp are mini build systems
	getting into a sub directory the user shouldn't hav eto do more than "pbs" on the command line
	to build that sub system again, with the top config!

Use file type "matchers"
	rather than use regex use a file type (which still can be implemented as a regex for simple cases)
	the "matcher" can match depending on a configuration (eg the current OS, tool chain, ...)
	
multiple GetBuildName subs defined in the code

example in Pbsfile/Creator do not work

Merge STE PBS

Create template/example for STE like data driven pbsfile (neo)

option to filter out/in nodes when displaying tree
	generate list of files that are displayed and not displayed, their amount, ...
	
	if options are pbsconfig variables, the REH an use them

possibility to generate multiple trees with different roots
	to display only the relevant nodes
	can still filter nodes in and out
	
generate DHTML tree


Multiple node builders
	builders run in the order of rule matching
		option to display the builders and their order
		
	builders declare if they are singletons, an error is generated if builders exists before or are added after
	
	
build phases
	a node with multiple builders has its builders run in a specific order
	a builder declares which phase it belongs to
	a rule can declare the phases a builder can belong to 
		check is done for multiple rules declaring different orders
		check is done to verify that each phase has a builder
			should it fail?
	
	
Mechanism, at a higher level, to declare in which order REH are run
	similar to build phases
	
	
Builders chaining
	get all previous builders results
	get information pointer to the previous builder
	get the last builder build buffer
		can request the build buffers of all the previous builders
		
	builder can make variables available after build
		eg: tell the rest of the build chain what files are created, ...
		

Build buffers 
	switch to keep build buffers
		filter which ones are kept
			faild ones always kept

failed build buffer
	always keep the buffer
		garbage collect the filters after successful build
			rule matching all the files
			do we want to garbage collect at the global level or after each node, after each subpbs, ...
				object_files can collect the names of the buffers to garbage collect

			do we keep the build buffers in case of multiple failures?
		
		
	keep information about the builder
	create a mini build system
		configuration
		possibility to run just that builder/ builders for a node
		
	
possibility to display the tree in warp mode
	global tree, warp tree
	filters should still apply
	
File not found for MD5 computation
	most probably a node that is not declared as virtual (although we have defined somewhere that virtual nodes should have a digest)
	builder failed
	=> display as much information as possible
		mini build system for just that error
		
generation of mini build systems
	in big systems, the developer does not want to re-run the build system just to find out what the problem is but wants a small build to debug the problem
	
Post pbs 
	can be implemented with a rule that matches the pbs run virtual node
		better than a rule that matches a target in case there are multiple runs of the pbsfile with different targets
		REH_POST_PBS => \&post_pbs_sub
		REH_POST_PBS => GetPBSConfig('POST_PBS_FILE')
		
	C depender should use the flags defined by the user rather than force flags on the user
	
	
override after **------ -dpl
	don't know what I meant
	if subpbs run (two extra phases here prepare to run subpbs and run subpbs, plus subpbs ran) matches a rule, it can display something else
	
	
Make clear how build command line parameters (%FILE_TO_BUILD, ...) are defined and handles
	also defined in plugins or rules
	
	
plugin to generate report on configuration usage
	a post pbs part of the examples in the distribution 
	
	configuration in, defined, used, delta with parent, delta with children (available after the subpbs runs)
	knowing how many configuration variables (along with rules, targets) can be used to automatically find what pbsfiles should be split
	
give example of C code generation
	or anything that needs to be depended
	explanation and example of IMMEDIATE_BUILD or creators
	
Creator, IMMEDIATE_BUILD, VIRTUAL, FORCED
	all can be REH!
	
rethink/document IMMEDIATE_BUILD FORCED, LOCAL_RULES and other exceptional mechanisms

put generated file and source file under version control
	allows us to run pbs and undo what the run as done
		we can pattialy undo if we want

	
IMMEDIATE build in a REPL
	user can point at a node and ask to have it build
		a bit like IMMEDIATE_BUILD
	

IMMEDIATE_BUILD
	first depend the node then build it before returning from the node's depend phase
		what if the node has a late dependency?
			warn, error, wait till all the late dependencies are available to run the node's
				IMMEDIATE_BUILD (fancy!)

CREATOR differs from IMMEDIATE_BUILD as the node is immediately available for other depender to run on it
	eg: C file creator has the node on disk before the C file depender runs on it
	
	if IMMEDIATE_BUILD was used, the C_depender wouldn't find the file on disk

	one solution to run multiple rules
		IMMEDIATE_BUILD C -> some dependencies
			create C

		C -> C_DEPENDER
			CC C -o ...

		the problem is that rules are often loaded and the first rule must be registered before the standard C rule
			but we have a solution! as the rules are named it is possible to ask for a rule to be run before or after another

		in rule.pm
			[NAME] C -> C_DEPENDER
				CC C -o ...


		use rule.pm

		IMMEDIATE_BUILD C -> some dependencies
			BEFORE_RULE => NAME
			create C

	next problem is that a node that has been depended (possibly by multiple rules in the same package) will
		not be depended again but linked to

		the example above works ass all the rules are in the same package but if the are created in one package 
		and the rules to build them are in another package, those rules will not be run

	next problem is that the node is build twice and nodes are never build twice
		a solution would be to make the node as not build but immediate build, that stops us to immediate build a node twice


	we REALLY want IMMEDIATE_BUILD to be a normal build and CREATOR to be an IMMEDIATE_BUILD because we just have one system then
	and there is no need to handle digests, caches, warp in a special way as we do for CREATOR in pbs1

	REH_CREATOR can run before the depend step
		here's arequirement that says REHs can define their step name and put it before other steps
			REH_CREATOR, STEP_CREATOR before STEP_DEPEND


		REH_CREATOR has its own dependencies
		REH_CREATOR can do whatever it wants, including generating a build system and run it!
			we don't need to generate files (PBS_CONTENT) but it's a good idea, we get a log of the build to
				pbs genertes a file for PBS_CONTENT anyway
			
			it's possible to run pbs (since we are in pbs) and ask the core to build in a different graph layer

			it's possible to run pbs as an external command and as it to generate a layer that is imported

			it's possible to run another build system, or even a bash script, an import nodes after


importing graph layer
	pbs can serialize and import layers generated by other builds


REH_TRIGGER rather than pbs functionality
	starts a subpbs before the REH_LINK_TO
	
C_FILE_SYNCH
	it's a post build REH
	Make it visible
	
ON_ERROR REH
	for un interesting parts of the build just ignore the failures
		need to make it very clear
		
		
Express complicated dependencies
	references to dependency lists, but still locally defined
	depender subs are all mighty already
	simple interface to the dependers, better wizard
	
user defined wizards
	possibility to have multiple wizard directories 
		no need to put user wizards where pbs wizards are

replace plugins with rules
	CheckNodeName, rule matching all nodes
	CreateDump, rule matching top node, what if the build fails
	CreateLog, rule matching top node, what if the build fails
	EvaluateShellCommand, replace with EVAL_CONFIG, there is no reason to evaluate the shell command just
		to handle %SOMETHING. Adding SOMETHING to the config as the side effect to be self documenting
			- SOMETHING stays visible rather than transient during EvaluateShellCommand
			- the rule doing the evaluation is logged
	ExpandObjects, just a specific EvaluateShellCommand that should only define a configuration variable
		can be used directly as a REH (pre build) in specific rule:
			Rule
				Name => 'some_lib'.
				Match ...
				...
				ExpandObject,
				Build => "linker %DEPENDENCY_LIST_OBJECTS_EXPANDED"
				EndRule

			or as a REH global in the pbs run
			
			use 'Lib/ExpandObjects' ;
			# which contains
				Rule
					Name => 'expand object'.
					MatchAll,
					ExpandObject,
					EndRule

			or use the top rule injection mechanism at the top and have the rule available everywhere
			
	FileWatchClient, REH CHECK
		advantage, 
			nodes checked individually, maybe some optimization can come from it
				IE: node does not have to check itself if dependency has triggered already

		disadvantage:
			
	GraphGeneration, rule matching top, or any node if sub graph is wanted
	PackageVisualisation, rule matching all nodes
	PostPbs, rule matching top node, what if build fails?, can we have nodes representing PBS steps?
	SimplifyRule, a simplified, named, REH
	TreeVisualisation, rule matching top or any node
	Visualisation (-dc and other node visualisation switches), rule matching any node


	making the plugin match nodes means that we can run some functionality at any level we want
		IE: sub graph, sub tree, ....

		timing REH
			today we get timing for the whole build or a pbsfile run, we could get information
			per node or sub system

		most display options, especially if we have pbs, pbs step, config, ... nodes


plugins cli switches handling
	today:
		so they are called, ie: --generate_tree_graph
		so they get arguments, ie: --tree_graph_file xyz.png
		so they get options , ie: -- tree_graph_no_configuration
	
	call plugins/plugin-directory like switches?

	=> scan directory for switches?
		call a registration sub, let it register itself, return registration data
			cache the registration if libs are unchanged
				could be useful to cache the rules
					cache them in memory if server is used
	
	=> solution
		remove possibility to set path in prf, instead have a pbs_set.prf, this removes the need
		to run pbs.prf.
		
		pbs can scan the cli itself, as it already does, for cli path options

		
	defining "plugin" switches
		plugins are replaced by pbsfiles, libs really), so any pbsfile can ad switches
			the switches are scanned at start time, the pbsfile is loaded and registration
			sub is called in the pbsfile, the registration sub is replaced by a NOP sub, or
			a verification sub to catch non scan-registrated pbsfile which call the sub.

		the pbsfiles that define switches need to be declared in pbs_set.prf
			pbs can scan a single file, a directory, a directory with sub directories, use
			regex to select or deselect specific files when scanning a directory

		the options can take arguments, be called multiple times (array), ...

		option definition contains a help text

	user can define it's own switches via sub in a pbsfile (lib) where options are defined
		define aliases for pbs switches, eg: for switches that do not have short versions or simpler names
		switch grouping, eg: --debug for "-dd -dur -post_pbs ..."
		alias for -D, eg: --user_defined 1 => goes into the config as -D special=1 would do
			a nicer looking integration of the user's process

	we can verify which user defined switch that was put on the command line was not used during the run
		and warn user!

	option can add target
		also good for pbs so we can make some pbs options rules
			-h, -v, -gtg, -tt, ...

	user options can be cached so the pbsfiles defining them do not need to be reloaded
		"generating user options cache ..." # no cache
			--display_option_cach_generation
				shows what files are scanned and what options are added

			--no_user_option_override makes the optionsimmutable 

		"" # silent if cache is valid
		"regenerating user option cache", same options as above apply
		
help is a target
	option -h still exists but it adds a target to the top pbs

	let -h display user help rather than pbs help, user decides by renaming rules if they want

	possibility to tailor the pbs help by removing or reordering entries which is not easy when the
	help is part of PBS distribution.


wizards are rules and started with targets
	wizards target and normal target can be mixed, which is surprising and maybe should be warned for

	wizards are still defined in the wizard directory for pbs but user wizards could be anywhere as long
		as the rules are included in the pbsfile

	option -w can add the targets

keep a list of all the files loaded
	libs, pbs modules, prf, plugins, ...

	keep in warp file (today we have prf data entries but it's not set properly!)


warp file is a pbs run file
	it contains information about a build
	some of the information is warp speedup information
	warp 0 has a warp file


if a hierarchical display is requested (IE pbs config for all pbs runs, config, ...)
	display a delta between the level
	if no delta, display changed values in different colors
	if config is local, display it in special color

	use two color scheme for levels
		eg: level % 2 in green, level !% 2 in bright green or cyan ...
		it's easy to miss the level change in large data amounts

256 color mode if the terminal supports it
	not 256 color but different colors for different elements, user defines colors sequences they want
		we stop using Terminal::AnsiColor
			or recognize "red on blue" otherwise use the user code
			or immediately tranform "red on blue in the config" which is a user config so we don't care about it

Immutable rule set
	pbsfiles that always generate the same rules 
		no if-else or REH_IF used to decide which rules are added
		they can use configuration variables in the builders

	can be cached in memory
		pbsuse should be faster
		in server memory in between pbs tool runs

	may be serialized in a more efficient format than the pbsfile

	cache is invalidated when pbsfile is changed (clearly!)

