
REH_TIMER and REH_NO_TIMER
	possibility to add timing via rules
	
	the timer need to either wrap all the builder or to be run at the start and the end

	how do we time individual builders or build steps?

	can we use REH_TIMER to time the loading of pbsfile and where is the statistic gathered
		?REH_STATS 'where/how to save'

	display of stats can be via a rule applied on the root node
		? how do we display stats on a failed build
	 


pbsfile is normal perl file
	running subpbs is, already, handled by a reh, one less thing to have in the pbs core

	packages, package configs and anything local to the pbsfile can be handled in a normal pbsfile

	global actions are more difficult
		options, prf, targets handling, colors, debugging, progress bar,
		statistics, indentation level, plugins, ...

		phases: depend, check, build, post_pbs
			maybe possible to replace them with rules

	possibly 
		use "pbs" ;
		use "reh_some_flavor" ;

		the pbs module can instantiate a singleton that does what pbs.pl does

	what did pbs_use do that use can't 
		adds the required package to the current package dependencies
		load time statistics


rule component can be shell scripts, service via rpc or http, ...
	rule name, regex, depender, builder, node_sub, ...
		perl dependers have access to everything, that's difficult when the depender
		is an external command. 
		
		we can chose some data and either pass them as arguments or serialize them in a file

		the great advantage of REH is that we can have different REHs depending on how we want to call the external code

		the REH can be given a command name to run and that command can be an executable with a shebang
			external command can be written in any language

		REH pointing to a language and give code is possible too
			REH_CMD_DEPENDER 
				language => Python
				code => some code 


		wich takes us to other languages to write the pbsfiles
			given that REH are not built in but included, nothing stops one to load language
			specific REHs

			use 'perl_reh' ; # with the possibility to filter what reh and change their name

			Rule
				Regex => /must be perl regex/

			use 'java_reh' ;

			Rule
				Regex => /must be java regex/ 

			making pbs a REH tunner and nothing else

			=> of course there's more than writting the REH in other language for good support
				the possibility to define variables, includes, ...


	can AddRule be a REH?
		rule name needs to be a REH to be handled in another language
		
		Merge AddRule and REH_MATCH
			AddRule
				NAME         => 'first',
				Match        => /regex/,
				Dependencies => qw(x y z),
				Builder => 
					[
						'do something' ,
						'do another thing'
					];
  

			Match /regex/ =>
				NAME         => 'first',
				Dependencies => qw(x y z),
				Builder => 
					[
						'do something' ,
						'do another thing'
					];
	
			Target /regex/ , qw(x y z) => # target is AddRule, Match, and Dependencies at the same time
				do(
					'something' ,
					'another thing'
				);
	

? stop mandating NAME
	naming has the main advantage of forcing the user to think about what she's doing
	debugging gives the file name and the line, and could print the rule, matchers, ...,
		reducing the need for a name

Target-specific variables
	are considered harmful by other make implementations: kati, Mozilla pymake.
	Because of them, a target can be built differently depending on if it's built standalone, 
	or as a dependency of a parent target with a target-specific variable.
	And you won't know which way it was, because you don't know what is already built.  
		=> this means the node config has to be added to the digest
			we do that for some variables for object files, a generalized mechanism to chose which variable is needed
		=> pbs checks configuration variables when linking nodes

let user define log level
	use relative level, ie not a fixed value but relative to already defined levels (text without value)

intermediate files
	no such thing in pbs since we need to have a digest for any file that's not a source

structured text output
	output structured object (yaml|json|perl) natively and let external application transform the output for the user

		pbs ... | special_pager
		pbs ... | html_data | nc ... ; pbs_web ... &
				pbs_web can keep sessions, have timeline, ...


show current directory when executing commands
	--pwd

	we can also add the cwd to thwe structured data


work flow primitives
	on_error
	or_build
	and_build
	try_catch
	! parallel => everything must run parralel, one can use a dependency to synch if necessary

	distribute
		to heterogen nodes, with synch, timeout, re-try on a different node

Interactive build/ kinda REPL except it runs in or around the build

	given a dependency graph, do interactive steps for debugging and recording
		close to debug mode but rather than running in a debugger user runs in an 
		interactive shell.

		INTERACTIVE_REH => 1
			BUILD => [shell command 1, shell command 2, ...]

		at each command start a shell, with the command inserted, and let the user control the command
	
	save the session to create build commands from it

	interactively add node to graph
		building node_1 the user realizes that a dependency must be build first
			add_node $current node dependency
				adds the dependency and takes user to the node build where
				commands can be added
	
	if we could record the state of the file system (hu hoo docker!) we could go back and 
		forth in the build time line.

	
PBS2 
	AddNode adds by name
		creates node if necessary and adds it

	we should be able to give already created nodes (eg: from cache or other graphs) to AddNode

	node and node content can be separated


matching urls and other objects
	rule 'name' matches some_URL, just a node in the graph, doesn't matter if it's an url or not
		depend on nodes which themselves can be as remote as this one or not, again just a node name
		checker the digest for the file, which may be on another machine
		builder, can be local or remote, pbs doesn't care as long as it gets a digest
		digest generator computes digest of not

	checker and generator could be the same code
	

	alternative: url is just an installed file
		url/file depends on file
			checker checks the remote file
			builder copies dependency to remote
			digest returns the digest of file

		file depends on local dependencies
			builder


	if the object type corresponds to a set of specialized depender, checker, builder, digest generator (or a subset of them)
		we can wrap those REH in one of
			Specialized add rule
			Specialized add REH


		AddRule
			name => *.special
			depender => special_depender
			checker => special_checker
			builder => special_builder
			digest => special_digest_generator

		AddRuleForSpecial
			name => *.special
			reh => ...

		AddRule
			name => *.special
			special_REH()
			reh => ...

		*** all user defined
			but depend, build, check, digest must be overridable

		Best would be that AddRule adds the default depend, build, check, digest (making AddRule a user defined function)
			overriding any of the base REH can generate warnings, log, whatever the REH want
				How does the "default" checker REH know that it has been overriden?
					Does the overriding REH manipulate the REH list?
					Is there a single slot for checker REH? or builder REH (although we already want multiple build REH)
				Do we need to allow the REH to check the REH list? 

		Addrule is in a module that needs to be pbsused

		top pbs just runs the top pbsfile

shorter rule syntax
	AddRule
		NAME => 'the name',
		REGEX => "some_regex",
		ACTIVE => 1,
		COMMENT => "some description",
		REH_WHATNOT => 1

	Rule 'the name', MATCHES("some regex"), IS_ACTIVE,
		COMMENT("some description")
		REH_WHATNOT => 1

command to add non active rules
	Rule 'the name', MATCHES("some regex") ; #ACTIVE
	
	NOP_Rule 'the name', MATCHES("some regex") 

can REH add REH in rule or do we wrap the addition of REHs in a normal function?
	
remote file syntax
	all files are local, how do we describe and access remote files

		service:server:/path/file
			service scp, webdav, http, ...
			credentials (and the dreaded key ring)

		virtual file or state
			eg: entry in DB
			not different from service:server/ ...


		difference with local file is that the signature must be computed differently
		pbs dependent doesn't care a node just a name


		Rule 
			Match(/$URL)
			REH_COMPUTE_SIGNATURE(any code/pre existing code reference)
			BUILD

		
		Rule
			match_ssh_file $server_file, credentials:xxxx
			# this also adds a REH_COMPUTE_SIGNATURE to the REH list

			# much  easier to have match_ssh_file return a list of REH rather than be an REH

REH_COMMENT
	(maybe backport to pbs)
	display the comment when the node is build
	comments could have level, like a log, and be shown over a threshold
		threshold should be dynamic

pbsuse
	does it look in the pbsfile directory or just in the include paths?

	it's not scoped within a rule (do we want that?)
	the include path are set on the CLI
		can be hacked via pbs_config but it's  hack
		needs a clean interface, with warnings, and scoping to local pbs run

	warnings (option?) if multiple files with the same name exist
	
	can we generate the libs dynamically?
		use case? (we have the mechanism to generate pbsfiles dynamically)

	modules are assumed to have a pm extentsion
	full path modules are handled but ./module is looked for in module_path not root

REH_ON_FAILURE

hardware failure
	on large clusters failure is inevitable
		how do we detect and differentiate hardware failures from build failures?
			how do we restart the part of build failing because of hardware?

text mode, data oriented
	node and node group orientd
		textual data to describe the node

	work on a local group of data text files

	distribution of processing for large data sets

	small utilities (unix style)
		check_digest
		...

remote debuggers
	possibility to run shell builders in a debugging shell
		eg:
		AR  .....
			name
			builder
				shell command 1
				shell command 2

		pbs -interactive_shell IP -name 
		
		when shell commands are to be run, a list of commands is displayed in
		the client's shell window
		
			the client can be on a different machine, thus we need to ssh back

	running the build system (or part of it) in a remote debugger

how to handle no memory in very large builds
	separate the different phases to reduce memory usage
	put nodes in DB
	on demand node loading/unloading

interactive repl, scons .98
	once the graph is created, the repl allows one to build repeatedly
		is the graph hashed at each build?
 
limit parallel build based on resource usage 
	eg do not schedule while load is high

multiple node created by a single build command

	Match(pat1 pat2), BUILD(...) 
		means both nodes are build by BUILD
		but there is not relationship between Macth and BUILD

		in pbs it is TWO builds and we try to  remember that one of the node
			was build and the second one is not run
			problem, it doesn't work in parallel, it doesn't work for multiple libs, ... 

		see the gnu make book ~ p94

	BEWARE of parallel build where process don't share data (pbs1 fails there)
		which means that we need to check if the node was already build via the file system!

		BEWARE 2 even if we use the file system sentinel 
			the processes may be NOT running on the same machine (distribution)
				can only be fixed by a common build database but complicates unnecessarily
				and is much slower and gets slower as we parallelize more

			even on the same  machine, two nodes may check for the presence of the sentinel
				at the exact same time
				this can be fixed by making the nodes depend on the last node thus forcing
				the last node (and the sentinel) to be build first
	

	AR BUILDS(regex1, regex2, ...), BUILD_MULTIPLE...

	regexes are ANDed

	BUILDS runs at depend AND build stage to link them together
		we can "tag" nodes as "build together" and act accordingly

	BUILD_MULTIPLE is a specialized BUILD


REH can be run at multiple stages

REH run order is defined by saying which REH are run before or after
	this lets us check at compile time if the REH sequence is valid

variable dependencies
	in pbs dependencies to variables are handled through a set of commands
		AddVariableDependency, ... and equivalent commands that act on uniq nodes

	REHs can replace them
		global REHs are added in a rule that matches globally 
		node REHs are added in the node rule or a rule that matches a specifi node
			node REH can remove gloabla variable dependencies
			
			config node REH can take a script as argument for complex cases
	
	this unifies the handling and reporting of dependency listing with the handling
	and reporting of digest entries


	in other build systems this is usually done by remembering the command used to build the 
	node, if it changes it has to be rebuild. In pbs  we decided that it didn't have the right
	granularity, what if a file location change, if an argument doesn't change the output, ...
	
	we also want to build nodes with multiple commands and even, maybe with builders defined
	in multiple rules

	this put a burden on the user which can be lightened slightly
		warn if a non dependency config variable is used in a build command
			separate commands to declare variables that are not part of the digest
				this removes the warning
	
	the user may want to depend on the command line and not bother with declaring variable dependencies
		Rule match((), dependencies('*.c')
			build("%CC

multiple rules builder
	a warning is displayed when multiple builders are used
	warning end up in log (all warnings should)

--dd, ddl, ddr
	often followed with --tno
	ca we present --dd as a tee to start with ?

use case
	Rule V all_tests: test1 test2

	# we want the tests to be run in individual directories and in parallel
	# they can't be in the same directory because they use the same input files
	# which must be different for each test

	Rule V ^test*: sub_dir*/test* @dependencies
		cd sub_dir*/test
		run the test
		
	Rule sub_dir*/test*
		mkdir %FILE_TO_BUILD


generalize the concept of object_files
	a sub target creates a list of information to be used at a higher level
		eg: a list of oject files to be linked
			the concept was created to reduce the time spend creating
			intermediary libraries

	command line utility to manipulate the object_files
	%EXTRACT to manipulate the object_files in the build commands
	change name to reflect the generic nature

	another case could be a lib that returns
		a lib
		the  location of header files

		my_stuff: informative_lib



		informative_lib: @c_files
			%cc @c_files
			object_files %FILE_TO_BUILD add LIB %FILE_TO_BUILD
			object_files %FILE_TO_BUILD add INCLUDE_PATH somewhere/ ....
			...
			
easier named dependencies
	by common prefix?
		t: dir/a dir/b another_dir/c d
			command %MATCH(dir/, %DEPENDENCIES) # lol, this is gmake macros

check gmake debugging options
	database dumo
	variable definition file:line
	--warn-undefined-variable
	...


multiple versions of the same node in the graph
	is it really worth it? 


%FILE_TO_BUILD short versions
	a la make $@
	%FTB
	%TARGET

	the build time variables should be in the config too to allow easier debugging
		they are computed per line run in a shell, not at the build start!
			users can define their own!

	in any case they should be visible for debugging
		there's an EVAL_DEBUG switch bit not sure it works on user defined %VARS

pass arguments to prerequisites targets
	a: setup(a, a1, a2, ...)
	b: setup(b, b1, b2, ...)

	setup:

	if setup is a target, it will be run just onece

	it is possible to have multiple targets by using
		%setup or setup%
		but this runs the same setup when we want to run different setups

	we can define a function and call it
		a:
			 setup(a, a1, a2, ...)
		b:
			 setup(b, b1, b2, ...)
		but it doesn't show the prerequisite nature 

matching a rule, run one time
		when all match?
		when a single one matches?

templatized output
	eg: output for when a node is build (-bni) corresponds to a template that the user can modify
		access to basic element/color, and options, are provided

		a bit like the code that does the visualization but elements are computed and made available to user
			may be less efficient

		we can start by using templates internally and moving display code to plugins
			note that node visualization is already a plugin
		
		breakpoints + bni + bnir 'no_match' is a workaround
	
	the templates should use functions that can be used in post processing
	the templates themselves should be run on node
		say during post processing or debugging I want to see the same output as during the build
			fill_template(node, template, args)


output is by default off
	opposite of -q, we need to ask for output
	output is often necessary during creation of the build rules
	output is needed when a node fails to build
		and only for the node that fails and its dependencies
			and we can generate dependency node output on demand
			if we need to see the output of the successful nodes, if not kept),
				we can build those nodes

Microwarp composition
	as the warp cache is composed of MW, multiple configuration can share the nodes
	can we check them in?
		this would give us a history of the dependency graph
			it would be nice if the system history was also under version control
				difficult to force devs to checking code when doing local builds/tests
				the nodes hash is kept in the dependency graph
					we can find which version of the source repo is used
						or if it is a local version


log is not generated
	the log contains node information and build time information (including output from the tools)
		no need to keep tool output for node which build succeed

		better to generate a log on demand from the dependency graph and build time information
			see templatized output

		possibility to generate a single node build environment for debugging

		

REH_BREAKPOINT
	a bit like a pbs1 breakpoint, called back on events
	REH_EVENT?

	possibility to alias the name and arguments easily
		REH_ON_BUILD => REH_EVENT(TYPE => 'BUILD', POST => 1, PROCESS => sub { ...})

REH_USER_TARGET
	"targets" target which queries the registered rules and displays the user targets
	https://marmelab.com/blog/2016/02/29/auto-documented-makefile.html

REH SHELL and ENV
	as ENV is stripped off we need a way to bring it back for applications 
	running under some SHELL

	setting ENV is done by rules which allows us to selectively apply ENV to a 
	node build

	there is a need to control which rules set environment variables (the same need
	we have to control which rules bring which dependencies, also a node
	environment control problem) 
		how to define node environment and what rules are active within it
		how external environment impacts the node environment
			in pbs rules are controlled by running in a specific package
			but that separated parts of the graph not the nodes within that
			part

			still in pbs (without --neo_config) configuration are global even
			if they are hierarchically inherited

		how to visualize the rules changing the environment
		how to control the of the interaction between rules
			how one rule can stop other rules
  
	

Create a clear display framework
	in pbs, redirection is in multiple files to handle multiple options
	it is difficult to change and get an overview of

Move dependencies to __DEPENDENCIES

graph generation, node display filters
	per node
		rules decide which nodes get which filters, this is not part of the pbsfiles (but can be)
		but use the same mechanism, eg: the nodes, even if they already are in the graph (depended) get
		subject to a set of rules that define how, and if, a node will be be displayed

			if part of a subpbs, graph rules work as node subs and save data in the node

		

		the rules also create, one or more, graph nodes where the nodes are inserted,
		this allows the creation of multiple graphs
			do we add a target for the graph generation or create graphs directly?

		
		the graphs can be rendered differently, eg: table, CSV, image, ...
		
		nodes can belong to multiple graphs and be displayed differently in each one


		eg:
		
		PBS_ROOT: graph

		graph: root_node
			create_graph
		
			


target naming
	given the rule:
		A => B, C(A)

		where C, it's name, depends on A's name
			ie '*.lib' => tests, *.module1

		if B also depends on C
			B => C(?).test

		how do we propagate A's name?


	this is because the rules are completely separated, they should be or we would not be able to
	reuse rules

	this has been implemented this way in the passed:
		my @modules = qw( module1 module ) ; 

		# we can also get the modules from a configuration
		AddConfig MODULES =>  'module1 module' ; # keep them as text
		my @modules = split /\s+/, GetConfig 'MODULES' ; # which can be hidden behind some sub


		And then rules added:

		AddRule *.lib => tests, @modules ;
		AddRule tests => map {"$_.test"} @modules ; which can also be hidden behind a sub


		with syntactic sugar where no variable is used:
		PbsUse 'sugar' ;  # define get_modules and get_modules_tests

		AddConfig MODULES =>  'module1 module' ;

		AddRule *.lib => tests, get_modules('MODULES') ;
		AddRule tests => get_module_tests( 'MODULES') ;

		# the above has the asdvantage of hidding all the details and keeping it Config and Rules only

		it is also possible to replace both rules by a meta rule:
		PbsUse 'sugar' ;  # define AddModuleRules

		AddConfig MODULES =>  'module1 module' ;

		AddModuleRules => 'MODULES' ; # looks like a module but is a rule wrapper

		we can even make it a one shot config + rules wrapper
		PbsUse 'sugar' ;  # define AddModuleRules

		AddModuleRules => 'module1 module' ; # adds config, adds rules


 		There are differences between the different solutions
			details are more or less hidden
				but it is still under the users control

			in the sugary solution, the rules and configurations are not created in the pbsfile
			but in the lib file, that's what pbs will report, except if we force it otherwise

			

		* Aternatives *

		* export the TARGET for the rules
			! a very bad idea, rules coould be far down the chain and would have to know about other
			rules


		* create config variables before running rules
		PbsUse 'ModulesConfig' ;

		AddConfig MODULES =>  'module1 module' ;
		AddModulesConfig 'MODULES' # Adds in the config everything a module needs
						# MODULES_PATHS => 'module1/module1 module2/module2'
						# MODULES_TESTS => 'module1/module1.test module2/module2.tests'
						# AddModulesConfig is verbose about the new variable creation

		AddRule *.lib => tests, GetConfigSplit => MODULES_PATHS ;
		AddRule tests => GetConfigSplit => MODULES_TESTS ; 
		

		or 

		PbsUse 'ModuleConfigs' ;

		AddConfig         MODULES =>  'module1 module' ;
		AddModulesConfig 'MODULES' ; 

		AddRule *.lib => tests, GetConfigSplit => MODULES_PATHS ;
		AddRule tests => GetConfigSplit => MODULES_TESTS ; 

		# pros and cons
		pros:
			very simple, no modifications of the API, flexibility for the user
			everything ends up in the configuration which is visible with --nc/--bni
			config locking/override is already in place
			rules are added in the right pbs file

		cons: 
			everything ends up in the configuration!
				the configuration is passed to levels below
					may be a good thing
					may be made LOCAL


Language independent builders
	when possible transform perl builder in external perl builders
		take the code and put it in a bin directory 
		if the builder accesses internals, split the builder in a builder that writes
			the needed information and a builder that is a shell command
		
			this has the disadvantage of starting a new process

	use inline::Language and keep the external language in the pbsfiles
		how easy is it to debug then?

Setting up paths for binaries under the build system control
	if the build system has a set of utilities that are used to build, add the set's path
	to $PATH before running the commands

		this removes the need for the build system users to change $PATH



	

Binary repository layer
	rather than as if a specific file has an md5 on the repository we should
	ask if it has a file with that md5, it doesn't matter where it is

	repositories can send a patch over the list of previously available artefacts to reduce traffic

	the artefacts are needed locally only when a dependent has to be build

	a node that needs to be build does not need to be checked with the repos

	feels close to what warp does except we need a physical file rather than a pseudo node in $inserted_nodes

	if a node needs to be build, it doesn't exist or a depdndency has changed, we can still find it
		using it's dependencies and config as a hash

	

in large project, digest files take a significant time to load (1s :) ) can the size be reduced?
	C includes are repeated many times

Can we get the nd5 of a sub? of a closure?

PrintDebug2
	we need to have a color tag at the beginning of print output to be able to differentiate
	different entries in the same category
		PrintInfo
			what kind of info?
				if we have three categories then it should be made clear
					print output to be able to differentiate
	different entries in the same category
		PrintInfo
			what kind of info?
				if we have three categories then it should be made clear
					aprint output to be able to differentiate
	different entries in the same category
		PrintInfo
			what kind of info?
				if we have three categories then it should be made clear
					■ ■ ■ 
					each ■ gets colored 
					alternatively the catagory name is displayed
--display build result displays result for each builder element

Warp verification message
	display some statistics about the types of nodes in the cache in verification message
	make display single line

		Warp load time: 0.06 s.
		Verifying warp: 3801 nodes ...
		Warp verification time: 0.14 s.
		Warp total time: 0.20 s.
		Warp: Up to date.
		Total time in PBS: 0.23 s.

		Warp: UP TO DATE. 3801 nodes (1400s, 1600g, 801p), load: 0.06 s. verification: 0.14 s
		Total time in PBS: 0.23 s.


wrapper to check test result
	AddRule :Name(test)
		Is("any_command arg %arg", regex)	



bash completion should be done by pbs as some options are loaded at run time
	wrong, the options are loaded after the application is executed so we don't
	know what will happend in the completion script if we do not run the current command line
	which is prohibitive for completion purpose

warp, on pbsfile different => need to rebuild everything
	because warp doesn't have pbsfile-nodes dependency? see mini warp


Add a mode to depend as little as possible, IE no C dependencies cache, no checking sub nodes if top node needs to be rebuild, ...
	generate the dependency graph after building if the information is available

--no_warp creates warp file even if it doesn't use it

how much more time does it take to create a warp 1.5 and 1.8 compared to only warp file 


declare possible top target in pbsfile
	allows us to scan all pbsfiles (what files?! pbsfiles can be called anything) to find targets
		 - can help the user decide what target to build or where a target is build

	we can cache the target names in warp

	target can be  pbsfile tag or a reh
		1/ Target(something)
		2/ Rule :NameMatch(/xxx/), :IsTarget ;

pbs/subpbs return a miniwarp
	there is no reason to give the top part of the graph to subnode 
		linking to existing node?
		better encapsulation of dub level

	the pbs serialized the miniwarp
		calling pbs can ask for a live sub graph

	calling pbs has to link nodes if they exist in the graph
		if same node and config
			otherwise we have conflicting configs

	create check file/build sequence at the same time as the creation of the mini warp
		we can parallelize the check 
		nodes in mini warp build sequence must be tagged if they are terminal
			we don't need to compute it that in the parallelizer if it is already done

	returning a warp file + check/build sequence means that we do not have to keep the live nodes 
	in memory
		NOTE: the build sequence is warp 1.8 first level dependencies, up to the calling pbs
		
		we can create w1.8 first level on the fly

how do we detect circular dependencies if nodes are depended in different process each creating
	a graph that is not circular
	
	we can run a check pass on the dry graph just for that, same as today but much lighter

	we can merge the build sequence and if a node is present twice, we have a cyclic dependency

	use tsort

work out the user help vs the pbsfile help
	=for PBS 


sub options set the parent option
	IE: --ttno needs --tt on the cli or it will have no effect

	writing --ttno will automatically set -tt

	if parent option needs an argument, display error message and stop

display options as a tree when -h is used
	category
		sub category
			top option
				sub options
	options
	├─ help
	│  ├─ -h|help = Displays this help.
	│  ├─ -hs|help_switch=s = Displays help for the given switch.
	│  ├─ -hnd|help_narrow_display = Writes the flag name and its explanation on separate lines.
	│  ├─ -generate_bash_completion_script = Output a bash completion script and exits.
	│  ├─ -pp|pbsfile_pod =
	│  │  Pod declared with "=for PBS =pod_formating title" is extracted when option is set
	│  │
	│  │  =for PBS =head1 Targets\n"
	│  │
	│  │  =item * all
	│  │
	│  │  =item * debug
	│  │
	│  │  =cut
	│  ├─ pod
	│  │  ├─ -d|display_pod_documenation:s = Interactive PBS documentation display and search.
	│  │  ├─ -pbs2pod = Extracts the pod contained in the Pbsfile (except user documentation POD).
	│  │  └─ -raw_pod = -pbsfile_pod or -pbs2pod is dumped in raw pod format.
	│  └─ wizards
	│     ├─ -wh|display_wizard_help = Tell the choosen wizards to show help.
	│     ├─ -w|wizard:s = Starts a wizard.
	│     └─ -wi|display_wizard_info = Shows Informatin about the found wizards.
	└─ colors
	   ├─ -no_colorization = Removes colors from output. Usefull when redirecting to a file.
	   ├─ -c|colorize =
	   │  If Term::AnsiColor is installed on your system, use this switch to
	   │  colorize PBS output.
	   │
	   │  PBS has default colors but colorization is not turned on by default.
	   │
	   │  Colors can be defined through switches (try pbs -h | grep color) or
	   │  by setting you colors in the environement variable 'PBS_FLAGS', ex:
	   │          export PBS_FLAGS='-c -ci green -ci2 blink green -cw yellow'
	   │
	   │  Recognized colors are :
	   │          'bold'
	   │          'dark'
	   │          ...
	   │  Check 'Term::AnsiColor' for more information.
	   └─ element colors
	      ├─ -cw|color_warning=s = Set the warning color.
	      └─ -ce|color_error=s = Set the error color.
	  
keep logs, output, ... for every run of pbs
	so we have something to look at when somethingg builds, or doesn't, when we expected it not to, even if pbs is most probably right

	one often reruns pbs just to get more information with an extra switch
	if the information is not to be displayed, keep its data in a separate file

	we need a system to cleanup periodically the unnecessary data, an option
	--run_data_limit/-rdl (and --no_run_data) which is set by default to, say 5, and that can be
	changed in the configuration.

		--nrd 		don't keep this run's data
		--rdl 0		never keep run data and remove the data found on disk

		if we keep multiple logs, we will need to keep multiple node data for nodes that are rebuild

	node data should be in the node's digest and only tree data kept in log
		we can reconstruct on the fly

		node digest's name should contain the md5 to ease its localization
		with command line tools

		node data should be kept as string to avoid compiling it when loading the digest
			in comments that we can "uncomment"
			in strings that we can ask the digest to return to us

		the node object could itself be the digest, eg: the node is serialized and it can
		be queried for its digest or data

		node data contains the build output

	* support --jobs
		builder creates node's data files

		master creates 
			warp file
			build sequence (linear, although it can be recreated from the graph if we have a list of triggered nodes)
			build event (queuing, starting, ending) which can be used to create a timeline
		
tree display additive fields
	--tt is the bare tree, one can add elements to the tree via switches
	completion and help are available

tree display color
	node name should be made clear
	additive fields are color like command line switch

HTML output
	terminal that is HTML aware to allow DHTL tree dumps

	fast output
		revert to pure text without html
		display html with max depth set
			user can still open data structure dump further
		load data on demand
			trees are just links

		filters
			show only some of the pbs phases
			show specific node
			show specific subsystem


	pbs shell should support HTML

continuous integration
	keep logs of builds and html interface
		builds are not started via interface nor via commit hooks

	check everything is commited
		commit in a CI repo

	builds when local machine is not powerful enough
		
	start remote build on local machine

	check and reuse environement if possible
		

IPFS as synchronized build cache
	a node to be build can request itself from some other build-node cache

	caches can  synchronize as long as they have disk space so if a node is 
	requested, it can be delivered in parallel from multiple build-nodes
Tmux
	like gnu parallel, in a specific session for the pbs run
		output  in a window or in a pane
		do not keep parts of the builds that succeed by default

	controlled by REH

Build distribution
	the build system can clone itself (and all deppendencies) to another computer
	if a node is changed
		if it impacts many other nodes, the node is synchronized and all the 
		build nodes co-operate to the build

		if it impacts little, we can know that from warp 1.8 data, the build is
		made locally and nodes are synchronized after the build

debugging
	when node fails to build, setup a miniature environment with copies
	of the failing node, the necessary dependencies (a warp tree for the nodes that only be
	present but not used), ... 


	use rules and REH
		ca setup an environment specific to the node type

		Rule
			MATCH_ALL
			REH_ON_FAIL => \&setup debug environment

Tmux test environment
	keep only failing tests
	stop after x number of failing tests
	

three pane debugger with preview
	a ranger like display parents (can be multiple parents), this, children
	preview shows rules, config, ...

debugger
	trace: shows how we got to the point where we are

	"next": show the rules that match, this should also show in the graphical browser

	in case of error show all the relevant data, including the results of the command, and the
	possible output, graphical browser shows what other rules would fail

	break point show type of breakpoint, pre-depend, ...

	set/unset debug point easily and quickly
		set and unset based on some code returned value
			ie, as long as number of nodes in graph < 50 continue
			have a rich data set the functions can work with
			data set is per node, module, level, range of levels, ...

	REH_DEBUG_SET_BREAKPOINT

environment variables
	pbs removes all the environment variables
		this helps but when a sub command fails or produces an unexpected output
		(because it did not check properly) this helps the user very little

	pbs removes all the environment variables if option is set

	pbs replaces all the environment variables with "SET BY PBS" to make it clear

	find a system to "break" the commands that use environment variables

	removal of environment variables should be per node/level
		node/level can set variables

	warnings if environment variables are set or not set

 
debugger watches
	extracted by code snippets from the available data set, they don't have 
	to represent a variable, they can represent whatever, a bit like an element in
	a bash prompt
		
	doesn't have to be perl code or an element of the graph
		ex. a service is available on a remote server
		ex. cpu load, free disk space, ...
	
	good if implemented as debugging REH in a rule

warp does not need normal digest
	when running in warp mode and something need to be rebuild
		this helps but when a sub command fails or produces an unexpected output
		(because it did not check properly) this helps the user very little


warp does not need normal digest
	when running in warp mode and something need to be rebuild
	the no-warp mode building is run; it generates digest files
	and c dependency files. those files are not used by the warp 
	mode. the warp mode should apply to normal build too to avoid
	generating the digests. normal mode data normally ready from 
	digest files can be obtained from the warp file.
	Note that this is not a big problem as digest file generation 
	is trivial and that most of the time is spend computing md5
	which are also needed for the warp file. It does have an 
	"advantage" if the warp data is deleted or a different warp
	mode is selected, the normal mode is fast.

warp 1.8
	can we reduce the format of the warp files, textual amount as
	well as fields in the warp files

	there is ROOT directory in the warp 1.8 data that should not be there!

	single node warp file take the longest to process, is there a way to 
	optimize the process by collating related nodes in the same file?

	node regeneration should not need to set BUILD_NAME
	try to make node regeneration more light weight
		if some rarely used plugins need more data, it is better
		to do more re-generation in the plugins

parallel depend
	done on the subpbs level
	can generate a mini warp
		who merges back
		merging should be distributed too

	merging is mini warp is no necessity, we can follow the chain
	and read multiple mini warp files

	mini warp must merge very quickly
		two data sets
			one with complete data
			one lightweight to merge quickly
				see warp 1.8 node regeneration
			keep a md5 between them
		mini warp have warp 1.8 format
		merging 
			merge checks link errors
			adding node md5 to list
			single node warp files need parent dependent added
			 
	a set of hierarchical mini warp can be merged at multiple levels
		only merge the quick data and keep reference to the complete data

	

warp nodes can be formatted as the reconstructed nodes to speed up loading
	
nodes that are the deepest trigger the most files
	verify them first as they re-vivify the most nodes and we cache those

nodes that trigger often are more likely to trigger again

sample data digest generation for big files
	md5 on a 5 Mb file takes longer than on a 4 kb file
	compute md5 on a few well chosen sections  rather than the whole file

option to generate digests for source files only
	MUCH less secure but much faster

	we should actually CHECK the digests for source only, we can still generate digests
	for generated files, that would give the possibility to run with checking or do an
	external check if needed.  

check optimization in Ag
	https://github.com/ggreer/the_silver_searcher

node name completion can be done with the help of nodes listed in the warp file
	we still need to know which warp file

log all builds
	every command line that builds should be save in a log

keep statistics about what is the most often build
	gives us an idea about what has most impact on the system to build and
	what part of the build system is most often used

	let a user specific post pbs handle that (we can give an example)

time cursor (see Log)

	a history of the build is kept so that a cursor can be placed at a user chosen
	time when she can look at what the graph looks like, what will happen next, what rules
	inserted a node, ...

	keep a history of the graph generation, a history of the  build makes no sense as we can build in parallel
	and the order changes. It makes more sense to get the build for a specific node and select
	its children or parents to look at their build.

C depender triggering
	if dependency file does not exist, trigger and post generate the file instead for an immediate generation 
	this should be the default but an --option may be added for people that want an analysis not a build

C depender generates digests in mini warp format (because it is exactly that) and the 
	linking of the mini warp nodes for the C depender is no different from other
	mini warp

mini warp does not contain node data, just enough for regeneration

document why, when using warp, the C dependency files are neither used nor verified
	=> they are merged in the warp graph and warp doesn't care about them, but the 
	dependencies are still verified

Output from Hashes must be sorted as Perl randomized the key order

shell build nodes can receive tools, commands and the code to work on from master
	reduce the need for a shared file system

shell build nodes can share code with other nodes
	when the master starts a build, it also starts the sharing of the code, while the
	master works to find what is going to be build, the build nodes share the code.

shel build nodes keep a repository of shared code

shell build nodes can update their repository
 
build system has startup rules that install tools and checkout code
	although checking out the code with the build system (e.g. in the same commit) makes more sense

parallel depend
	OK to have multiple instances (in each process doing the depend) if the common nodes have the
	same digest, including the configuration. if the digests are different we need a mechanism to
	let the user decide what happens next.

documentation is part of every release

REH instead for named dependencies or as a complement
		rule
			named => (c-dependencies => a, b, c), 
			c-dependencies => (a, b, c), 

	this also allows the REH to 'type' the dependencies and itself


REH user defined type
	allow visualization of the REH in a user specified manner
	e.g. also when the dependencies are listed, the dependencies from a specific REH could be in red color


display dependency list 

	Node [V] './objects':
		Inserted at ./pbsfile [PBS]:__ROOT.
		dependencies:
			./1.objects
		rebuild because of:
			./1.objects (Subdependency or self)
		matching rule: #2[B] 'objects'
			=> ./1.objects
	  

	displays the REH that matched and uses the color from the REH type


rebuild because of: ./1.objects (Subdependency or self)
	be precise, dependency or self, or both, and which dependencies


always save the subpbs configuration
	possible to query the configuration for a specific node
	history of configuration is also displayed

build subset
	a better way than node@root and save/load configuration

			  .---------------------.
			  | .---.               |
	       .------------| A |--------.      |
	       |          | '---'        |      |   wait for B to build 
	       v          |              v      |   correctly to build  
	 .-----------.    |            .---.    |
	 | subsystem |    |            | C |    |
	 '-----------'    |            '---'    |
			  |              |      |
			  '--------------|------'
					 |
			  .--------------|------------.
			  |              |            |
			  |              v            |
			  |            .---.          |
			  |        .---| D |---.      |
			  |        |   '---'   |      |  Only build this till OK 
			  |        v           v      |
			  |      .---.       .---.    |
			  |      | E |       | F |    |
			  |      '---'       '---'    |
			  |                           |
			  '---------------------------'


	When building A, if an error occurs, find which subsystem the error occurs in and
	give all the necessary information to build that subset from the command line, see
	setup subset build for a manual setup

	building a subset is like building a specific subpbs with a saved configuration, there is no
	need to build the parents of the subset

	it is possible to limit the build to the first level of the subset even if configuration
	or graph changes are made at the subset level

	when building a subset, make it clear to the user that it is only a subset build so that 
	it is clear that a top build needs to be made, return an error code and the warnings


setup a subset build
	the user can ask the build system to generate the command line to build a specific file or
	subsystem.

	say that a subsystem build a library
		the user can ask for the closes pbsfile to build that library
		the build system
			creates a (universal based) command line
			creates a configuration file copy for the subsystem
	
	the creation is done 
		based on data in the warp file
			if no pbs related files trigger (libs, pbs, pbsfiles) 

		by running the system in depend only mode to generate the necessary data

user defined node type
	subsystem, debug, UI, ..

	the user can give attributes to nodes

	those attributes can be used to 
		display nodes differently
		display the attributes in a --tt
		match rules
		search for the nodes 

	queries can use node type

universal root for build
	it is possible to build from any location
		a cwd not the root of the system to build
			finding the files under . (virtual root)

		a cwd root of the system to build
			normal build

		a cwd under the root of the system to build
			subset build			

subpbs rule should not care about the path of node
	rules now have to match */regex as the start node has a path
	but the path doesn't make sense in the local environment of the subpbs, the nodes
	should get their path based on their parent's path. this would simplify the run of 
	pbs -load_config as we can give the local node name instead.

	some node have path information ./lib/xxx gets under the pbs root under ./lib (late depend)
	some nodes may be related to the root (/)

	before starting a subpbs, the node to depend could get an alias without the path
		

 	this would allow moving whole subsets of the system and still be able to use the warp file
	as is since there is no path recorded anymore




All display code is a plugin, allowing the user to change how the display looks like
	without changing the core code

	use a templating system or subs

	can the display code be triggered by injected rules?

H files are dependencies of the object file
	if two object files use the same C file, the dependencies may be different because of the
	object file configuration. Linking the C file with a configuration that is wrong, when
	depending the second object file, is an error.



virtual nodes without dependencies ALWAYS trigger
	a virtual node without dependencies always trigger
	there is no way we can know if a virtual node without dependencies "succeeded"
	we can know that it failed because the commands it runs fail but if they all succeed
	we don't have a node we can make a md5 on so we must re build the virtual node each  time

	need example of virtual node without dependencies

	this is so frustrating. All those years talking about virtual node digests in complex setup (subpbs linked nodes, warp graphs, ...) when we should have kept it simple and see what the problem is with VIRTUAL, it's simple, we didn't know where to write its digest (that's so hard to decide) and that it didn't trigger. So rather than do the right thing we introduced FORCED, Do I hate make and all the make files that other morons like me wrote that forced things.
	FORCED must die and than everything will be fine because we must do the right thing.
	a virtual node, whatever the context, as it has no digest will ONLY trigger if one of its dependencies also triggers. This means that you can add a virtual node rule and it will never trigger, you can modify a virtual node rule and it will never trigger, virtual nodes don't trigger, they have no control over their triggering, it's the dependencies that trigger them, and the dependencies may trigger for various reasons none of them related to the virtual node rule.
	You can even write a virtual node rule that has dependencies on source nodes, EG document the source nodes, and it will never trigger.
	VIRTUAL nodes don't trigger, so rather than make them trigger properly we FORCED them to trigger, I hate make.
	if virtual nodes have triggers, they will trigger on a dependency that trigger, as before, on source files, and on the pbsfile that where the virtual node rule is defined. 
	Why did we need a virtual node digest? For everything!
	Where to put the virtual node digest is obvious, in the out directory, as everything else. What if they match a directory name, no problem, the directory and the digest don't have the same name.
	It's easy to be smart after the fact but it is pretty obvious that if you don't handle nodes the same way, there are going to be differences, why didn't we handle VIRTUAL  as a normal node? why didn't we handle VIRTUAL as we defined in the documentation " a node that has no physical representation in the file system", a 2 lines check, but instead make a monster out of it.
	Next PBS will be better, VIRTUAL will have a digest. 

	what if the dependency is a virtual node itself, what do we write in the digest?
		the virtual dependency digest!

2 dependents share dependencies. I don't think it will be in the form t1, t2 -> d1, ....
	but rather t1 -> d1, ... ; t2 -> get_deps(t1).
	that allows a tx -> get_deps(t1, t2, whatnot) picking up dependencies other nodes have without having to change t1, t2 pbsfiles. probably late_depend will work on them too so t2 can be dependent on t1 dependencies even if t1 is not in the graph yet


NAME => 'any generated data is exportable to another format',

NAME => 'defining_subpbs_with_a_sub',
LONG_DESCRIPTION => <<'EOD',
	give a sub that checks the type of the node to generate and create
	a virtual subpbs if non exists in the sub directory (for known types)
 
	show a message if a subpbs is generated
	use pbsfile if it exists
	if given --generate_missing_pbsfile, ask if a real
		pbsfile should be generated. practical if the virtual
		generated pbsfile doesn't fit (ex directories or files to ignore, more configurations)


REH to match configurations
	AddRule
		MATCH => $regex
		MATCH_CONFIG => ...

	this is related to REH_IF

REH to match steps in the pbs run
	this is already done in the registration of the REH which doesn't look like rules
		and REH can not be registered to multiple steps 

	it also need to match a node and pbssteps are not nodes (yet)

	a good example is -gtg, we generate a graph at specific moment
		gtg is implemented as a plugin in pbs1, thus pbs calls it

All output is done via a registrable sub to allow PBS2 to be embedded in another
	system, EG: neo

	display system can be setup to display information in specific cases only
		node
		pbsfile
		pbs run
		...


REH  NAMED_CONFIG AND NAMED_CONFIG_GROUP
	no problem adding the REH but how does one get to the config
	by name later?

command line options and wizards
	can a rule match a command line option as if it was a target
		MATCH_CLI => /dd/
		
		how do we define our command line options
			some global place plus in the plugins
	
	wizard is just a target 
		how do we add all the targets?
			just load a library with the rules

plugins
	some are just targets, ie generate_graph, which need to be run at specific
	pbs time

	Rule 'generate_graph'
		MATCH_CLI => /-gtg/
		GENERATE_GRAPH_REH => ....  # knows when to be run


	bleah! make the target a virtual node!
		Rule 'generate_graph'
			MATCH => '__pbs_graph'
			AND_MATCH_CLI => /-gtg/
			BUILD => ...
		
plugins
	some are called during the pbs run, eg they register subs that pbs calls
		EvaluateShellCommand
		CheckNodeName
		
		The problem being is that they are not node specific
		we can eliminate global plugins and query the nodes for the 
		plugins to run. A global plugin simply matches all nodes

	AddRule
		MATCH => $regex
		EVALUATE_SHELL_COMMAND_specific => 1

	We can't audit plugins as we did with pbs1, finding the plugins becomes more
	difficult as they can be inlined 

prefix all the REH that have an impact on internal implementation
	DISPLAY_DEPENDENCIES => 1
	PBS_DISPLAY_DEPENDENCIES => 1

	except if it is the DISPLAY_DEPENDENCIES REH itself that does the displaying

generalize the C depender
	- what configuration variables must be in the configuration should not be in the code but arguments
	- the same code can be used for other types of files
		- creators use an equivalent mechanism, could be merged

	AddRuleTo 'BuiltIn', [POST_DEPEND], 'C_dependencies', \&C_SourceDepender, \&C_Builder ;

	Rule 
		NAME =>  'C_Depender'
		REGEX => '*.cpp'
		C_DEPENDER => { ........ }

	Rule 
		NAME =>  'C_config'
		REGEX => '*.cpp'
		CONFIG => { ........ }

		C_DEPENDER_VARIABLES => named list




-bni ,which show a node header, makes it clear which node is being build, there is no such system
	for -dd, the display is very compact

NODE_LINKING is a PBS step with potentially matching REH
	what configuration to check
	allowed or not

configuration dependency for node
	knowing what configuration is used to build a node makes it possible to check
	if a linked node uses the same configuration as the current pbsconfig  build node
		maybe the linked node, with a different configuration and dependencies is OK!
			need a mechanism to allow that.

	extract all the configuration variables from the build commands
	force declarative style for linked node checking
		note that this can still check the build commands

	addrule
		name xxx
		regex '*all*'
		NODE_LINKING_ENABLE => 0 # current pbsfile run must provide one

		NODE_LINKING_ENABLE => 1
		NODE_LINKING_CHECK_VARIABLES => ['name', 'name2', ..]
		NODE_LINKING_CHECK_VARIABLES => {'name' => \&checker, ..}
		NODE_LINKING_CHECK_COMMAND_LINE => 1

		
	knowing which configuration is used also allows to not rebuild some nodes declared in
		the same pbsfile because of a configuration change that has no impact
		on the node

if a rule uses sub rules, EG lib uses object rules, when the rules for obj
	are used, the user can easily trace it back to the lib rule and to where it was used
	
	IE. inserted_at is not enough as all libs are inserted at the same place, the lib rule

move statistic display to plugin (or plugins)
	add a plugin display point in pbs

console
	possibility to query between steps

	starts build or loads a graph dump

	loads warp and checks
		query about warp and 	
	
	depend
		can stop
		can add breakpoints
		can query about graph or node
		can generate graphs

	check

	build
		
option so warp information display is mutted

ExcludeFromDigest may not make sense
	use to say that a node is a source node, better call it that
	why not give a digest to a source node
		win time generating and checking it
		forces the node to be treated differently


EvaluateShellCommand REH
	pre-build REH

	does not evaluate the shell command but createa node specific configuration to match %Whaetever
	
	this allows us to define the REH in a user lib and it can be reused

	a better name is REH_EVAL_CONFIG for a generic sub declaration 

REH can be argument less
	they are more like sub calls then

	this allows a more declarative style for the REH tha do not need arguments
		Rule
			Name => 'declarative rule'
			DECLARATIVE_REH,
			...
			;
		
Pbs creators generate mini build systems
	mechanism used to generate and check the creator digest
	pbs command line switches (-bni, -conf, ...) are also used
	
Mini warp are mini build systems
	getting into a sub directory the user shouldn't hav eto do more than "pbs" on the command line
	to build that sub system again, with the top config!

Use file type "matchers"
	rather than use regex use a file type (which still can be implemented as a regex for simple cases)
	the "matcher" can match depending on a configuration (eg the current OS, tool chain, ...)
	
multiple GetBuildName subs defined in the code

example in Pbsfile/Creator do not work

Merge STE PBS

Create template/example for STE like data driven pbsfile (neo)

option to filter out/in nodes when displaying tree
	generate list of files that are displayed and not displayed, their amount, ...
	
	if options are pbsconfig variables, the REH an use them

possibility to generate multiple trees with different roots
	to display only the relevant nodes
	can still filter nodes in and out
	
generate DHTML tree


Multiple node builders
	builders run in the order of rule matching
		option to display the builders and their order
		
	builders declare if they are singletons, an error is generated if builders exists before or are added after
	
	
build phases
	a node with multiple builders has its builders run in a specific order
	a builder declares which phase it belongs to
	a rule can declare the phases a builder can belong to 
		check is done for multiple rules declaring different orders
		check is done to verify that each phase has a builder
			should it fail?
	
	
Mechanism, at a higher level, to declare in which order REH are run
	similar to build phases
	
	
Builders chaining
	get all previous builders results
	get information pointer to the previous builder
	get the last builder build buffer
		can request the build buffers of all the previous builders
		
	builder can make variables available after build
		eg: tell the rest of the build chain what files are created, ...
		

Build buffers 
	switch to keep build buffers
		filter which ones are kept
			faild ones always kept

failed build buffer
	always keep the buffer
		garbage collect the filters after successful build
			rule matching all the files
			do we want to garbage collect at the global level or after each node, after each subpbs, ...
				object_files can collect the names of the buffers to garbage collect

			do we keep the build buffers in case of multiple failures?
		
		
	keep information about the builder
	create a mini build system
		configuration
		possibility to run just that builder/ builders for a node
		
	
possibility to display the tree in warp mode
	global tree, warp tree
	filters should still apply
	
File not found for MD5 computation
	most probably a node that is not declared as virtual (although we have defined somewhere that virtual nodes should have a digest)
	builder failed
	=> display as much information as possible
		mini build system for just that error
		
generation of mini build systems
	in big systems, the developer does not want to re-run the build system just to find out what the problem is but wants a small build to debug the problem
	
Post pbs 
	can be implemented with a rule that matches the pbs run virtual node
		better than a rule that matches a target in case there are multiple runs of the pbsfile with different targets
		REH_POST_PBS => \&post_pbs_sub
		REH_POST_PBS => GetPBSConfig('POST_PBS_FILE')
		
	C depender should use the flags defined by the user rather than force flags on the user
	
	
override after **------ -dpl
	don't know what I meant
	if subpbs run (two extra phases here prepare to run subpbs and run subpbs, plus subpbs ran) matches a rule, it can display something else
	
	
Make clear how build command line parameters (%FILE_TO_BUILD, ...) are defined and handles
	also defined in plugins or rules
	
	
plugin to generate report on configuration usage
	a post pbs part of the examples in the distribution 
	
	configuration in, defined, used, delta with parent, delta with children (available after the subpbs runs)
	knowing how many configuration variables (along with rules, targets) can be used to automatically find what pbsfiles should be split
	
give example of C code generation
	or anything that needs to be depended
	explanation and example of IMMEDIATE_BUILD or creators
	
Creator, IMMEDIATE_BUILD, VIRTUAL, FORCED
	all can be REH!
	
rethink/document IMMEDIATE_BUILD FORCED, LOCAL_RULES and other exceptional mechanisms

REH_TRIGGER rather than pbs functionality
	starts a subpbs before the REH_LINK_TO
	
C_FILE_SYNCH
	it's a post build REH
	Make it visible
	
ON_ERROR REH
	for un interesting parts of the build just ignore the failures
		need to make it very clear
		
		
Express complicated dependencies
	references to dependency lists, but still locally defined
	depender subs are all mighty already
	simple interface to the dependers, better wizard
	
user defined wizards
	possibility to have multiple wizard directories 
		no need to put user wizards where pbs wizards are

replace plugins with rules
	CheckNodeName, rule matching all nodes
	CreateDump, rule matching top node, what if the build fails
	CreateLog, rule matching top node, what if the build fails
	EvaluateShellCommand, replace with EVAL_CONFIG, there is no reason to evaluate the shell command just
		to handle %SOMETHING. Adding SOMETHING to the config as the side effect to be self documenting
			- SOMETHING stays visible rather than transient during EvaluateShellCommand
			- the rule doing the evaluation is logged
	ExpandObjects, just a specific EvaluateShellCommand that should only define a configuration variable
		can be used directly as a REH (pre build) in specific rule:
			Rule
				Name => 'some_lib'.
				Match ...
				...
				ExpandObject,
				Build => "linker %DEPENDENCY_LIST_OBJECTS_EXPANDED"
				EndRule

			or as a REH global in the pbs run
			
			use 'Lib/ExpandObjects' ;
			# which contains
				Rule
					Name => 'expand object'.
					MatchAll,
					ExpandObject,
					EndRule

			or use the top rule injection mechanism at the top and have the rule available everywhere
			
	FileWatchClient, REH CHECK
		advantage, 
			nodes checked individually, maybe some optimization can come from it
				IE: node does not have to check itself if dependency has triggered already

		disadvantage:
			
	GraphGeneration, rule matching top, or any node if sub graph is wanted
	PackageVisualisation, rule matching all nodes
	PostPbs, rule matching top node, what if build fails?, can we have nodes representing PBS steps?
	SimplifyRule, a simplified, named, REH
	TreeVisualisation, rule matching top or any node
	Visualisation (-dc and other node visualisation switches), rule matching any node


	making the plugin match nodes means that we can run some functionality at any level we want
		IE: sub graph, sub tree, ....

		timing REH
			today we get timing for the whole build or a pbsfile run, we could get information
			per node or sub system

		most display options, especially if we have pbs, pbs step, config, ... nodes


plugins cli switches handling
	today:
		so they are called, ie: --generate_tree_graph
		so they get arguments, ie: --tree_graph_file xyz.png
		so they get options , ie: -- tree_graph_no_configuration
	
	call plugins/plugin-directory like switches?

	=> scan directory for switches?
		call a registration sub, let it register itself, return registration data
			cache the registration if libs are unchanged
				could be useful to cache the rules
					cache them in memory if server is used
	
	=> solution
		remove possibility to set path in prf, instead have a pbs_set.prf, this removes the need
		to run pbs.prf.
		
		pbs can scan the cli itself, as it already does, for cli path options

		
	defining "plugin" switches
		plugins are replaced by pbsfiles, libs really), so any pbsfile can ad switches
			the switches are scanned at start time, the pbsfile is loaded and registration
			sub is called in the pbsfile, the registration sub is replaced by a NOP sub, or
			a verification sub to catch non scan-registrated pbsfile which call the sub.

		the pbsfiles that define switches need to be declared in pbs_set.prf
			pbs can scan a single file, a directory, a directory with sub directories, use
			regex to select or deselect specific files when scanning a directory

		the options can take arguments, be called multiple times (array), ...

		option definition contains a help text

	user can define it's own switches via sub in a pbsfile (lib) where options are defined
		define aliases for pbs switches, eg: for switches that do not have short versions or simpler names
		switch grouping, eg: --debug for "-dd -dur -post_pbs ..."
		alias for -D, eg: --user_defined 1 => goes into the config as -D special=1 would do
			a nicer looking integration of the user's process

	we can verify which user defined switch that was put on the command line was not used during the run
		and warn user!

	option can add target
		also good for pbs so we can make some pbs options rules
			-h, -v, -gtg, -tt, ...

	user options can be cached so the pbsfiles defining them do not need to be reloaded
		"generating user options cache ..." # no cache
			--display_option_cach_generation
				shows what files are scanned and what options are added

			--no_user_option_override makes the optionsimmutable 

		"" # silent if cache is valid
		"regenerating user option cache", same options as above apply
		
help is a target
	option -h still exists but it adds a target to the top pbs

	let -h display user help rather than pbs help, user decides by renaming rules if they want

	possibility to tailor the pbs help by removing or reordering entries which is not easy when the
	help is part of PBS distribution.


wizards are rules and started with targets
	wizards target and normal target can be mixed, which is surprising and maybe should be warned for

	wizards are still defined in the wizard directory for pbs but user wizards could be anywhere as long
		as the rules are included in the pbsfile

	option -w can add the targets

keep a list of all the files loaded
	libs, pbs modules, prf, plugins, ...

	keep in warp file (today we have prf data entries but it's not set properly!)


warp file is a pbs run file
	it contains information about a build
	some of the information is warp speedup information
	warp 0 has a warp file


if a hierarchical display is requested (IE pbs config for all pbs runs, config, ...)
	display a delta between the level
	if no delta, display changed values in different colors
	if config is local, display it in special color

	use two color scheme for levels
		eg: level % 2 in green, level !% 2 in bright green or cyan ...
		it's easy to miss the level change in large data amounts

256 color mode if the terminal supports it
	not 256 color but different colors for different elements, user defines colors sequences they want
		we stop using Terminal::AnsiColor
			or recognize "red on blue" otherwise use the user code

Immutable rule set
	pbsfiles that always generate the same rules 
		no if-else or REH_IF used to decide which rules are added
		they can use configuration variables in the builders

	can be cached in memory
		pbsuse should be faster
		in server memory in between pbs tool runs

	may be serialized in a more efficient format than the pbsfile

	cache is invalidated when pbsfile is changed (clearly!)




