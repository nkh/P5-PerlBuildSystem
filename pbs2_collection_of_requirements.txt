keep logs, output, ... for every run of pbs
	one often reruns pbs just to get more information with an extra switch
	if the information is not to be displayed, keep it in a separate file

	we need a system to cleanup periodically the unnecessary data, an option
	--run_data_limit/-rdl (and --no_run_data) which is set by default to, say 5, and that can be
	changed in the configuration.

	--nrd 		don't keep this run's data
	--rdl 0		never keep run data and remove the data found on disk


	node data should be in the node's digest and only tree data kept in log
		we can reconstruct on the fly

		node digest's name should contain the md5 to ease its localization
		with command line tools


tree display additive fields
	--tt is the bare tree, one can add elements to the tree via switches
	switches are automatically generated from tree contents
	completion, with help, is available

tree display color
	node name should be made clear
	additive fields switches could have a related _color sibling switch


HTML output
	terminal that is HTML aware to allow DHTL tree dumps

	fast output
		revert to pure text without html
		display html with max depth set
			user can still open data structure dump further
		load data on demand
			trees are just links

		filters
			show only some of the pbs phases
			show specific node
			show specific subsystem


	pbs shell should support HTML


continuous integration
	keep logs of builds and html interface
		builds are not started via interface nor via commit hooks

	check everything is commited
		commit in a CI repo

	builds when local machine is not powerful enough
		
	start remote build on local machine

	check and reuse environement if possible
		


IPFS as synchronized build cache
	a node to be build can request itself from some other build-node cache

	caches can  synchronize as long as they have disk space so if a node is 
	requested, it can be delivered in parallel from multiple build-nodes
Tmux
	like gnu parallel, in a specific session for the pbs run
		output  in a window or in a pane
		do not keep parts of the builds that succeed by default

	controlled by REH

Build distribution
	the build system can clone itself (and all deppendencies) to another computer
	if a node is changed
		if it impacts many other nodes, the node is synchronized and all the 
		build nodes co-operate to the build

		if it impacts little, we can know that from warp 1.8 data, the build is
		made locally and nodes are synchronized after the build

debugging
	when node fails to build, setup a miniature environment with copies
	of the failing node, the necessary dependencies (a warp tree for the nodes that only be
	present but not used), ... 


	use rules and REH
		ca setup an environment specific to the node type

		Rule
			MATCH
			REH_ON_FAIL => \&setup debug environemtn

Tmux test environment
	keep only failing tests
	stop after x number of failing tests
	


debugger
	trace: shows how we got to the point where we are

	"next": show the rules that match, this should also show in the graphical browser

	in case of error show all the relevant data, including the results of the command, and the
	possible output, graphical browser shows what other rules would fail

	break point show type of breakpoint, pre-depend, ...

	set/unset debug point easily and quickly
		set and unset based on some code returned value
			ie, as long as number of nodes in graph < 50 continue
			have a rich data set the functions can work with
			data set is per module, level, range of levels, ...

environment variables
	pbs removes all the environment variables
		this helps but when a sub command fails or produces an unexpected output
		(because it did not check properly) this helps the user very little

	pbs removes all the environment variables if option is set

	pbs replaces all the environment variables with "SET BY PBS" to make it clear

	find a system to "break" the commands that use environment variables

	removal of environment variables should be per node/level
		node/level can set variables

	warnings if environment variables are set or not set

 
	
debugger watches
	extracted by code snippets from the available data set, they don't have 
	to represent a variable, they can represent whatever, a bit like an element in
	a bash prompt
		
	doesn't have to be perl code or an element of the graph
		ex. a service is available on a remote server
		ex. cpu load, free disk space, ...
	

warp does not need normal digest
	when running in warp mode and something need to be rebuild
		this helps but when a sub command fails or produces an unexpected output
		(because it did not check properly) this helps the user very little

	pbs removes all the environment variables if option is set

	pbs replaces all the environment variables with "SET BY PBS" to make it clear

	find a system to "break" the commands that use environment variables

	removal of environment variables should be per node/level
		node/level can set variables

	warnings if environment variables are set or not set

 
	
debugger watches
	extracted by code snippets from the available data set, they don't have 
	to represent a variable, they can represent whatever, a bit like an element in
	a bash prompt
		
	doesn't have to be perl code or an element of the graph
		ex. a service is available on a remote server
		ex. cpu load, free disk space, ...
	

warp does not need normal digest
	when running in warp mode and something need to be rebuild
	the no-warp mode building is run; it generates digest files
	and c dependency files. those files are not used by the warp 
	mode. the warp mode should apply to normal build too to avoid
	generating the digests. normal mode data normally ready from 
	digest files can be obtained from the warp file.
	Note that this is not a big problem as digest file generation 
	is trivial and that most of the time is spend computing md5
	which are also needed for the warp file. It does have an 
	"advantage" if the warp data is deleted or a different warp
	mode is selected, the normal mode is fast.

warp 1.8
	can we reduce the format of the warp files, textual amount as
	well as fields in the warp files

	there is ROOT directory in the warp 1.8 data that should not be there!

	single node warp file take the longest to process, is there a way to 
	optimize the process by collating related nodes in the same file?

	node regeneration should not need to set BUILD_NAME
	try to make node regeneration more light weight
		if some rarely used plugins need more data, it is better
		to do more re-generation in the plugins

parallel depend
	done on the subpbs level
	can generate a mini warp
		who merges back
		merging should be distributed too

	merging is mini warp is no necessity, we can follow the chain
	and read multiple mini warp files

	mini warp must merge very quickly
		two data sets
			one with complete data
			one lightweight to merge quickly
				see warp 1.8 node regeneration
			keep a md5 between them
		mini warp have warp 1.8 format
		merging 
			merge checks link errors
			adding node md5 to list
			single node warp files need parent dependent added
			 
	a set of hierarchical mini warp can be merged at multiple levels
		only merge the quick data and keep reference to the complete data

	

warp nodes can be formatted as the reconstructed nodes to speed up loading
	
nodes that are the deepest trigger the most files
	verify them first as they re-vivify the most nodes and we cache those

nodes that trigger often are more likely to trigger again

sample data digest generation for big files
	md5 on a 5 Mb file takes longer than on a 4 kb file
	compute md5 on a few well chosen sections  rather than the whole file

option to generate digests for source files only
	less secure but much faster

check optimization in Ag
	https://github.com/ggreer/the_silver_searcher

node name completion can be done with the help of nodes listed in the warp file
	we still need to know which warp file

log all builds
	every command line that builds should be save in a log

keep statistics about what is the most often build
	gives us an idea about what has most impact on the system to build and
	what part of the build system is most often used

time cursor
	a history of the build is kept so that a cursor can be placed at a user chosen
	time when she can look at what the graph looks like, what will happen next, what rules
	inserted a node, ...

C depender generates digests in mini warp format (because it is exactly that) and the 
	linking of the mini warp nodes for the C depender is no different from other
	mini warp

document why, when using warp, the C dependency files are neither used nor verified
	=> they are merged in the warp graph and warp doesn't care about them, but the 
	dependencies are still verified

Output from Hashes must be sorted as Perl randomized the key order

shell build nodes can receive tools, commands and the code to work on from master
	reduce the need for a shared file system

shell build nodes can share code with other nodes
	when the master starts a build, it also starts the sharing of the code, while the
	master works to find what is going to be build, the build nodes share the code.

shel build nodes keep a repository of shared code

shell build nodes can update their repository
 
build system has startup rules that install tools and checkout code
	although checking out the code with the build system (e.g. in the same commit) makes more sense

parallel depend
	OK to have multiple instances (in each process doing the depend) if the common nodes have the
	same digest, including the configuration. if the digests are different we need a mechanism to
	let the user decide what happens next.

documentation is part of every release

REH instead for named dependencies or as a complement
		rule
			named => (c-dependencies => a, b, c), 
			c-dependencies => (a, b, c), 

	this also allows the REH to 'type' the dependencies and itself


REH user defined type
	allow visualization of the REH in a user specified manner
	e.g. also when the dependencies are listed, the dependencies from a specific REH could be in red color


display dependency list 

	Node [V] './objects':
		Inserted at ./pbsfile [PBS]:__ROOT.
		dependencies:
			./1.objects
		rebuild because of:
			./1.objects (Subdependency or self)
		matching rule: #2[B] 'objects'
			=> ./1.objects
	  

	displays the REH that matched and uses the color from the REH type


rebuild because of: ./1.objects (Subdependency or self)
	be precise, dependency or self, or both, and which dependencies


always save the subpbs configuration
	possible to query the configuration for a specific node
	history of configuration is also displayed

build subset
	a better way than node@root and save/load configuration

			  .---------------------.
			  | .---.               |
	       .------------| A |--------.      |
	       |          | '---'        |      |   wait for B to build 
	       v          |              v      |   correctly to build  
	 .-----------.    |            .---.    |
	 | subsystem |    |            | C |    |
	 '-----------'    |            '---'    |
			  |              |      |
			  '--------------|------'
					 |
			  .--------------|------------.
			  |              |            |
			  |              v            |
			  |            .---.          |
			  |        .---| D |---.      |
			  |        |   '---'   |      |  Only build this till OK 
			  |        v           v      |
			  |      .---.       .---.    |
			  |      | E |       | F |    |
			  |      '---'       '---'    |
			  |                           |
			  '---------------------------'


	When building A, if an error occurs, find which subsystem the error occurs in and
	give all the necessary information to build that subset from the command line, see
	setup subset build for a manual setup

	building a subset is like building a specific subpbs with a saved configuration, there is no
	need to build the parents of the subset

	it is possible to limit the build to the first level of the subset even if configuration
	or graph changes are made at the subset level

	when building a subset, make it clear to the user that it is only a subset build so that 
	it is clear that a top build needs to be made, return an error code and the warnings


setup a subset build
	the user can ask the build system to generate the command line to build a specific file or
	subsystem.

	say that a subsystem build a library
		the user can ask for the closes pbsfile to build that library
		the build system
			creates a (universal based) command line
			creates a configuration file copy for the subsystem
	
	the creation is done 
		based on data in the warp file
			if no pbs related files trigger (libs, pbs, pbsfiles) 

		by running the system in depend only mode to generate the necessary data

user defined node type
	subsystem, debug, UI, ..

	the user can give attributes to nodes

	those attributes can be used to 
		display nodes differently
		display the attributes in a --tt
		match rules
		search for the nodes 

	queries can use node type

universal root for build
	it is possible to build from any location
		a cwd not the root of the system to build
			finding the files under . (virtual root)

		a cwd root of the system to build
			normal build

		a cwd under the root of the system to build
			subset build			

subpbs rule should not care about the path of node
	rules now have to match */regex as the start node has a path
	but the path doesn't make sense in the local environment of the subpbs, the nodes
	should get their path based on their parent's path. this would simplify the run of 
	pbs -load_config as we can give the local node name instead.

	some node have path information ./lib/xxx gets under the pbs root under ./lib (late depend)
	some nodes may be related to the root (/)

	before starting a subpbs, the node to depend could get an alias without the path
		

 	this would allow moving whole subsets of the system and still be able to use the warp file
	as is since there is no path recorded anymore




All display code is a plugin, allowing the user to change how the display looks like
	without changing the core code

	use a templating system or subs

	can the display code be triggered by injected rules?

H files are dependencies of the object file
	if two object files use the same C file, the dependencies may be different because of the
	object file configuration. Linking the C file with a configuration that is wrong, when
	depending the second object file, is an error.



virtual nodes without dependencies ALWAYS trigger
	a virtual node without dependencies always trigger
	there is no way we can know if a virtual node without dependencies "succeeded"
	we can know that it failed because the commands it runs fail but if they all succeed
	we don't have a node we can make a md5 on so we must re build the virtual node each  time

	need example of virtual node without dependencies

	this is so frustrating. All those years talking about virtual node digests in complex setup (subpbs linked nodes, warp graphs, ...) when we should have kept it simple and see what the problem is with VIRTUAL, it's simple, we didn't know where to write its digest (that's so hard to decide) and that it didn't trigger. So rather than do the right thing we introduced FORCED, Do I hate make and all the make files that other morons like me wrote that forced things.
	FORCED must die and than everything will be fine because we must do the right thing.
	a virtual node, whatever the context, as it has no digest will ONLY trigger if one of its dependencies also triggers. This means that you can add a virtual node rule and it will never trigger, you can modify a virtual node rule and it will never trigger, virtual nodes don't trigger, they have no control over their triggering, it's the dependencies that trigger them, and the dependencies may trigger for various reasons none of them related to the virtual node rule.
	You can even write a virtual node rule that has dependencies on source nodes, EG document the source nodes, and it will never trigger.
	VIRTUAL nodes don't trigger, so rather than make them trigger properly we FORCED them to trigger, I hate make.
	if virtual nodes have triggers, they will trigger on a dependency that trigger, as before, on source files, and on the pbsfile that where the virtual node rule is defined. 
	Why did we need a virtual node digest? For everything!
	Where to put the virtual node digest is obvious, in the out directory, as everything else. What if they match a directory name, no problem, the directory and the digest don't have the same name.
	It's easy to be smart after the fact but it is pretty obvious that if you don't handle nodes the same way, there are going to be differences, why didn't we handle VIRTUAL  as a normal node? why didn't we handle VIRTUAL as we defined in the documentation " a node that has no physical representation in the file system", a 2 lines check, but instead make a monster out of it.
	Next PBS will be better, VIRTUAL will have a digest. 

	what if the dependency is a virtual node itself, what do we write in the digest?
		the virtual dependency digest!

2 dependents share dependencies. I don't think it will be in the form t1, t2 -> d1, ....
	but rather t1 -> d1, ... ; t2 -> get_deps(t1).
	that allows a tx -> get_deps(t1, t2, whatnot) picking up dependencies other nodes have without having to change t1, t2 pbsfiles. probably late_depend will work on them too so t2 can be dependent on t1 dependencies even if t1 is not in the graph yet


NAME => 'any generated data is exportable to another format',

NAME => 'defining_subpbs_with_a_sub',
LONG_DESCRIPTION => <<'EOD',
	give a sub that checks the type of the node to generate and create
	a virtual subpbs if non exists in the sub directory (for known types)
 
	show a message if a subpbs is generated
	use pbsfile if it exists
	if given --generate_missing_pbsfile, ask if a real
		pbsfile should be generated. practical if the virtual
		generated pbsfile doesn't fit (ex directories or files to ignore, more configurations)


REH to match configurations
	AddRule
		MATCH => $regex
		MATCH_CONFIG => ...

	this is related to REH_IF

REH to match steps in the pbs run
	this is already done in the registration of the REH which doesn't look like rules
		and REH can not be registered to multiple steps 

	it also need to match a node and pbssteps are not nodes (yet)

	a good example is -gtg, we generate a graph at specific moment
		gtg is implemented as a plugin in pbs1, thus pbs calls it

All output is done via a registrable sub to allow PBS2 to be embedded in another
	system, EG: neo

	display system can be setup to display information in specific cases only
		node
		pbsfile
		pbs run
		...


REH  NAMED_CONFIG AND NAMED_CONFIG_GROUP
	no problem adding the REH but how does one get to the config
	by name later?

command line options and wizards
	can a rule match a command line option as if it was a target
		MATCH_CLI => /dd/
		
		how do we define our command line options
			some global place plus in the plugins
	
	wizard is just a target 
		how do we add all the targets?
			just load a library with the rules

plugins
	some are just targets, ie generate_graph, which need to be run at specific
	pbs time

	Rule 'generate_graph'
		MATCH_CLI => /-gtg/
		GENERATE_GRAPH_REH => ....  # knows when to be run


	bleah! make the target a virtual node!
		Rule 'generate_graph'
			MATCH => '__pbs_graph'
			AND_MATCH_CLI => /-gtg/
			BUILD => ...
		
plugins
	some are called during the pbs run, eg they register subs that pbs calls
		EvaluateShellCommand
		CheckNodeName
		
		The problem being is that they are not node specific
		we can eliminate global plugins and query the nodes for the 
		plugins to run. A global plugin simply matches all nodes

	AddRule
		MATCH => $regex
		EVALUATE_SHELL_COMMAND_specific => 1

	We can't audit plugins as we did with pbs1, finding the plugins becomes more
	difficult as they can be inlined 

prefix all the REH that have an impact on internal implementation
	DISPLAY_DEPENDENCIES => 1
	PBS_DISPLAY_DEPENDENCIES => 1

	except if it is the DISPLAY_DEPENDENCIES REH itself that does the displaying

generalize the C depender
	- what configuration variables must be in the configuration should not be in the code but arguments
	- the same code can be used for other types of files
		- creators use an equivalent mechanism, could be merged

	AddRuleTo 'BuiltIn', [POST_DEPEND], 'C_dependencies', \&C_SourceDepender, \&C_Builder ;

	Rule 
		NAME =>  'C_Depender'
		REGEX => '*.cpp'
		C_DEPENDER => { ........ }

	Rule 
		NAME =>  'C_config'
		REGEX => '*.cpp'
		CONFIG => { ........ }

		C_DEPENDER_VARIABLES => named list




-bni ,which show a node header, makes it clear which node is being build, there is no such system
	for -dd, the display is very compact

NODE_LINKING is a PBS step with potentially matching REH
	what configuration to check
	allowed or not

configuration dependency for node
	knowing what configuration is used to build a node makes it possible to check
	if a linked node uses the same configuration as the current pbsconfig  build node
		maybe the linked node, with a different configuration and dependencies is OK!
			need a mechanism to allow that.

	extract all the configuration variables from the build commands
	force declarative style for linked node checking
		note that this can still check the build commands

	addrule
		name xxx
		regex '*all*'
		NODE_LINKING_ENABLE => 0 # current pbsfile run must provide one

		NODE_LINKING_ENABLE => 1
		NODE_LINKING_CHECK_VARIABLES => ['name', 'name2', ..]
		NODE_LINKING_CHECK_VARIABLES => {'name' => \&checker, ..}
		NODE_LINKING_CHECK_COMMAND_LINE => 1

		
	knowing which configuration is used also allows to not rebuild some nodes declared in
		the same pbsfile because of a configuration change that has no impact
		on the node

if a rule uses sub rules, EG lib uses object rules, when the rules for obj
	are used, the user can easily trace it back to the lib rule and to where it was used
	
	IE. inserted_at is not enough as all libs are inserted at the same place, the lib rule

move statistic display to plugin (or plugins)
	add a plugin display point in pbs

console
	possibility to query between steps

	starts build or loads a graph dump

	loads warp and checks
		query about warp and 	
	
	depend
		can stop
		can add breakpoints
		can query about graph or node
		can generate graphs

	check

	build
		
option so warp information display is mutted

ExcludeFromDigest may not make sense
	use to say that a node is a source node, better call it that
	why not give a digest to a source node
		win time generating and checking it
		forces the node to be treated differently


EvaluateShellCommand REH

Pbs creators generate mini build systems
	mechanism used to generate and check the creator digest
	pbs command line switches (-bni, -conf, ...) are also used
	
Use file type "matchers"
	rather than use regex use a file type (which still can be implemented as a regex for simple cases)
	the "matcher" can match depending on a configuration (eg the current OS, tool chain, ...)
	
multiple GetBuildName subs defined in the code

example in Pbsfile/Creator do not work

Merge STE PBS

Create template/example for STE like data driven pbsfile (neo)

option to filter out/in nodes when displaying tree
	generate list of files that are displayed and not displayed, their amount, ...
	
	if options are pbsconfig variables, the REH an use them

possibility to generate multiple trees with different roots
	to display only the relevant nodes
	can still filter nodes in and out
	
generate DHTML tree


Multiple node builders
	builders run in the order of rule matching
		option to display the builders and their order
		
	builders declare if they are singletons, an error is generated if builders exists before or are added after
	
	
build phases
	a node with multiple builders has its builders run in a specific order
	a builder declares which phase it belongs to
	a rule can declare the phases a builder can belong to 
		check is done for multiple rules declaring different orders
		check is done to verify that each phase has a builder
			should it fail?
	
	
Mechanism, at a higher level, to declare in which order REH are run
	similar to build phases
	
	
Builders chaining
	get all previous builders results
	get information pointer to the previous builder
	get the last builder build buffer
		can request the build buffers of all the previous builders
		
	builder can make variables available after build
		eg: tell the rest of the build chain what files are created, ...
		

Build buffers 
	switch to keep build buffers
		filter which ones are kept
			faild ones always kept

failed build buffer
	always keep the buffer
		garbage collect the filters after successful build
			rule matching all the files
			do we want to garbage collect at the global level or after each node, after each subpbs, ...
				object_files can collect the names of the buffers to garbage collect

			do we keep the build buffers in case of multiple failures?
		
		
	keep information about the builder
	create a mini build system
		configuration
		possibility to run just that builder/ builders for a node
		
	
possibility to display the tree in warp mode
	global tree, warp tree
	filters should still apply
	
File not found for MD5 computation
	most probably a node that is not declared as virtual (although we have defined somewhere that virtual nodes should have a digest)
	builder failed
	=> display as much information as possible
		mini build system for just that error
		
generation of mini build systems
	in big systems, the developer does not want to re-run the build system just to find out what the problem is but wants a small build to debug the problem
	
Post pbs 
	can be implemented with a rule that matches the pbs run virtual node
		better than a rule that matches a target in case there are multiple runs of the pbsfile with different targets
		REH_POST_PBS => \&post_pbs_sub
		REH_POST_PBS => GetPBSConfig('POST_PBS_FILE')
		
	C depender should use the flags defined by the user rather than force flags on the user
	
	
override after **------ -dpl
	don't know what I meant
	if subpbs run (two extra phases here prepare to run subpbs and run subpbs, plus subpbs ran) matches a rule, it can display something else
	
	
Make clear how build command line parameters (%FILE_TO_BUILD, ...) are defined and handles
	also defined in plugins or rules
	
	
plugin to generate report on configuration usage
	a post pbs part of the examples in the distribution 
	
	configuration in, defined, used, delta with parent, delta with children (available after the subpbs runs)
	knowing how many configuration variables (along with rules, targets) can be used to automatically find what pbsfiles should be split
	
give example of C code generation
	or anything that needs to be depended
	explanation and example of IMMEDIATE_BUILD or creators
	
Creator, IMMEDIATE_BUILD, VIRTUAL, FORCED
	all can be REH!
	
rethink/document IMMEDIATE_BUILD FORCED, LOCAL_RULES and other exceptional mechanisms

REH_TRIGGER rather than pbs functionality
	starts a subpbs before the REH_LINK_TO
	
C_FILE_SYNCH
	it's a post build REH
	Make it visible
	
ON_ERROR REH
	for un interesting parts of the build just ignore the failures
		need to make it very clear
		
		
Express complicated dependencies
	references to dependency lists, but still locally defined
	depender subs are all mighty already
	simple interface to the dependers, better wizard
	
user defined wizards
	possibility to have multiple wizard directories 
		no need to put user wizards where pbs wizards are

