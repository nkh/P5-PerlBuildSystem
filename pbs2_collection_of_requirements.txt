
PrintDebug2
	we need to have a color tag at the beginning of print output to be able to differentiate
	different entries in the same category
		PrintInfo
			what kind of info?
				if we have three categories then it should be made clear
					print output to be able to differentiate
	different entries in the same category
		PrintInfo
			what kind of info?
				if we have three categories then it should be made clear
					aprint output to be able to differentiate
	different entries in the same category
		PrintInfo
			what kind of info?
				if we have three categories then it should be made clear
					■ ■ ■ 
					each ■ gets colored 
					alternatively the catagory name is displayed
--display build result displays result for each builder element

Warp verification message
	display some statistics about the types of nodes in the cache in verification message
	make display single line

		Warp load time: 0.06 s.
		Verifying warp: 3801 nodes ...
		Warp verification time: 0.14 s.
		Warp total time: 0.20 s.
		Warp: Up to date.
		Total time in PBS: 0.23 s.

		Warp: UP TO DATE. 3801 nodes (1400s, 1600g, 801p), load: 0.06 s. verification: 0.14 s
		Total time in PBS: 0.23 s.


wrapper to check test result
	AddRule :Name(test)
		Is("any_command arg %arg", regex)	



bash completion should be done by pbs as some options are loaded at run time
	wrong, the options are loaded after the application is executed so we don't
	know what will happend in the completion script if we do not run the current command line
	which is prohibitive for completion purpose

warp, on pbsfile different => need to rebuild everything
	because warp doesn't have pbsfile-nodes dependency? see mini warp


Add a mode to depend as little as possible, IE no C dependencies cache, no checking sub nodes if top node needs to be rebuild, ...
	generate the dependency graph after building if the information is available

--no_warp creates warp file even if it doesn't use it

how much more time does it take to create a warp 1.5 and 1.8 compared to only warp file 


declare possible top target in pbsfile
	allows us to scan all pbsfiles (what files?! pbsfiles can be called anything) to find targets
		 - can help the user decide what target to build or where a target is build

	we can cache the target names in warp

	target can be  pbsfile tag or a reh
		1/ Target(something)
		2/ Rule :NameMatch(/xxx/), :IsTarget ;

pbs/subpbs return a miniwarp
	there is no reason to give the top part of the graph to subnode 
		linking to existing node?
		better encapsulation of dub level

	the pbs serialized the miniwarp
		calling pbs can ask for a live sub graph

	calling pbs has to link nodes if they exist in the graph
		if same node and config
			otherwise we have conflicting configs

	create check file/build sequence at the same time as the creation of the mini warp
		we can parallelize the check 
		nodes in mini warp build sequence must be tagged if they are terminal
			we don't need to compute it that in the parallelizer if it is already done

	returning a warp file + check/build sequence means that we do not have to keep the live nodes 
	in memory
		NOTE: the build sequence is warp 1.8 first level dependencies, up to the calling pbs
		
		we can create w1.8 first level on the fly

how do we detect circular dependencies if nodes are depended in different process each creating
	a graph that is not circular
	
	we can run a check pass on the dry graph just for that, same as today but much lighter

	we can merge the build sequence and if a node is present twice, we have a cyclic dependency

	use tsort

work out the user help vs the pbsfile help
	=for PBS 


sub options set the parent option
	IE: --ttno needs --tt on the cli or it will have no effect

	writing --ttno will automatically set -tt

	if parent option needs an argument, display error message and stop

display options as a tree when -h is used
	category
		sub category
			top option
				sub options
	options
	├─ help
	│  ├─ -h|help = Displays this help.
	│  ├─ -hs|help_switch=s = Displays help for the given switch.
	│  ├─ -hnd|help_narrow_display = Writes the flag name and its explanation on separate lines.
	│  ├─ -generate_bash_completion_script = Output a bash completion script and exits.
	│  ├─ -pp|pbsfile_pod =
	│  │  Pod declared with "=for PBS =pod_formating title" is extracted when option is set
	│  │
	│  │  =for PBS =head1 Targets\n"
	│  │
	│  │  =item * all
	│  │
	│  │  =item * debug
	│  │
	│  │  =cut
	│  ├─ pod
	│  │  ├─ -d|display_pod_documenation:s = Interactive PBS documentation display and search.
	│  │  ├─ -pbs2pod = Extracts the pod contained in the Pbsfile (except user documentation POD).
	│  │  └─ -raw_pod = -pbsfile_pod or -pbs2pod is dumped in raw pod format.
	│  └─ wizards
	│     ├─ -wh|display_wizard_help = Tell the choosen wizards to show help.
	│     ├─ -w|wizard:s = Starts a wizard.
	│     └─ -wi|display_wizard_info = Shows Informatin about the found wizards.
	└─ colors
	   ├─ -no_colorization = Removes colors from output. Usefull when redirecting to a file.
	   ├─ -c|colorize =
	   │  If Term::AnsiColor is installed on your system, use this switch to
	   │  colorize PBS output.
	   │
	   │  PBS has default colors but colorization is not turned on by default.
	   │
	   │  Colors can be defined through switches (try pbs -h | grep color) or
	   │  by setting you colors in the environement variable 'PBS_FLAGS', ex:
	   │          export PBS_FLAGS='-c -ci green -ci2 blink green -cw yellow'
	   │
	   │  Recognized colors are :
	   │          'bold'
	   │          'dark'
	   │          ...
	   │  Check 'Term::AnsiColor' for more information.
	   └─ element colors
	      ├─ -cw|color_warning=s = Set the warning color.
	      └─ -ce|color_error=s = Set the error color.
	  
keep logs, output, ... for every run of pbs
	so we have something to look at when somethingg builds, or doesn't, when we expected it not to, even if pbs is most probably right

	one often reruns pbs just to get more information with an extra switch
	if the information is not to be displayed, keep its data in a separate file

	we need a system to cleanup periodically the unnecessary data, an option
	--run_data_limit/-rdl (and --no_run_data) which is set by default to, say 5, and that can be
	changed in the configuration.

		--nrd 		don't keep this run's data
		--rdl 0		never keep run data and remove the data found on disk

		if we keep multiple logs, we will need to keep multiple node data for nodes that are rebuild

	node data should be in the node's digest and only tree data kept in log
		we can reconstruct on the fly

		node digest's name should contain the md5 to ease its localization
		with command line tools

		node data should be kept as string to avoid compiling it when loading the digest
			in comments that we can "uncomment"
			in strings that we can ask the digest to return to us

		the node object could itself be the digest, eg: the node is serialized and it can
		be queried for its digest or data

		node data contains the build output

	* support --jobs
		builder creates node's data files

		master creates 
			warp file
			build sequence (linear, although it can be recreated from the graph if we have a list of triggered nodes)
			build event (queuing, starting, ending) which can be used to create a timeline
		
tree display additive fields
	--tt is the bare tree, one can add elements to the tree via switches
	completion and help are available

tree display color
	node name should be made clear
	additive fields are color like command line switch

HTML output
	terminal that is HTML aware to allow DHTL tree dumps

	fast output
		revert to pure text without html
		display html with max depth set
			user can still open data structure dump further
		load data on demand
			trees are just links

		filters
			show only some of the pbs phases
			show specific node
			show specific subsystem


	pbs shell should support HTML

continuous integration
	keep logs of builds and html interface
		builds are not started via interface nor via commit hooks

	check everything is commited
		commit in a CI repo

	builds when local machine is not powerful enough
		
	start remote build on local machine

	check and reuse environement if possible
		

IPFS as synchronized build cache
	a node to be build can request itself from some other build-node cache

	caches can  synchronize as long as they have disk space so if a node is 
	requested, it can be delivered in parallel from multiple build-nodes
Tmux
	like gnu parallel, in a specific session for the pbs run
		output  in a window or in a pane
		do not keep parts of the builds that succeed by default

	controlled by REH

Build distribution
	the build system can clone itself (and all deppendencies) to another computer
	if a node is changed
		if it impacts many other nodes, the node is synchronized and all the 
		build nodes co-operate to the build

		if it impacts little, we can know that from warp 1.8 data, the build is
		made locally and nodes are synchronized after the build

debugging
	when node fails to build, setup a miniature environment with copies
	of the failing node, the necessary dependencies (a warp tree for the nodes that only be
	present but not used), ... 


	use rules and REH
		ca setup an environment specific to the node type

		Rule
			MATCH_ALL
			REH_ON_FAIL => \&setup debug environment

Tmux test environment
	keep only failing tests
	stop after x number of failing tests
	

three pane debugger with preview
	a ranger like display parents (can be multiple parents), this, children
	preview shows rules, config, ...

debugger
	trace: shows how we got to the point where we are

	"next": show the rules that match, this should also show in the graphical browser

	in case of error show all the relevant data, including the results of the command, and the
	possible output, graphical browser shows what other rules would fail

	break point show type of breakpoint, pre-depend, ...

	set/unset debug point easily and quickly
		set and unset based on some code returned value
			ie, as long as number of nodes in graph < 50 continue
			have a rich data set the functions can work with
			data set is per node, module, level, range of levels, ...

	REH_DEBUG_SET_BREAKPOINT

environment variables
	pbs removes all the environment variables
		this helps but when a sub command fails or produces an unexpected output
		(because it did not check properly) this helps the user very little

	pbs removes all the environment variables if option is set

	pbs replaces all the environment variables with "SET BY PBS" to make it clear

	find a system to "break" the commands that use environment variables

	removal of environment variables should be per node/level
		node/level can set variables

	warnings if environment variables are set or not set

 
debugger watches
	extracted by code snippets from the available data set, they don't have 
	to represent a variable, they can represent whatever, a bit like an element in
	a bash prompt
		
	doesn't have to be perl code or an element of the graph
		ex. a service is available on a remote server
		ex. cpu load, free disk space, ...
	
	good if implemented as debugging REH in a rule

warp does not need normal digest
	when running in warp mode and something need to be rebuild
		this helps but when a sub command fails or produces an unexpected output
		(because it did not check properly) this helps the user very little


warp does not need normal digest
	when running in warp mode and something need to be rebuild
	the no-warp mode building is run; it generates digest files
	and c dependency files. those files are not used by the warp 
	mode. the warp mode should apply to normal build too to avoid
	generating the digests. normal mode data normally ready from 
	digest files can be obtained from the warp file.
	Note that this is not a big problem as digest file generation 
	is trivial and that most of the time is spend computing md5
	which are also needed for the warp file. It does have an 
	"advantage" if the warp data is deleted or a different warp
	mode is selected, the normal mode is fast.

warp 1.8
	can we reduce the format of the warp files, textual amount as
	well as fields in the warp files

	there is ROOT directory in the warp 1.8 data that should not be there!

	single node warp file take the longest to process, is there a way to 
	optimize the process by collating related nodes in the same file?

	node regeneration should not need to set BUILD_NAME
	try to make node regeneration more light weight
		if some rarely used plugins need more data, it is better
		to do more re-generation in the plugins

parallel depend
	done on the subpbs level
	can generate a mini warp
		who merges back
		merging should be distributed too

	merging is mini warp is no necessity, we can follow the chain
	and read multiple mini warp files

	mini warp must merge very quickly
		two data sets
			one with complete data
			one lightweight to merge quickly
				see warp 1.8 node regeneration
			keep a md5 between them
		mini warp have warp 1.8 format
		merging 
			merge checks link errors
			adding node md5 to list
			single node warp files need parent dependent added
			 
	a set of hierarchical mini warp can be merged at multiple levels
		only merge the quick data and keep reference to the complete data

	

warp nodes can be formatted as the reconstructed nodes to speed up loading
	
nodes that are the deepest trigger the most files
	verify them first as they re-vivify the most nodes and we cache those

nodes that trigger often are more likely to trigger again

sample data digest generation for big files
	md5 on a 5 Mb file takes longer than on a 4 kb file
	compute md5 on a few well chosen sections  rather than the whole file

option to generate digests for source files only
	MUCH less secure but much faster

	we should actually CHECK the digests for source only, we can still generate digests
	for generated files, that would give the possibility to run with checking or do an
	external check if needed.  

check optimization in Ag
	https://github.com/ggreer/the_silver_searcher

node name completion can be done with the help of nodes listed in the warp file
	we still need to know which warp file

log all builds
	every command line that builds should be save in a log

keep statistics about what is the most often build
	gives us an idea about what has most impact on the system to build and
	what part of the build system is most often used

	let a user specific post pbs handle that (we can give an example)

time cursor (see Log)

	a history of the build is kept so that a cursor can be placed at a user chosen
	time when she can look at what the graph looks like, what will happen next, what rules
	inserted a node, ...

	keep a history of the graph generation, a history of the  build makes no sense as we can build in parallel
	and the order changes. It makes more sense to get the build for a specific node and select
	its children or parents to look at their build.

C depender triggering
	if dependency file does not exist, trigger and post generate the file instead for an immediate generation 
	this should be the default but an --option may be added for people that want an analysis not a build

C depender generates digests in mini warp format (because it is exactly that) and the 
	linking of the mini warp nodes for the C depender is no different from other
	mini warp

mini warp does not contain node data, just enough for regeneration

document why, when using warp, the C dependency files are neither used nor verified
	=> they are merged in the warp graph and warp doesn't care about them, but the 
	dependencies are still verified

Output from Hashes must be sorted as Perl randomized the key order

shell build nodes can receive tools, commands and the code to work on from master
	reduce the need for a shared file system

shell build nodes can share code with other nodes
	when the master starts a build, it also starts the sharing of the code, while the
	master works to find what is going to be build, the build nodes share the code.

shel build nodes keep a repository of shared code

shell build nodes can update their repository
 
build system has startup rules that install tools and checkout code
	although checking out the code with the build system (e.g. in the same commit) makes more sense

parallel depend
	OK to have multiple instances (in each process doing the depend) if the common nodes have the
	same digest, including the configuration. if the digests are different we need a mechanism to
	let the user decide what happens next.

documentation is part of every release

REH instead for named dependencies or as a complement
		rule
			named => (c-dependencies => a, b, c), 
			c-dependencies => (a, b, c), 

	this also allows the REH to 'type' the dependencies and itself


REH user defined type
	allow visualization of the REH in a user specified manner
	e.g. also when the dependencies are listed, the dependencies from a specific REH could be in red color


display dependency list 

	Node [V] './objects':
		Inserted at ./pbsfile [PBS]:__ROOT.
		dependencies:
			./1.objects
		rebuild because of:
			./1.objects (Subdependency or self)
		matching rule: #2[B] 'objects'
			=> ./1.objects
	  

	displays the REH that matched and uses the color from the REH type


rebuild because of: ./1.objects (Subdependency or self)
	be precise, dependency or self, or both, and which dependencies


always save the subpbs configuration
	possible to query the configuration for a specific node
	history of configuration is also displayed

build subset
	a better way than node@root and save/load configuration

			  .---------------------.
			  | .---.               |
	       .------------| A |--------.      |
	       |          | '---'        |      |   wait for B to build 
	       v          |              v      |   correctly to build  
	 .-----------.    |            .---.    |
	 | subsystem |    |            | C |    |
	 '-----------'    |            '---'    |
			  |              |      |
			  '--------------|------'
					 |
			  .--------------|------------.
			  |              |            |
			  |              v            |
			  |            .---.          |
			  |        .---| D |---.      |
			  |        |   '---'   |      |  Only build this till OK 
			  |        v           v      |
			  |      .---.       .---.    |
			  |      | E |       | F |    |
			  |      '---'       '---'    |
			  |                           |
			  '---------------------------'


	When building A, if an error occurs, find which subsystem the error occurs in and
	give all the necessary information to build that subset from the command line, see
	setup subset build for a manual setup

	building a subset is like building a specific subpbs with a saved configuration, there is no
	need to build the parents of the subset

	it is possible to limit the build to the first level of the subset even if configuration
	or graph changes are made at the subset level

	when building a subset, make it clear to the user that it is only a subset build so that 
	it is clear that a top build needs to be made, return an error code and the warnings


setup a subset build
	the user can ask the build system to generate the command line to build a specific file or
	subsystem.

	say that a subsystem build a library
		the user can ask for the closes pbsfile to build that library
		the build system
			creates a (universal based) command line
			creates a configuration file copy for the subsystem
	
	the creation is done 
		based on data in the warp file
			if no pbs related files trigger (libs, pbs, pbsfiles) 

		by running the system in depend only mode to generate the necessary data

user defined node type
	subsystem, debug, UI, ..

	the user can give attributes to nodes

	those attributes can be used to 
		display nodes differently
		display the attributes in a --tt
		match rules
		search for the nodes 

	queries can use node type

universal root for build
	it is possible to build from any location
		a cwd not the root of the system to build
			finding the files under . (virtual root)

		a cwd root of the system to build
			normal build

		a cwd under the root of the system to build
			subset build			

subpbs rule should not care about the path of node
	rules now have to match */regex as the start node has a path
	but the path doesn't make sense in the local environment of the subpbs, the nodes
	should get their path based on their parent's path. this would simplify the run of 
	pbs -load_config as we can give the local node name instead.

	some node have path information ./lib/xxx gets under the pbs root under ./lib (late depend)
	some nodes may be related to the root (/)

	before starting a subpbs, the node to depend could get an alias without the path
		

 	this would allow moving whole subsets of the system and still be able to use the warp file
	as is since there is no path recorded anymore




All display code is a plugin, allowing the user to change how the display looks like
	without changing the core code

	use a templating system or subs

	can the display code be triggered by injected rules?

H files are dependencies of the object file
	if two object files use the same C file, the dependencies may be different because of the
	object file configuration. Linking the C file with a configuration that is wrong, when
	depending the second object file, is an error.



virtual nodes without dependencies ALWAYS trigger
	a virtual node without dependencies always trigger
	there is no way we can know if a virtual node without dependencies "succeeded"
	we can know that it failed because the commands it runs fail but if they all succeed
	we don't have a node we can make a md5 on so we must re build the virtual node each  time

	need example of virtual node without dependencies

	this is so frustrating. All those years talking about virtual node digests in complex setup (subpbs linked nodes, warp graphs, ...) when we should have kept it simple and see what the problem is with VIRTUAL, it's simple, we didn't know where to write its digest (that's so hard to decide) and that it didn't trigger. So rather than do the right thing we introduced FORCED, Do I hate make and all the make files that other morons like me wrote that forced things.
	FORCED must die and than everything will be fine because we must do the right thing.
	a virtual node, whatever the context, as it has no digest will ONLY trigger if one of its dependencies also triggers. This means that you can add a virtual node rule and it will never trigger, you can modify a virtual node rule and it will never trigger, virtual nodes don't trigger, they have no control over their triggering, it's the dependencies that trigger them, and the dependencies may trigger for various reasons none of them related to the virtual node rule.
	You can even write a virtual node rule that has dependencies on source nodes, EG document the source nodes, and it will never trigger.
	VIRTUAL nodes don't trigger, so rather than make them trigger properly we FORCED them to trigger, I hate make.
	if virtual nodes have triggers, they will trigger on a dependency that trigger, as before, on source files, and on the pbsfile that where the virtual node rule is defined. 
	Why did we need a virtual node digest? For everything!
	Where to put the virtual node digest is obvious, in the out directory, as everything else. What if they match a directory name, no problem, the directory and the digest don't have the same name.
	It's easy to be smart after the fact but it is pretty obvious that if you don't handle nodes the same way, there are going to be differences, why didn't we handle VIRTUAL  as a normal node? why didn't we handle VIRTUAL as we defined in the documentation " a node that has no physical representation in the file system", a 2 lines check, but instead make a monster out of it.
	Next PBS will be better, VIRTUAL will have a digest. 

	what if the dependency is a virtual node itself, what do we write in the digest?
		the virtual dependency digest!

2 dependents share dependencies. I don't think it will be in the form t1, t2 -> d1, ....
	but rather t1 -> d1, ... ; t2 -> get_deps(t1).
	that allows a tx -> get_deps(t1, t2, whatnot) picking up dependencies other nodes have without having to change t1, t2 pbsfiles. probably late_depend will work on them too so t2 can be dependent on t1 dependencies even if t1 is not in the graph yet


NAME => 'any generated data is exportable to another format',

NAME => 'defining_subpbs_with_a_sub',
LONG_DESCRIPTION => <<'EOD',
	give a sub that checks the type of the node to generate and create
	a virtual subpbs if non exists in the sub directory (for known types)
 
	show a message if a subpbs is generated
	use pbsfile if it exists
	if given --generate_missing_pbsfile, ask if a real
		pbsfile should be generated. practical if the virtual
		generated pbsfile doesn't fit (ex directories or files to ignore, more configurations)


REH to match configurations
	AddRule
		MATCH => $regex
		MATCH_CONFIG => ...

	this is related to REH_IF

REH to match steps in the pbs run
	this is already done in the registration of the REH which doesn't look like rules
		and REH can not be registered to multiple steps 

	it also need to match a node and pbssteps are not nodes (yet)

	a good example is -gtg, we generate a graph at specific moment
		gtg is implemented as a plugin in pbs1, thus pbs calls it

All output is done via a registrable sub to allow PBS2 to be embedded in another
	system, EG: neo

	display system can be setup to display information in specific cases only
		node
		pbsfile
		pbs run
		...


REH  NAMED_CONFIG AND NAMED_CONFIG_GROUP
	no problem adding the REH but how does one get to the config
	by name later?

command line options and wizards
	can a rule match a command line option as if it was a target
		MATCH_CLI => /dd/
		
		how do we define our command line options
			some global place plus in the plugins
	
	wizard is just a target 
		how do we add all the targets?
			just load a library with the rules

plugins
	some are just targets, ie generate_graph, which need to be run at specific
	pbs time

	Rule 'generate_graph'
		MATCH_CLI => /-gtg/
		GENERATE_GRAPH_REH => ....  # knows when to be run


	bleah! make the target a virtual node!
		Rule 'generate_graph'
			MATCH => '__pbs_graph'
			AND_MATCH_CLI => /-gtg/
			BUILD => ...
		
plugins
	some are called during the pbs run, eg they register subs that pbs calls
		EvaluateShellCommand
		CheckNodeName
		
		The problem being is that they are not node specific
		we can eliminate global plugins and query the nodes for the 
		plugins to run. A global plugin simply matches all nodes

	AddRule
		MATCH => $regex
		EVALUATE_SHELL_COMMAND_specific => 1

	We can't audit plugins as we did with pbs1, finding the plugins becomes more
	difficult as they can be inlined 

prefix all the REH that have an impact on internal implementation
	DISPLAY_DEPENDENCIES => 1
	PBS_DISPLAY_DEPENDENCIES => 1

	except if it is the DISPLAY_DEPENDENCIES REH itself that does the displaying

generalize the C depender
	- what configuration variables must be in the configuration should not be in the code but arguments
	- the same code can be used for other types of files
		- creators use an equivalent mechanism, could be merged

	AddRuleTo 'BuiltIn', [POST_DEPEND], 'C_dependencies', \&C_SourceDepender, \&C_Builder ;

	Rule 
		NAME =>  'C_Depender'
		REGEX => '*.cpp'
		C_DEPENDER => { ........ }

	Rule 
		NAME =>  'C_config'
		REGEX => '*.cpp'
		CONFIG => { ........ }

		C_DEPENDER_VARIABLES => named list




-bni ,which show a node header, makes it clear which node is being build, there is no such system
	for -dd, the display is very compact

NODE_LINKING is a PBS step with potentially matching REH
	what configuration to check
	allowed or not

configuration dependency for node
	knowing what configuration is used to build a node makes it possible to check
	if a linked node uses the same configuration as the current pbsconfig  build node
		maybe the linked node, with a different configuration and dependencies is OK!
			need a mechanism to allow that.

	extract all the configuration variables from the build commands
	force declarative style for linked node checking
		note that this can still check the build commands

	addrule
		name xxx
		regex '*all*'
		NODE_LINKING_ENABLE => 0 # current pbsfile run must provide one

		NODE_LINKING_ENABLE => 1
		NODE_LINKING_CHECK_VARIABLES => ['name', 'name2', ..]
		NODE_LINKING_CHECK_VARIABLES => {'name' => \&checker, ..}
		NODE_LINKING_CHECK_COMMAND_LINE => 1

		
	knowing which configuration is used also allows to not rebuild some nodes declared in
		the same pbsfile because of a configuration change that has no impact
		on the node

if a rule uses sub rules, EG lib uses object rules, when the rules for obj
	are used, the user can easily trace it back to the lib rule and to where it was used
	
	IE. inserted_at is not enough as all libs are inserted at the same place, the lib rule

move statistic display to plugin (or plugins)
	add a plugin display point in pbs

console
	possibility to query between steps

	starts build or loads a graph dump

	loads warp and checks
		query about warp and 	
	
	depend
		can stop
		can add breakpoints
		can query about graph or node
		can generate graphs

	check

	build
		
option so warp information display is mutted

ExcludeFromDigest may not make sense
	use to say that a node is a source node, better call it that
	why not give a digest to a source node
		win time generating and checking it
		forces the node to be treated differently


EvaluateShellCommand REH
	pre-build REH

	does not evaluate the shell command but createa node specific configuration to match %Whaetever
	
	this allows us to define the REH in a user lib and it can be reused

	a better name is REH_EVAL_CONFIG for a generic sub declaration 

REH can be argument less
	they are more like sub calls then

	this allows a more declarative style for the REH tha do not need arguments
		Rule
			Name => 'declarative rule'
			DECLARATIVE_REH,
			...
			;
		
Pbs creators generate mini build systems
	mechanism used to generate and check the creator digest
	pbs command line switches (-bni, -conf, ...) are also used
	
Mini warp are mini build systems
	getting into a sub directory the user shouldn't hav eto do more than "pbs" on the command line
	to build that sub system again, with the top config!

Use file type "matchers"
	rather than use regex use a file type (which still can be implemented as a regex for simple cases)
	the "matcher" can match depending on a configuration (eg the current OS, tool chain, ...)
	
multiple GetBuildName subs defined in the code

example in Pbsfile/Creator do not work

Merge STE PBS

Create template/example for STE like data driven pbsfile (neo)

option to filter out/in nodes when displaying tree
	generate list of files that are displayed and not displayed, their amount, ...
	
	if options are pbsconfig variables, the REH an use them

possibility to generate multiple trees with different roots
	to display only the relevant nodes
	can still filter nodes in and out
	
generate DHTML tree


Multiple node builders
	builders run in the order of rule matching
		option to display the builders and their order
		
	builders declare if they are singletons, an error is generated if builders exists before or are added after
	
	
build phases
	a node with multiple builders has its builders run in a specific order
	a builder declares which phase it belongs to
	a rule can declare the phases a builder can belong to 
		check is done for multiple rules declaring different orders
		check is done to verify that each phase has a builder
			should it fail?
	
	
Mechanism, at a higher level, to declare in which order REH are run
	similar to build phases
	
	
Builders chaining
	get all previous builders results
	get information pointer to the previous builder
	get the last builder build buffer
		can request the build buffers of all the previous builders
		
	builder can make variables available after build
		eg: tell the rest of the build chain what files are created, ...
		

Build buffers 
	switch to keep build buffers
		filter which ones are kept
			faild ones always kept

failed build buffer
	always keep the buffer
		garbage collect the filters after successful build
			rule matching all the files
			do we want to garbage collect at the global level or after each node, after each subpbs, ...
				object_files can collect the names of the buffers to garbage collect

			do we keep the build buffers in case of multiple failures?
		
		
	keep information about the builder
	create a mini build system
		configuration
		possibility to run just that builder/ builders for a node
		
	
possibility to display the tree in warp mode
	global tree, warp tree
	filters should still apply
	
File not found for MD5 computation
	most probably a node that is not declared as virtual (although we have defined somewhere that virtual nodes should have a digest)
	builder failed
	=> display as much information as possible
		mini build system for just that error
		
generation of mini build systems
	in big systems, the developer does not want to re-run the build system just to find out what the problem is but wants a small build to debug the problem
	
Post pbs 
	can be implemented with a rule that matches the pbs run virtual node
		better than a rule that matches a target in case there are multiple runs of the pbsfile with different targets
		REH_POST_PBS => \&post_pbs_sub
		REH_POST_PBS => GetPBSConfig('POST_PBS_FILE')
		
	C depender should use the flags defined by the user rather than force flags on the user
	
	
override after **------ -dpl
	don't know what I meant
	if subpbs run (two extra phases here prepare to run subpbs and run subpbs, plus subpbs ran) matches a rule, it can display something else
	
	
Make clear how build command line parameters (%FILE_TO_BUILD, ...) are defined and handles
	also defined in plugins or rules
	
	
plugin to generate report on configuration usage
	a post pbs part of the examples in the distribution 
	
	configuration in, defined, used, delta with parent, delta with children (available after the subpbs runs)
	knowing how many configuration variables (along with rules, targets) can be used to automatically find what pbsfiles should be split
	
give example of C code generation
	or anything that needs to be depended
	explanation and example of IMMEDIATE_BUILD or creators
	
Creator, IMMEDIATE_BUILD, VIRTUAL, FORCED
	all can be REH!
	
rethink/document IMMEDIATE_BUILD FORCED, LOCAL_RULES and other exceptional mechanisms

REH_TRIGGER rather than pbs functionality
	starts a subpbs before the REH_LINK_TO
	
C_FILE_SYNCH
	it's a post build REH
	Make it visible
	
ON_ERROR REH
	for un interesting parts of the build just ignore the failures
		need to make it very clear
		
		
Express complicated dependencies
	references to dependency lists, but still locally defined
	depender subs are all mighty already
	simple interface to the dependers, better wizard
	
user defined wizards
	possibility to have multiple wizard directories 
		no need to put user wizards where pbs wizards are

replace plugins with rules
	CheckNodeName, rule matching all nodes
	CreateDump, rule matching top node, what if the build fails
	CreateLog, rule matching top node, what if the build fails
	EvaluateShellCommand, replace with EVAL_CONFIG, there is no reason to evaluate the shell command just
		to handle %SOMETHING. Adding SOMETHING to the config as the side effect to be self documenting
			- SOMETHING stays visible rather than transient during EvaluateShellCommand
			- the rule doing the evaluation is logged
	ExpandObjects, just a specific EvaluateShellCommand that should only define a configuration variable
		can be used directly as a REH (pre build) in specific rule:
			Rule
				Name => 'some_lib'.
				Match ...
				...
				ExpandObject,
				Build => "linker %DEPENDENCY_LIST_OBJECTS_EXPANDED"
				EndRule

			or as a REH global in the pbs run
			
			use 'Lib/ExpandObjects' ;
			# which contains
				Rule
					Name => 'expand object'.
					MatchAll,
					ExpandObject,
					EndRule

			or use the top rule injection mechanism at the top and have the rule available everywhere
			
	FileWatchClient, REH CHECK
		advantage, 
			nodes checked individually, maybe some optimization can come from it
				IE: node does not have to check itself if dependency has triggered already

		disadvantage:
			
	GraphGeneration, rule matching top, or any node if sub graph is wanted
	PackageVisualisation, rule matching all nodes
	PostPbs, rule matching top node, what if build fails?, can we have nodes representing PBS steps?
	SimplifyRule, a simplified, named, REH
	TreeVisualisation, rule matching top or any node
	Visualisation (-dc and other node visualisation switches), rule matching any node


	making the plugin match nodes means that we can run some functionality at any level we want
		IE: sub graph, sub tree, ....

		timing REH
			today we get timing for the whole build or a pbsfile run, we could get information
			per node or sub system

		most display options, especially if we have pbs, pbs step, config, ... nodes


plugins cli switches handling
	today:
		so they are called, ie: --generate_tree_graph
		so they get arguments, ie: --tree_graph_file xyz.png
		so they get options , ie: -- tree_graph_no_configuration
	
	call plugins/plugin-directory like switches?

	=> scan directory for switches?
		call a registration sub, let it register itself, return registration data
			cache the registration if libs are unchanged
				could be useful to cache the rules
					cache them in memory if server is used
	
	=> solution
		remove possibility to set path in prf, instead have a pbs_set.prf, this removes the need
		to run pbs.prf.
		
		pbs can scan the cli itself, as it already does, for cli path options

		
	defining "plugin" switches
		plugins are replaced by pbsfiles, libs really), so any pbsfile can ad switches
			the switches are scanned at start time, the pbsfile is loaded and registration
			sub is called in the pbsfile, the registration sub is replaced by a NOP sub, or
			a verification sub to catch non scan-registrated pbsfile which call the sub.

		the pbsfiles that define switches need to be declared in pbs_set.prf
			pbs can scan a single file, a directory, a directory with sub directories, use
			regex to select or deselect specific files when scanning a directory

		the options can take arguments, be called multiple times (array), ...

		option definition contains a help text

	user can define it's own switches via sub in a pbsfile (lib) where options are defined
		define aliases for pbs switches, eg: for switches that do not have short versions or simpler names
		switch grouping, eg: --debug for "-dd -dur -post_pbs ..."
		alias for -D, eg: --user_defined 1 => goes into the config as -D special=1 would do
			a nicer looking integration of the user's process

	we can verify which user defined switch that was put on the command line was not used during the run
		and warn user!

	option can add target
		also good for pbs so we can make some pbs options rules
			-h, -v, -gtg, -tt, ...

	user options can be cached so the pbsfiles defining them do not need to be reloaded
		"generating user options cache ..." # no cache
			--display_option_cach_generation
				shows what files are scanned and what options are added

			--no_user_option_override makes the optionsimmutable 

		"" # silent if cache is valid
		"regenerating user option cache", same options as above apply
		
help is a target
	option -h still exists but it adds a target to the top pbs

	let -h display user help rather than pbs help, user decides by renaming rules if they want

	possibility to tailor the pbs help by removing or reordering entries which is not easy when the
	help is part of PBS distribution.


wizards are rules and started with targets
	wizards target and normal target can be mixed, which is surprising and maybe should be warned for

	wizards are still defined in the wizard directory for pbs but user wizards could be anywhere as long
		as the rules are included in the pbsfile

	option -w can add the targets

keep a list of all the files loaded
	libs, pbs modules, prf, plugins, ...

	keep in warp file (today we have prf data entries but it's not set properly!)


warp file is a pbs run file
	it contains information about a build
	some of the information is warp speedup information
	warp 0 has a warp file


if a hierarchical display is requested (IE pbs config for all pbs runs, config, ...)
	display a delta between the level
	if no delta, display changed values in different colors
	if config is local, display it in special color

	use two color scheme for levels
		eg: level % 2 in green, level !% 2 in bright green or cyan ...
		it's easy to miss the level change in large data amounts

256 color mode if the terminal supports it
	not 256 color but different colors for different elements, user defines colors sequences they want
		we stop using Terminal::AnsiColor
			or recognize "red on blue" otherwise use the user code

Immutable rule set
	pbsfiles that always generate the same rules 
		no if-else or REH_IF used to decide which rules are added
		they can use configuration variables in the builders

	can be cached in memory
		pbsuse should be faster
		in server memory in between pbs tool runs

	may be serialized in a more efficient format than the pbsfile

	cache is invalidated when pbsfile is changed (clearly!)




